{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00917813-6f69-487d-b273-f7403da3d935"}}},{"cell_type":"markdown","source":["# Aggregation\n\n##### Objectives\n1. Group data by specified columns\n1. Apply grouped data methods to aggregate data\n1. Apply built-in functions to aggregate data\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `groupBy`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html#pyspark.sql.GroupedData\" target=\"_blank\" target=\"_blank\">Grouped Data</a>: `agg`, `avg`, `count`, `max`, `sum`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>: `approx_count_distinct`, `avg`, `sum`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be18699-a5e2-4aeb-8fe1-ed5f3cc03ab6"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c303dd0e-9273-4616-88d1-4f8191f64e48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use the BedBricks events dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afa929a6-6930-48a8-893b-86377e2a74f0"}}},{"cell_type":"code","source":["df = spark.read.parquet(eventsPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f0289e1-b050-4201-8945-464f1bdf2c15"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Grouping data\n\n<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e0a91a4-aee8-4778-bb05-f52b097a4a67"}}},{"cell_type":"markdown","source":["### groupBy\nUse the DataFrame `groupBy` method to create a grouped data object. \n\nThis grouped data object is called `RelationalGroupedDataset` in Scala and `GroupedData` in Python."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16dc6160-e7bf-49db-80e1-b8dbe39daa21"}}},{"cell_type":"code","source":["df.groupBy(\"event_name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78ea6b40-7f89-4545-b8be-f3172d5abb54"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.groupBy(\"geo.state\", \"geo.city\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41f2bfcd-df1a-4c42-afd4-46e11fb0e1ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Grouped data methods\nVarious aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.html\" target=\"_blank\">GroupedData</a> object.\n\n\n| Method | Description |\n| --- | --- |\n| agg | Compute aggregates by specifying a series of aggregate columns |\n| avg | Compute the mean value for each numeric columns for each group |\n| count | Count the number of rows for each group |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| min | Compute the min value for each numeric column for each group |\n| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n| sum | Compute the sum for each numeric columns for each group |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b17365-4cc1-404d-8b24-c22888eb8370"}}},{"cell_type":"code","source":["eventCountsDF = df.groupBy(\"event_name\").count()\ndisplay(eventCountsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f2204eb-2b84-43f4-aac4-26854507096c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here, we're getting the average purchase revenue for each."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc44e4b1-4317-4791-abfa-38f6f54817ea"}}},{"cell_type":"code","source":["avgStatePurchasesDF = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\ndisplay(avgStatePurchasesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35f483aa-a738-468f-b115-f570e2ecb39e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And here the total quantity and sum of the purchase revenue for each combination of state and city."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83165ec2-28e7-47ff-8375-0c018794348c"}}},{"cell_type":"code","source":["cityPurchaseQuantitiesDF = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\ndisplay(cityPurchaseQuantitiesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e242e6a-5284-40ed-896c-df1e27967e02"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Built-In Functions\nIn addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module.\n\nIn Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_bank\">`org.apache.spark.sql.functions`</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">`pyspark.sql.functions`</a> in Python. Functions from this module must be imported into your code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3c9989d-9913-4de6-a595-ef9dd0bd028b"}}},{"cell_type":"markdown","source":["### Aggregate Functions\n\nHere are some of the built-in functions available for aggregation.\n\n| Method | Description |\n| --- | --- |\n| approx_count_distinct | Returns the approximate number of distinct items in a group |\n| avg | Returns the average of the values in a group |\n| collect_list | Returns a list of objects with duplicates |\n| corr | Returns the Pearson Correlation Coefficient for two columns |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| stddev_samp | Returns the sample standard deviation of the expression in a group |\n| sumDistinct | Returns the sum of distinct values in the expression |\n| var_pop | Returns the population variance of the values in a group |\n\nUse the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">`agg`</a> to apply built-in aggregate functions\n\nThis allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">`alias`</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8d37314-24a7-405a-a77d-b9634e4ab4c8"}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n\nstatePurchasesDF = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\ndisplay(statePurchasesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cfbf211-53c9-4824-b068-5e0bf9764d51"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Apply multiple aggregate functions on grouped data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6722b506-fe36-4368-a03f-759c65ec48cf"}}},{"cell_type":"code","source":["from pyspark.sql.functions import avg, approx_count_distinct\n\nstateAggregatesDF = (df\n                     .groupBy(\"geo.state\")\n                     .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n                          approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n                    )\n\ndisplay(stateAggregatesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd6638b4-83de-429d-b791-975b022b88a6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Math Functions\nHere are some of the built-in functions for math operations.\n\n| Method | Description |\n| --- | --- |\n| ceil | Computes the ceiling of the given column. |\n| cos | Computes the cosine of the given value. |\n| log | Computes the natural logarithm of the given value. |\n| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n| sqrt | Computes the square root of the specified float value. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c5cb382-ada7-4bbb-adf0-0232ed381610"}}},{"cell_type":"code","source":["from pyspark.sql.functions import cos, sqrt\n\ndisplay(\n    spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n    .withColumn(\"sqrt\", sqrt(\"id\"))\n    .withColumn(\"cos\", cos(\"id\"))\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e6103fb-9c30-4913-90eb-f4a8056edce2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Revenue by Traffic Lab\nGet the 3 traffic sources generating the highest total revenue.\n1. Aggregate revenue by traffic source\n2. Get top 3 traffic sources by total revenue\n3. Clean revenue columns to have two decimal places\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: groupBy, sort, limit\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html?highlight=column#pyspark.sql.Column\" target=\"_blank\">Column</a>: alias, desc, cast, operators\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-in Functions</a>: avg, sum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c170bb04-3f26-4258-a282-c44134919bf3"}}},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame **`df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d507120-e7d9-4263-8815-bd14a60023bf"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\n# Purchase events logged on the BedBricks website\ndf = (spark.read.parquet(eventsPath)\n      .withColumn(\"revenue\", col(\"ecommerce.purchase_revenue_in_usd\"))\n      .filter(col(\"revenue\").isNotNull())\n      .drop(\"event_name\")\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e43031-2c82-4300-a452-fe3a89bf00f5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Aggregate revenue by traffic source\n- Group by **`traffic_source`**\n- Get sum of **`revenue`** as **`total_rev`**\n- Get average of **`revenue`** as **`avg_rev`**\n\nRemember to import any necessary built-in functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db088b23-814e-46b1-b1f0-481e52d13d85"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import avg, col, sum\n\ntrafficDF = (df\n             .groupBy(\"traffic_source\")\n             .agg(sum(col(\"revenue\")).alias(\"total_rev\"),\n                  avg(col(\"revenue\")).alias(\"avg_rev\"))\n            )\n\ndisplay(trafficDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d106823-218b-402a-8cdd-79e713d384da"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"263f2bbb-64fc-49f0-b8e8-6816215b2c5e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import round\n\nexpected1 = [(12704560.0, 1083.175), (78800000.3, 983.2915), (24797837.0, 1076.6221), (47218429.0, 1086.8303), (16177893.0, 1083.4378), (8044326.0, 1087.218)]\ntestDF = trafficDF.sort(\"traffic_source\").select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\nresult1 = [(row.total_rev, row.avg_rev) for row in testDF.collect()]\n\nassert(expected1 == result1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"268bd346-0bd7-4252-8246-a423b6c36908"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get top three traffic sources by total revenue\n- Sort by **`total_rev`** in descending order\n- Limit to first three rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c250cfdc-e101-4336-8c17-2e6d8a842a78"}}},{"cell_type":"code","source":["# ANSWER\ntopTrafficDF = trafficDF.sort(col(\"total_rev\").desc()).limit(3)\ndisplay(topTrafficDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ade8e24a-7504-4655-87e7-5902b02af1c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4e00f06-26f6-4314-bed2-ad628d98ced1"}}},{"cell_type":"code","source":["expected2 = [(78800000.3, 983.2915), (47218429.0, 1086.8303), (24797837.0, 1076.6221)]\ntestDF = topTrafficDF.select(round(\"total_rev\", 4).alias(\"total_rev\"), round(\"avg_rev\", 4).alias(\"avg_rev\"))\nresult2 = [(row.total_rev, row.avg_rev) for row in testDF.collect()]\n\nassert(expected2 == result2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6c65261-2e79-4515-bba7-5395fceba814"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Limit revenue columns to two decimal places\n- Modify columns **`avg_rev`** and **`total_rev`** to contain numbers with two decimal places\n  - Use **`withColumn()`** with the same names to replace these columns\n  - To limit to two decimal places, multiply each column by 100, cast to long, and then divide by 100"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed4fa5d1-5867-4fc8-86bc-2ced559376fd"}}},{"cell_type":"code","source":["# ANSWER\nfinalDF = (topTrafficDF\n           .withColumn(\"avg_rev\", (col(\"avg_rev\") * 100).cast(\"long\") / 100)\n           .withColumn(\"total_rev\", (col(\"total_rev\") * 100).cast(\"long\") / 100)\n          )\n\ndisplay(finalDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59d6de1e-a8af-4073-b062-3ebb0dfd3d3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82d9aa68-432a-4553-a511-d52cd455f1b4"}}},{"cell_type":"code","source":["expected3 = [(78800000.29, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult3 = [(row.total_rev, row.avg_rev) for row in finalDF.collect()]\n\nassert(expected3 == result3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc22690c-692b-4d0e-b28c-fa5d4b5556f3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Bonus: Rewrite using a built-in math function\nFind a built-in math function that rounds to a specified number of decimal places"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d473333a-5807-411f-be9b-c7729346ff17"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import round\n\nbonusDF = (topTrafficDF\n           .withColumn(\"avg_rev\", round(\"avg_rev\", 2))\n           .withColumn(\"total_rev\", round(\"total_rev\", 2))\n          )\n\ndisplay(bonusDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c777ac8-c3a7-4304-98a9-b11141f49982"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba47e2af-cbbd-4567-b637-38fc593482c2"}}},{"cell_type":"code","source":["expected4 = [(78800000.3, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult4 = [(row.total_rev, row.avg_rev) for row in bonusDF.collect()]\n\nassert(expected4 == result4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d8b6462-dd2f-4227-b964-98c2404e6eb1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. Chain all the steps above"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c2cb7c8-e798-4ae3-919e-71cbec4e0278"}}},{"cell_type":"code","source":["# ANSWER\nchainDF = (df\n           .groupBy(\"traffic_source\")\n           .agg(sum(col(\"revenue\")).alias(\"total_rev\"),\n                avg(col(\"revenue\")).alias(\"avg_rev\"))\n           .sort(col(\"total_rev\").desc())\n           .limit(3)\n           .withColumn(\"avg_rev\", round(\"avg_rev\", 2))\n           .withColumn(\"total_rev\", round(\"total_rev\", 2))\n          )\n\ndisplay(chainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6747f10b-499a-40c3-a78d-ca1e639c7cf3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcc7cea5-75d8-4eb7-b0ad-d776b5d3ccd0"}}},{"cell_type":"code","source":["expected5 = [(78800000.3, 983.29), (47218429.0, 1086.83), (24797837.0, 1076.62)]\nresult5 = [(row.total_rev, row.avg_rev) for row in chainDF.collect()]\n\nassert(expected5 == result5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e38464-0b34-49d8-85d2-3d5740d19133"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8753392c-eea6-41d6-9961-e24a79f696d5"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c7fc883-91aa-42f9-b356-7cac1749ad25"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ecdf224-b200-4296-aff3-2c415d7c55da"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.1 - Aggregation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562398990}},"nbformat":4,"nbformat_minor":0}
