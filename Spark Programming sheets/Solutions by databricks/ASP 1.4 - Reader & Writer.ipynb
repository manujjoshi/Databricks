{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4c82f49-3845-46d6-8e6f-f69ba1850bbf"}}},{"cell_type":"markdown","source":["# Reader & Writer\n##### Objectives\n1. Read from CSV files\n1. Read from JSON files\n1. Write DataFrame to files\n1. Write DataFrame to tables\n1. Write DataFrame to a Delta table\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>: `csv`, `json`, `option`, `schema`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>: `mode`, `option`, `parquet`, `format`, `saveAsTable`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType\" target=\"_blank\">StructType</a>: `toDDL`\n\n##### Spark Types\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\" target=\"_blank\">Types</a>: `ArrayType`, `DoubleType`, `IntegerType`, `LongType`, `StringType`, `StructType`, `StructField`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85d6fea2-94bc-406d-9d94-53c7b6dbe253"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"501bcd31-2b5e-4c91-87d6-24a1d2449d39"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrameReader\nInterface used to load a DataFrame from external storage systems\n\n```\nspark.read.parquet(\"path/to/files\")\n```\n\nDataFrameReader is accessible through the SparkSession attribute `read`. This class includes methods to load DataFrames from different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02147889-5fb5-48d4-9381-6d59d89e2f0f"}}},{"cell_type":"markdown","source":["### Read from CSV files\nRead from CSV with the DataFrameReader's `csv` method and the following options:\n\nTab separator, use first line as header, infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a76c6c06-3c2e-49b2-8b0b-745a257ea0e7"}}},{"cell_type":"code","source":["usersCsvPath = \"/mnt/training/ecommerce/users/users-500k.csv\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .option(\"inferSchema\", True)\n           .csv(usersCsvPath)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56a2c888-381d-4119-b4c9-dae22c20b764"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Spark's Python API also allows you to specify the DataFrameReader options as parameters to the `csv` method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc460383-337d-4264-9c4f-6881842c3aab"}}},{"cell_type":"code","source":["usersDF = (spark\n           .read\n           .csv(usersCsvPath, sep=\"\\t\", header=True, inferSchema=True)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd73daa0-f995-4abb-96df-f50d2cd06223"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Manually define the schema by creating a `StructType` with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"022e9d60-404f-49b1-8544-6b9cc24ab0e9"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"email\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9842794e-b86b-40be-b9c6-d028d0c11d54"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read from CSV using this user-defined schema instead of inferring the schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38ae6223-58d5-4a69-a063-98b566cd4671"}}},{"cell_type":"code","source":["usersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(userDefinedSchema)\n           .csv(usersCsvPath)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6d209c-271d-471e-80f4-e8ab7a5626b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternatively, define the schema using <a href=\"https://en.wikipedia.org/wiki/Data_definition_language\" target=\"_blank\">data definition language (DDL)</a> syntax."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33751642-7487-422b-afc2-6ae178053d58"}}},{"cell_type":"code","source":["DDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(DDLSchema)\n           .csv(usersCsvPath)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6a78da-2a7b-4f4c-93af-7d9c32320bd1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read from JSON files\n\nRead from JSON with DataFrameReader's `json` method and the infer schema option"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdea4d4f-bc48-45bc-a83c-98e19df0b2c9"}}},{"cell_type":"code","source":["eventsJsonPath = \"/mnt/training/ecommerce/events/events-500k.json\"\n\neventsDF = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .json(eventsJsonPath)\n           )\n\neventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6af9a312-6458-4c2c-a23e-ea7892ba9f82"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read data faster by creating a `StructType` with the schema names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4e004cb-6e57-4e35-bbea-17d6523ffd1b"}}},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n    StructField(\"device\", StringType(), True),\n    StructField(\"ecommerce\", StructType([\n        StructField(\"purchaseRevenue\", DoubleType(), True),\n        StructField(\"total_item_quantity\", LongType(), True),\n        StructField(\"unique_items\", LongType(), True)\n    ]), True),\n    StructField(\"event_name\", StringType(), True),\n    StructField(\"event_previous_timestamp\", LongType(), True),\n    StructField(\"event_timestamp\", LongType(), True),\n    StructField(\"geo\", StructType([\n        StructField(\"city\", StringType(), True),\n        StructField(\"state\", StringType(), True)\n    ]), True),\n    StructField(\"items\", ArrayType(\n        StructType([\n            StructField(\"coupon\", StringType(), True),\n            StructField(\"item_id\", StringType(), True),\n            StructField(\"item_name\", StringType(), True),\n            StructField(\"item_revenue_in_usd\", DoubleType(), True),\n            StructField(\"price_in_usd\", DoubleType(), True),\n            StructField(\"quantity\", LongType(), True)\n        ])\n    ), True),\n    StructField(\"traffic_source\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"user_id\", StringType(), True)\n])\n\neventsDF = (spark\n            .read\n            .schema(userDefinedSchema)\n            .json(eventsJsonPath)\n           )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4f65a04-992b-46ff-8464-de424f0becd4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can use the `StructType` Scala method `toDDL` to have a DDL-formatted string created for you.\n\nIn a Python notebook, create a Scala cell to create the string to copy and paste."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0afcd7e1-2a42-4c4b-a952-41ccd4506d1d"}}},{"cell_type":"code","source":["%scala\nspark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").schema.toDDL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a026ed4f-2826-4660-9acb-f7ee31691a30"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DDLSchema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n\neventsDF = (spark\n            .read\n            .schema(DDLSchema)\n            .json(eventsJsonPath)\n           )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"101a16f4-198f-41a9-98ae-100dfd0763c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrameWriter\nInterface used to write a DataFrame to external storage systems\n\n```\n(df.write                         \n  .option(\"compression\", \"snappy\")\n  .mode(\"overwrite\")      \n  .parquet(outPath)       \n)\n```\n\nDataFrameWriter is accessible through the SparkSession attribute `write`. This class includes methods to write DataFrames to different external storage systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aa0d7cf-3b20-4415-bb4f-f0f15d3cb924"}}},{"cell_type":"markdown","source":["### Write DataFrames to files\n\nWrite `usersDF` to parquet with DataFrameWriter's `parquet` method and the following configurations:\n\nSnappy compression, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1905f4f4-2311-4798-8e37-e8303888030b"}}},{"cell_type":"code","source":["usersOutputPath = workingDir + \"/users.parquet\"\n\n(usersDF\n .write\n .option(\"compression\", \"snappy\")\n .mode(\"overwrite\")\n .parquet(usersOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"401af031-d632-45cb-9884-80e5661f1f6b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(\n    dbutils.fs.ls(usersOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb55c29d-321c-4d80-a2eb-3483ede4edcc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the `parquet` method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58a7f3fd-b026-4bf4-b430-b8dd50d95ae6"}}},{"cell_type":"code","source":["(usersDF\n .write\n .parquet(usersOutputPath, compression=\"snappy\", mode=\"overwrite\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95965ee1-c310-44fd-b928-be908f4d9f41"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write DataFrames to tables\n\nWrite `eventsDF` to a table using the DataFrameWriter method `saveAsTable`\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This creates a global table, unlike the local view created by the DataFrame method `createOrReplaceTempView`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fded22ec-2664-435b-9072-fec6be9fadba"}}},{"cell_type":"code","source":["eventsDF.write.mode(\"overwrite\").saveAsTable(\"events_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13f65231-90ca-4705-9866-509e063421fa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This table was saved in the database created for you in classroom setup. See database name printed below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4d28a78-ac67-4786-bc57-aef557757f4a"}}},{"cell_type":"code","source":["print(databaseName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24912001-9902-4ea7-a80b-abf3533f7985"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Delta Lake\n\nIn almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. \n\n<a href=\"https://delta.io/\" target=\"_blank\">Delta Lake</a> is an open source technology designed to work with Spark to bring reliability to data lakes.\n\n![delta](https://files.training.databricks.com/images/aspwd/delta_storage_layer.png)\n\n#### Delta Lake's Key Features\n- ACID transactions\n- Scalable metadata handline\n- Unified streaming and batch processing\n- Time travel (data versioning)\n- Schema enforcement and evolution\n- Audit history\n- Parquet format\n- Compatible with Apache Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8301848b-27aa-4f36-8d8a-fd3f9d3883ee"}}},{"cell_type":"markdown","source":["### Write Results to a Delta Table\n\nWrite `eventsDF` with the DataFrameWriter's `save` method and the following configurations: Delta format, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bed4f62-b03a-4fa1-afc4-0e4bfd09051b"}}},{"cell_type":"code","source":["eventsOutputPath = workingDir + \"/delta/events\"\n\n(eventsDF\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(eventsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dff3336-bfa2-48f1-9bbd-03d8d3405c77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Ingesting Data Lab\n\nRead in CSV files containing products data.\n\n##### Tasks\n1. Read with infer schema\n2. Read with user-defined schema\n3. Read with schema as DDL formatted string\n4. Write using Delta format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"515fda6c-8759-4ebc-be4c-6f0bf1ac8986"}}},{"cell_type":"markdown","source":["### 1. Read with infer schema\n- View the first CSV file using DBUtils method `fs.head` with the filepath provided in the variable `singleProductCsvFilePath`\n- Create `productsDF` by reading from CSV files located in the filepath provided in the variable `productsCsvPath`\n  - Configure options to use first line as header and infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06647773-7013-40b5-be3a-d65e179cacb7"}}},{"cell_type":"code","source":["# ANSWER\nsingleProductCsvFilePath = \"/mnt/training/ecommerce/products/products.csv/part-00000-tid-1663954264736839188-daf30e86-5967-4173-b9ae-d1481d3506db-2367-1-c000.csv\"\n\nprint(dbutils.fs.head(singleProductCsvFilePath))\n\nproductsCsvPath = \"/mnt/training/ecommerce/products/products.csv\"\n\nproductsDF = (spark\n              .read\n              .option(\"header\", True)\n              .option(\"inferSchema\", True)\n              .csv(productsCsvPath)\n             )\n\nproductsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94616831-1698-4302-a30b-01d601128ce7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c54b9851-e36c-4f00-87c3-b769876c39f9"}}},{"cell_type":"code","source":["assert(productsDF.count() == 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"044ad1a8-f2d6-4e14-9ed3-be6f559c01dc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Read with user-defined schema\nDefine schema by creating a `StructType` with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b30c4d66-2742-4a36-b9df-301bf3eb665a"}}},{"cell_type":"code","source":["# ANSWER\nuserDefinedSchema = StructType([\n    StructField(\"item_id\", StringType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"price\", DoubleType(), True)\n])\n\nproductsDF2 = (spark\n               .read\n               .option(\"header\", True)\n               .schema(userDefinedSchema)\n               .csv(productsCsvPath)\n              )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96c1e9ea-3d20-4750-9857-6bc0ee3c5691"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab0fa314-9b92-410c-8cf2-2bca3b3380c6"}}},{"cell_type":"code","source":["assert(userDefinedSchema.fieldNames() == [\"item_id\", \"name\", \"price\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21c32b94-a700-441f-b7cb-7e331e901fb6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\n\nexpected1 = Row(item_id=\"M_STAN_Q\", name=\"Standard Queen Mattress\", price=1045.0)\nresult1 = productsDF2.first()\n\nassert(expected1 == result1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0de4beb2-e187-418f-a8b8-1a2c876ad3bc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Read with DDL formatted string"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe9f844-7c98-4c31-b4b1-731e1231db8b"}}},{"cell_type":"code","source":["# ANSWER\nDDLSchema = \"`item_id` STRING,`name` STRING,`price` DOUBLE\"\n\nproductsDF3 = (spark\n               .read\n               .option(\"header\", True)\n               .schema(DDLSchema)\n               .csv(productsCsvPath)\n              )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee563c9a-2845-40d9-adb6-b7eed62d6371"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f7c9d84-1b81-4e6b-a086-3214238f3798"}}},{"cell_type":"code","source":["assert(productsDF3.count() == 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7ff54d4-a332-4779-96dc-169ee2a412d8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Write to Delta\nWrite `productsDF` to the filepath provided in the variable `productsOutputPath`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2eee98f2-cca6-4d10-8111-c85bab555960"}}},{"cell_type":"code","source":["# ANSWER\nproductsOutputPath = workingDir + \"/delta/products\"\n(productsDF\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(productsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff998001-8350-4096-8b72-dff5ee593548"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"721851be-8058-445e-927a-6c457fbcd193"}}},{"cell_type":"code","source":["verify_files = dbutils.fs.ls(productsOutputPath)\nverify_delta_format = False\nverify_num_data_files = 0\nfor f in verify_files:\n    if f.name == '_delta_log/':\n        verify_delta_format = True\n    elif f.name.endswith('.parquet'):\n        verify_num_data_files += 1\n\nassert verify_delta_format, \"Data not written in Delta format\"\nassert verify_num_data_files > 0, \"No data written\"\ndel verify_files, verify_delta_format, verify_num_data_files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76ee8d34-a099-4463-9c9e-3b032919483a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5199e70-dd3c-44ac-8178-6403a6dd45f6"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f003655-1beb-4d25-bef6-b797d144aae4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eae2f9e-60fd-42be-b6f9-4ca36b13c7ec"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 1.4 - Reader & Writer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399487}},"nbformat":4,"nbformat_minor":0}
