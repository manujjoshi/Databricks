{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3a7b6b1-4212-4569-ad1c-9cab4b9ace58"}}},{"cell_type":"markdown","source":["# DataFrames and Transformations Review\n## De-Duping Data Lab\n\nIn this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n\n* first, middle and last names\n* gender\n* birth date\n* Social Security number\n* salary\n\nBut, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n\n* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n\nIf all of the name fields match -- if you disregard character case -- then the birth dates and salaries are guaranteed to match as well,\nand the Social Security Numbers *would* match if they were somehow put in the same format.\n\nYour job is to remove the duplicate records. The specific requirements of your job are:\n\n* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n* Preserve the data format of the columns. For example, if you write the first name column in all lowercase, you haven't met this requirement.\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> The initial dataset contains 103,000 records.\nThe de-duplicated result has 100,000 records.\n\nNext, write the results in **Delta** format as a **single data file** to the directory given by the variable *deltaDestDir*.\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> Remember the relationship between the number of partitions in a DataFrame and the number of files written.\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">DataFrameWriter</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d695a128-7069-4b7a-a16c-da84c37131f2"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eb23ade-dfc8-4a2e-9728-5d25226ba20d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3202dc74-cea8-4094-9c66-3561472fcf72"}}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/dataframes/people-with-dups.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68030557-d31e-45d2-b963-f7e683204e90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\nsourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\ndeltaDestDir = workingDir + \"/people\"\n\n# In case it already exists\ndbutils.fs.rm(deltaDestDir, True)\n\n# dropDuplicates() will introduce a shuffle, so it helps to reduce the number of post-shuffle partitions.\nspark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n\n# Okay, now we can read this thing\ndf = (spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .option(\"sep\", \":\")\n      .csv(sourceFile)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d45604f-2130-4072-a93b-cdbf6ba45afa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, lower, translate\n\ndedupedDF = (df\n             .select(col(\"*\"),\n                     lower(col(\"firstName\")).alias(\"lcFirstName\"),\n                     lower(col(\"lastName\")).alias(\"lcLastName\"),\n                     lower(col(\"middleName\")).alias(\"lcMiddleName\"),\n                     translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\n                     # regexp_replace(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")  # An alternate function to strip the hyphens\n                     # regexp_replace(col(\"ssn\"), \"\"\"^(\\d{3})(\\d{2})(\\d{4})$\"\"\", \"$1-$2-$3\").alias(\"ssnNums\")  # An alternate that adds hyphens if missing\n                    )\n             .dropDuplicates([\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\", \"gender\", \"birthDate\", \"salary\"])\n             .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31e405c9-a7c3-4078-a89a-d6a973ab6b9f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\n# Now, write the results in Delta format as a single file. We'll also display the Delta files to make sure they were written as expected.\n\n(dedupedDF\n .repartition(1)\n .write\n .mode(\"overwrite\")\n .format(\"delta\")\n .save(deltaDestDir)\n)\n\ndisplay(dbutils.fs.ls(deltaDestDir))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2da8b6f-b4bd-47e9-9cdf-ee2dc87abded"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e69e0f8-7a29-4519-8426-49d4ce615378"}}},{"cell_type":"code","source":["verify_files = dbutils.fs.ls(deltaDestDir)\nverify_delta_format = False\nverify_num_data_files = 0\nfor f in verify_files:\n    if f.name == '_delta_log/':\n        verify_delta_format = True\n    elif f.name.endswith('.parquet'):\n        verify_num_data_files += 1\n\nassert verify_delta_format, \"Data not written in Delta format\"\nassert verify_num_data_files == 1, \"Expected 1 data file written\"\n\nverify_record_count = spark.read.format(\"delta\").load(deltaDestDir).count()\nassert verify_record_count == 100000, \"Expected 100000 records in final result\"\n\ndel verify_files, verify_delta_format, verify_num_data_files, verify_record_count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbf3693e-0f99-4005-a6ae-68fdf1f43bb4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Clean up classroom\nRun the cell below to clean up resources."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1387a430-a509-4fa3-acb3-476a07d32d8b"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"244c1177-1104-4e1a-9b42-b0b2dbad6329"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"214b6878-228f-447f-a8df-670bfb80d9fc"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 3.4 - Review","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399473}},"nbformat":4,"nbformat_minor":0}
