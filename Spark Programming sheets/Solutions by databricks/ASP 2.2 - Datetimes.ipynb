{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1c1a677-c836-4294-8426-c0735a24d772"}}},{"cell_type":"markdown","source":["# Datetime Functions\n\n##### Objectives\n1. Cast to timestamp\n2. Format datetimes\n3. Extract from timestamp\n4. Convert to date\n5. Manipulate datetimes\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.html#pyspark.sql.Column\" target=\"_blank\">Column</a>: `cast`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>: `date_format`, `to_date`, `date_add`, `year`, `month`, `dayofweek`, `minute`, `second`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e0c4ca1-766c-4b56-9f55-b103bc3eb915"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"732cda5c-40b0-4ce7-89f4-18afe2c2fd0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's use a subset of the BedBricks events dataset to practice working with date times."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c5e1fe0-7f88-45c9-b87c-b39955bbed21"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = spark.read.parquet(eventsPath).select(\"user_id\", col(\"event_timestamp\").alias(\"timestamp\"))\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93ec08e4-2bd5-4049-83c2-a196ef6bf645"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Built-In Functions: Date Time Functions\nHere are a few built-in functions to manipulate dates and times in Spark.\n\n| Method | Description |\n| --- | --- |\n| add_months | Returns the date that is numMonths after startDate |\n| current_timestamp | Returns the current timestamp at the start of query evaluation as a timestamp column |\n| date_format | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n| dayofweek | Extracts the day of the month as an integer from a given date/timestamp/string |\n| from_unixtime | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format |\n| minute | Extracts the minutes as an integer from a given date/timestamp/string. |\n| unix_timestamp | Converts time string with given pattern to Unix timestamp (in seconds) |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dd0a2bb-b897-46bf-93a0-dcdedc1b9d4e"}}},{"cell_type":"markdown","source":["### Cast to Timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"999c3cf3-660c-442f-8788-7d30aac8cdb1"}}},{"cell_type":"markdown","source":["#### `cast()`\nCasts column to a different data type, specified using string representation or DataType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52c837f3-0b9c-414c-8db8-cf04eaef93af"}}},{"cell_type":"code","source":["timestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(\"timestamp\"))\ndisplay(timestampDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6547c4f4-5c75-47c6-94db-585f39d047a4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\n\ntimestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(TimestampType()))\ndisplay(timestampDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c53c7b62-58ed-470f-a1d3-690b7f711648"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Datetime Patterns for Formatting and Parsing\nThere are several common scenarios for datetime usage in Spark:\n\n- CSV/JSON datasources use the pattern string for parsing and formatting datetime content.\n- Datetime functions related to convert StringType to/from DateType or TimestampType e.g. `unix_timestamp`, `date_format`, `from_unixtime`, `to_date`, `to_timestamp`, etc.\n\nSpark uses <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">pattern letters for date and timestamp parsing and formatting</a>. A subset of these patterns are shown below.\n\n| Symbol | Meaning         | Presentation | Examples               |\n| ------ | --------------- | ------------ | ---------------------- |\n| G      | era             | text         | AD; Anno Domini        |\n| y      | year            | year         | 2020; 20               |\n| D      | day-of-year     | number(3)    | 189                    |\n| M/L    | month-of-year   | month        | 7; 07; Jul; July       |\n| d      | day-of-month    | number(3)    | 28                     |\n| Q/q    | quarter-of-year | number/text  | 3; 03; Q3; 3rd quarter |\n| E      | day-of-week     | text         | Tue; Tuesday           |\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Spark's handling of dates and timestamps changed in version 3.0, and the patterns used for parsing and formatting these values changed as well. For a discussion of these changes, please reference <a href=\"https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html\" target=\"_blank\">this Databricks blog post</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"945eadae-b800-45df-acda-8ad96980fc4b"}}},{"cell_type":"markdown","source":["### Format date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f73c28d5-de04-4e3f-88c0-557556bb3010"}}},{"cell_type":"markdown","source":["#### `date_format()`\nConverts a date/timestamp/string to a string formatted with the given date time pattern."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd69ebf5-1ce5-41db-bf0f-797e8bfd6d99"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\n\nformattedDF = (timestampDF\n               .withColumn(\"date string\", date_format(\"timestamp\", \"MMMM dd, yyyy\"))\n               .withColumn(\"time string\", date_format(\"timestamp\", \"HH:mm:ss.SSSSSS\"))\n              )\ndisplay(formattedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8973d4d9-4561-4d89-b20d-dec0b0aaa7c7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Extract datetime attribute from timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55490123-e366-4ea4-9ea8-b4fda8b19a0a"}}},{"cell_type":"markdown","source":["#### `year`\nExtracts the year as an integer from a given date/timestamp/string.\n\n##### Similar methods: `month`, `dayofweek`, `minute`, `second`, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"273067ff-3a46-4607-8d22-d198f9475848"}}},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, dayofweek, minute, second\n\ndatetimeDF = (timestampDF\n              .withColumn(\"year\", year(col(\"timestamp\")))\n              .withColumn(\"month\", month(col(\"timestamp\")))\n              .withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n              .withColumn(\"minute\", minute(col(\"timestamp\")))\n              .withColumn(\"second\", second(col(\"timestamp\")))\n             )\ndisplay(datetimeDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dee8e6d7-6394-45eb-9753-aaf242f631c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Convert to Date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7b613a3-0db1-41e2-a111-f3aad4951c9d"}}},{"cell_type":"markdown","source":["#### `to_date`\nConverts the column into DateType by casting rules to DateType."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eb0f890-748b-4cb3-8e39-03b32411515f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n\ndateDF = timestampDF.withColumn(\"date\", to_date(col(\"timestamp\")))\ndisplay(dateDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50588fec-eac4-4303-9054-e3fa5d815802"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Manipulate Datetimes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58208333-c9d4-48c4-b877-0db7550e5e1b"}}},{"cell_type":"markdown","source":["#### `date_add`\nReturns the date that is the given number of days after start"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b64a7572-a63a-403f-a64c-31f598d292cf"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add\n\nplus2DF = timestampDF.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\ndisplay(plus2DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04c4bb2d-622d-4066-a7dd-f381f0b84014"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Active Users Lab\nPlot daily active users and average active users by day of week.\n1. Extract timestamp and date of events\n2. Get daily active users\n3. Get average number of active users by day of week\n4. Sort day of week in correct order"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce74c91f-0a34-44dd-8bc6-27ae657f38f1"}}},{"cell_type":"markdown","source":["### Setup\nRun the cell below to create the starting DataFrame of user IDs and timestamps of events logged on the BedBricks website."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2558dee-0c85-41da-968a-a8ebb0a07e1f"}}},{"cell_type":"code","source":["df = (spark\n      .read\n      .parquet(eventsPath)\n      .select(\"user_id\", col(\"event_timestamp\").alias(\"ts\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6917d723-4ed3-47e8-b513-49113b9f484f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Extract timestamp and date of events\n- Convert **`ts`** from microseconds to seconds by dividing by 1 million and cast to timestamp\n- Add **`date`** column by converting **`ts`** to date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a38d02db-b075-4695-90da-62c65cb53fa8"}}},{"cell_type":"code","source":["# ANSWER\ndatetimeDF = (df\n              .withColumn(\"ts\", (col(\"ts\") / 1e6).cast(\"timestamp\"))\n              .withColumn(\"date\", to_date(\"ts\"))\n             )\ndisplay(datetimeDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02aa527b-3571-4d94-827c-cfd8c066fabe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"876e613e-47ad-4751-8d4e-abdcbe9680fe"}}},{"cell_type":"code","source":["from pyspark.sql.types import DateType, StringType, StructField, StructType, TimestampType\n\nexpected1a = StructType([StructField(\"user_id\", StringType(), True),\n                         StructField(\"ts\", TimestampType(), True),\n                         StructField(\"date\", DateType(), True)])\n\nresult1a = datetimeDF.schema\n\nassert expected1a == result1a, \"datetimeDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfb31c41-bfdb-4672-9f73-e7a7f18b48c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime\n\nexpected1b = datetime.date(2020, 6, 19)\nresult1b = datetimeDF.sort(\"date\").first().date\n\nassert expected1b == result1b, \"datetimeDF does not have the expected date values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4803c66b-3b24-443f-b336-db93697c3993"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Get daily active users\n- Group by date\n- Aggregate approximate count of distinct **`user_id`** and alias to \"active_users\"\n  - Recall built-in function to get approximate count distinct\n- Sort by date\n- Plot as line graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6d7c68-219e-4268-970b-466692a84d99"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import approx_count_distinct\n\nactiveUsersDF = (datetimeDF\n                 .groupBy(\"date\")\n                 .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n                 .sort(\"date\")\n                )\ndisplay(activeUsersDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6271e4b6-34fa-40b9-9217-0153736a3e0e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d509696-982b-4b54-82cc-c220922d7932"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n\nexpected2a = StructType([StructField(\"date\", DateType(), True),\n                         StructField(\"active_users\", LongType(), False)])\n\nresult2a = activeUsersDF.schema\n\nassert expected2a == result2a, \"activeUsersDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40e46f6b-c123-44a1-aa4a-d25144c77490"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected2b = [(datetime.date(2020, 6, 19), 251573), (datetime.date(2020, 6, 20), 357215), (datetime.date(2020, 6, 21), 305055), (datetime.date(2020, 6, 22), 239094), (datetime.date(2020, 6, 23), 243117)]\n\nresult2b = [(row.date, row.active_users) for row in activeUsersDF.take(5)]\n\nassert expected2b == result2b, \"activeUsersDF does not have the expected values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80896dbe-f426-40f9-96ac-80307d41dda4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Get average number of active users by day of week\n- Add **`day`** column by extracting day of week from **`date`** using a datetime pattern string\n- Group by **`day`**\n- Aggregate average of **`active_users`** and alias to \"avg_users\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c42bd552-eb8a-4cfa-ab35-00c107342898"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import date_format, avg\n\nactiveDowDF = (activeUsersDF\n               .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n               .groupBy(\"day\")\n               .agg(avg(col(\"active_users\")).alias(\"avg_users\"))\n              )\ndisplay(activeDowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54028e3e-6a86-4ff5-a822-a5d22bbb40f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5acf6a28-f942-4e65-9799-cc4c9b7d3b7d"}}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n\nexpected3a = StructType([StructField(\"day\", StringType(), True),\n                         StructField(\"avg_users\", DoubleType(), True)])\n\nresult3a = activeDowDF.schema\n\nassert expected3a == result3a, \"activeDowDF does not have the expected schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3662fca7-36be-41ca-8969-e5386c415dfa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["expected3b = [(\"Fri\", 247180.66666666666), (\"Mon\", 238195.5), (\"Sat\", 278482.0), (\"Sun\", 282905.5), (\"Thu\", 264620.0), (\"Tue\", 260942.5), (\"Wed\", 227214.0)]\n\nresult3b = [(row.day, row.avg_users) for row in activeDowDF.sort(\"day\").collect()]\n\nassert expected3b == result3b, \"activeDowDF does not have the expected values\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29d7d75d-2c04-4b5c-83e2-e5981adee95f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6976342f-4f9a-4dc4-a559-d8d332430e8e"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e1ae97b-8040-4962-9f82-f374ebbb2d22"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce833742-5525-467a-acb1-4c2124827cb0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.2 - Datetimes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399331}},"nbformat":4,"nbformat_minor":0}
