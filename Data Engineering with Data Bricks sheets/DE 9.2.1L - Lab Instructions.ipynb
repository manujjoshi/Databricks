{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f04c3998-f4af-4ee7-9cfe-109de69f0611"}}},{"cell_type":"markdown","source":["# Lab: Orchestrating Jobs with Databricks\n\nIn this lab, you'll be configuring a multi-task job comprising of:\n* A notebook that lands a new batch of data in a storage directory\n* A Delta Live Table pipeline that processes this data through a series of tables\n* A notebook that queries the gold table produced by this pipeline as well as various metrics output by DLT\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Schedule a notebook as a Databricks Job\n* Schedule a DLT pipeline as a Databricks Job\n* Configure linear dependencies between tasks using the Databricks Jobs UI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2270bd25-2049-42eb-89fb-5fdeebba9b6f"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-9.2.1L"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5a7f559-5d91-4aa9-bf49-b53321c67817"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Land Initial Data\nSeed the landing zone with some data before proceeding. You will re-run this command to land additional data later."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2995cd8f-2250-4b35-801b-93cc5e3e02f8"}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ff832ae-81f1-49b7-a3d1-681b642b63da"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create and Configure a Pipeline\n\nThe pipeline we create here is nearly identical to the one in the previous unit.\n\nWe will use it as part of a scheduled job in this lesson.\n\nExecute the following cell to print out the values that will be used during the following configuration steps."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59ffeda0-64db-4d01-b8b6-172d39f2ffa8"}}},{"cell_type":"code","source":["print_pipeline_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2591de7-1af3-4891-9a84-a3e76b4226c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Steps:\n1. Click the **Jobs** button on the sidebar.\n1. Select the **Delta Live Tables** tab.\n1. Click **Create Pipeline**.\n1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n1. For **Notebook Libraries**, use the navigator to locate and select the companion notebook called **DE 9.2.3L - DLT Job**.\n    * Alternatively, you can copy the **Notebook Path** specified above and paste it into the field provided.\n1. Configure the Source\n    * Click **`Add configuration`**\n    * Enter the word **`source`** in the **Key** field\n    * Enter the **Source** value specified above to the **`Value`** field\n1. In the **Target** field, specify the database name printed out next to **Target** in the cell above.<br/>\nThis should follow the pattern **`dbacademy_<username>_dewd_jobs_lab_92`**\n1. In the **Storage location** field, copy the directory as printed above.\n1. For **Pipeline Mode**, select **Triggered**\n1. Uncheck the **Enable autoscaling** box\n1. Set the number of workers to **`1`** (one)\n1. Click **Create**.\n\n\n<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: we won't be executing this pipline directly as it will be executed by our job later in this lesson,<br/>\nbut if you want to test it real quick, you can click the **Start** button now."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b166c3e-7e02-4b31-8b3f-2fa0f478ade8"}}},{"cell_type":"markdown","source":["## Schedule a Notebook Job\n\nWhen using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by scheduling a single task.\n\nBefore we start run the following cell to get the values used in this step."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed073c07-968a-4a93-b05d-4ab5bbca57ab"}}},{"cell_type":"code","source":["print_job_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f969b041-f331-43b9-ad43-6abb98d51fac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here, we'll start by scheduling the notebook batch job.\n\nSteps:\n1. Navigate to the Jobs UI using the Databricks left side navigation bar.\n1. Click the blue **Create Job** button\n1. Configure the task:\n    1. Enter **Batch-Job** for the task name\n    1. Select the notebook **DE 9.2.2L - Batch Job** using the notebook picker\n    1. From the **Cluster** dropdown, under **Existing All Purpose Cluster**, select your cluster\n    1. Click **Create**\n1. In the top-left of the screen rename the job (not the task) from **`Batch-Job`** (the defaulted value) to the **Job Name** provided for you in the previous cell.    \n1. Click the blue **Run now** button in the top right to start the job to test the job real quick.\n\n<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all purpose cluster, you will get a warning about how this will be billed as all purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"638bdb4f-0b03-48df-a2dc-10d79bea4035"}}},{"cell_type":"markdown","source":["## Schedule a DLT Pipeline as a Task\n\nIn this step, we'll add a DLT pipeline to execute after the success of the task we configured at the start of this lesson.\n\nSteps:\n1. At the top left of your screen, click the **Tasks** tab if it is not already selected.\n1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n    1. Specify the **Task name** as **DLT-Pipeline**\n    1. From **Type**, select **`Delta Live Tables pipeline`**\n    1. Click the **Pipeline** field and select the DLT pipeline you configured previously<br/>\n    Note: The pipeline will start with **Jobs-Labs-92** and will end with your email address.\n    1. The **Depends on** field defaults to your previously defined task but may have renamed itself from the value **reset** that you specified previously to something like **Jobs-Lab-92-youremailaddress**.\n    1. Click the blue **Create task** button\n\nYou should now see a screen with 2 boxes and a downward arrow between them. \n\nYour **`Batch-Job`** task (possibly renamed to something like **Jobs-Labs-92-youremailaddress**) will be at the top, \nleading into your **`DLT-Pipeline`** task."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90d39edf-0c31-499d-bb1e-98429b5c5cd4"}}},{"cell_type":"markdown","source":["## Schedule an Additional Notebook Task\n\nAn additional notebook has been provided which queries some of the DLT metrics and the gold table defined in the DLT pipeline. \n\nWe'll add this as a final task in our job.\n\nSteps:\n1. At the top left of your screen, click the **Tasks** tab if it is not already selected.\n1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n    1. Specify the **Task name** as **Query-Results**\n    1. Leave the **Type** set to **Notebook**\n    1. Select the notebook **DE 9.2.4L - Query Results Job** using the notebook picker\n    1. Note that the **Depends on** field defaults to your previously defined task, **DLT-Pipeline**\n    1. From the **Cluster** dropdown, under **Existing All Purpose Cluster**, select your cluster\n    1. Click the blue **Create task** button\n\nClick the blue **Run now** button in the top right of the screen to run this job.\n\nFrom the **Runs** tab, you will be able to click on the start time for this run under the **Active runs** section and visually track task progress.\n\nOnce all your tasks have succeeded, review the contents of each task to confirm expected behavior."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87993484-5e97-4fc2-bd5f-888cb93c5b69"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3b1ce77-b00c-4a1c-9ead-68dbf042bb69"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 9.2.1L - Lab Instructions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562402949}},"nbformat":4,"nbformat_minor":0}
