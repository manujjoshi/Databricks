{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b685116-c6e3-4e3d-8701-848c90870d46"}}},{"cell_type":"markdown","source":["# SQL for Delta Live Tables\n\n**In the last lesson, we walked through the process of scheduling this notebook as a Delta Live Table (DLT) pipeline**. Now we'll explore the contents of this notebook to better understand the syntax used by Delta Live Tables.\n\n#### This notebook uses SQL to declare Delta Live Tables that together implement a simple multi-hop architecture based on a Databricks-provided example dataset loaded by default into Databricks workspaces.\n\n#### At its simplest, you can think of DLT SQL as a slight modification to traditional CTAS statements. DLT tables and views will always be preceded by the **`LIVE`** keyword.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* **Define tables and views with Delta Live Tables**\n* **Use SQL to incrementally ingest raw data with Auto Loader**\n* **Perform incremental reads on Delta tables with SQL**\n* **Update code and redeploy a pipeline**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f99e2927-2693-4c1d-8628-20747f5632d6"}}},{"cell_type":"markdown","source":["# Declare Bronze Layer Tables\n\nBelow we declare two tables implementing the bronze layer. This represents data in its rawest form, but captured in a format that can be retained indefinitely and queried with the performance and benefits that Delta Lake has to offer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17b6ad92-3eb4-4c99-b159-27f5d19ebe06"}}},{"cell_type":"markdown","source":["### sales_orders_raw\n\n**`sales_orders_raw`** ingests JSON data incrementally from the example dataset found in  */databricks-datasets/retail-org/sales_orders/*.\n\n#### Incremental processing via <a herf=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> (which uses the same processing model as Structured Streaming), requires the addition of the **`STREAMING`** keyword in the declaration as seen below. The **`cloud_files()`** method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:\n* The source location, as mentioned above\n* The source data format, which is JSON in this case\n* An arbitrarily sized array of optional reader options. In this case, we set **`cloudFiles.inferColumnTypes`** to **`true`**\n\nThe following declaration also demonstrates the declaration of additional table metadata (a comment and properties in this case) that would be visible to anyone exploring the data catalog."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"827682e0-1656-4a5d-a045-6398e4806564"}}},{"cell_type":"code","source":["%sql\nCREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\nCOMMENT \"The raw sales orders, ingested from /databricks-datasets.\"\nAS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c29d2e2c-1511-4974-8e81-9fde42b333b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### customers\n\n**`customers`** presents CSV customer data found in */databricks-datasets/retail-org/customers/*. This table will soon be used in a join operation to look up customer data based on sales records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3686886-41c7-45a8-9b13-e63864cc3049"}}},{"cell_type":"code","source":["%sql\nCREATE OR REFRESH STREAMING LIVE TABLE customers\nCOMMENT \"The customers buying finished products, ingested from /databricks-datasets.\"\nAS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\");"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77244c3e-428b-4c50-a203-db77eebfcfcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Declare Silver Layer Tables\n\nNow we declare tables implementing the silver layer. This layer represents a refined copy of data from the bronze layer, with the intention of optimizing downstream applications. At this level we apply operations like data cleansing and enrichment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7041c7d5-2d01-438a-b8d9-ebf2ce604560"}}},{"cell_type":"markdown","source":["### sales_orders_cleaned\n\nHere we declare our first silver table, which enriches the sales transaction data with customer information in addition to implementing quality control by rejecting records with a null order number.\n\nThis declaration introduces a number of new concepts.\n\n### Concepts:\n#### Quality Control\n\nThe **`CONSTRAINT`** keyword introduces quality control. Similar in function to a traditional **`WHERE`** clause, **`CONSTRAINT`** integrates with DLT, enabling it to collect metrics on constraint violations. Constraints provide an optional **`ON VIOLATION`** clause, specifying an action to take on records that violate the constraint. The three modes currently supported by DLT include:\n\n| **`ON VIOLATION`** | Behavior |\n| --- | --- |\n| **`FAIL UPDATE`** | Pipeline failure when constraint is violated |\n| **`DROP ROW`** | Discard records that violate constraints |\n| Omitted | Records violating constraints will be included (but violations will be reported in metrics) |\n\n#### References to DLT Tables and Views\nReferences to other DLT tables and views will always include the **`live.`** prefix. A target database name will automatically be substituted at runtime, allowing for easily migration of pipelines between DEV/QA/PROD environments.\n\n#### References to Streaming Tables\n\nReferences to streaming DLT tables use the **`STREAM()`**, supplying the table name as an argument."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2551978f-844d-41ca-b17c-d2fda9c6ffcf"}}},{"cell_type":"code","source":["%sql\nCREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n)\nCOMMENT \"The cleaned sales orders with valid order_number(s) and partitioned by order_datetime.\"\nAS\n  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n         date(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n         f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n  FROM STREAM(LIVE.sales_orders_raw) f\n  LEFT JOIN LIVE.customers c\n    ON c.customer_id = f.customer_id\n    AND c.customer_name = f.customer_name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a478809-8d01-4027-b7b6-88b54b058200"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Declare Gold Table\n\nAt the most refined level of the architecture, we declare a table delivering an aggregation with business value, in this case a collection of sales order data based in a specific region. In aggregating, the report generates counts and totals of orders by date and customer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fca008a-dee3-4ab2-88d3-428603dbf0e8"}}},{"cell_type":"code","source":["%sql\nCREATE OR REFRESH LIVE TABLE sales_order_in_la\nCOMMENT \"Sales orders in LA.\"\nAS\n  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n         sum(ordered_products_explode.price) as sales, \n         sum(ordered_products_explode.qty) as quantity, \n         count(ordered_products_explode.id) as product_count\n  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n        FROM LIVE.sales_orders_cleaned \n        WHERE city = 'Los Angeles')\n  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adcdca1b-5267-4c78-9177-daad0e63c07d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Explore Results\n\nExplore the DAG (Directed Acyclic Graph) representing the entities involved in the pipeline and the relationships between them. Click on each to view a summary, which includes:\n* Run status\n* Metadata summary\n* Schema\n* Data quality metrics\n\nRefer to this <a href=\"$./DE 8.3 - Pipeline Results\" target=\"_blank\">companion notebook</a> to inspect tables and logs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a32319ae-0f09-4044-9889-d0a889b87356"}}},{"cell_type":"markdown","source":["# Update Pipeline\n\nUncomment the following cell to declare another gold table. Similar to the previous gold table declaration, this filters for the **`city`** of Chicago. \n\nRe-run your pipeline to examine the updated results. \n\nDoes it run as expected? \n\nCan you identify any issues?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7189ed9-34c8-46aa-867c-9369fec5dddf"}}},{"cell_type":"code","source":["%sql\nCREATE OR REFRESH LIVE TABLE sales_order_in_chicago\nCOMMENT \"Sales orders in Chicago.\"\nAS\n  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n         sum(ordered_products_explode.price) as sales, \n         sum(ordered_products_explode.qty) as quantity, \n         count(ordered_products_explode.id) as product_count\n  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n        FROM LIVE.sales_orders_cleaned \n        WHERE city = 'Chicago')\n  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06cec86b-7a5b-48ff-aa71-b11e10a2a1f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"message","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1ea3cbd-99b0-4311-a075-e845312d600e"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 8.1.2 - SQL for Delta Live Tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562403107}},"nbformat":4,"nbformat_minor":0}
