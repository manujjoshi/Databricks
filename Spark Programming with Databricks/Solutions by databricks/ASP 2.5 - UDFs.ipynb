{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07e81974-d8de-4a80-b026-b294e87ef9a6"}}},{"cell_type":"markdown","source":["# User-Defined Functions\n\n##### Objectives\n1. Define a function\n1. Create and apply a UDF\n1. Register the UDF to use in SQL\n1. Create and register a UDF with Python decorator syntax\n1. Create and apply a Pandas (vectorized) UDF\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.udf.html?#pyspark.sql.functions.udf\" target=\"_blank\">UDF Registration (`spark.udf`)</a>: `register`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>: `udf`\n- <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-python.html#use-udf-with-dataframes\" target=\"_blank\">Python UDF Decorator</a>: `@udf`\n- <a href=\"https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html#pandas-user-defined-functions\" target=\"_blank\">Pandas UDF Decorator</a>: `@pandas_udf`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec67b2a0-6646-41a0-9c36-a6c2ac9e900f"}}},{"cell_type":"markdown","source":["### User-Defined Function (UDF)\nA custom column transformation function\n\n- Canâ€™t be optimized by Catalyst Optimizer\n- Function is serialized and sent to executors\n- Row data is deserialized from Spark's native binary format to pass to the UDF, and the results are serialized back into Spark's native format\n- For Python UDFs, additional interprocess communication overhead between the executor and a Python interpreter running on each worker node"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a3fcd46-e258-4e5c-bab4-b7ffc40e3d09"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db77a66d-45a9-4aaf-8314-6e9d32c88851"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For this demo, we're going to use the sales data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8abdf360-72f0-482c-a0ce-5e79958be534"}}},{"cell_type":"code","source":["salesDF = spark.read.parquet(salesPath)\ndisplay(salesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40e39ef9-e71e-43c0-97fb-3ffd81883c16"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Define a function\n\nDefine a function (on the driver) to get the first letter of a string from the `email` field."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f481efeb-8914-4c68-9848-9cd2465776ef"}}},{"cell_type":"code","source":["def firstLetterFunction(email):\n    return email[0]\n\nfirstLetterFunction(\"annagray@kaufman.com\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26232234-16cb-4cdb-adee-1dbeefa58416"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Create and apply UDF\nRegister the function as a UDF. This serializes the function and sends it to executors to be able to transform DataFrame records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45da0721-f969-47c2-a22b-85948279df28"}}},{"cell_type":"code","source":["firstLetterUDF = udf(firstLetterFunction)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22a140c8-9a7f-44a9-8e27-6b75a270ea1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Apply the UDF on the `email` column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edeb424d-7986-450b-bcba-652b1956d0b5"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndisplay(salesDF.select(firstLetterUDF(col(\"email\"))))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a552a4-e239-44af-9d58-b0db8bf7e7bc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Register UDF to use in SQL\nRegister the UDF using `spark.udf.register` to also make it available for use in the SQL namespace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfa4dad8-5e5d-4e94-ab48-1ae0517c1565"}}},{"cell_type":"code","source":["salesDF.createOrReplaceTempView(\"sales\")\n\nfirstLetterUDF = spark.udf.register(\"sql_udf\", firstLetterFunction)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fcc3f9a-504c-4e6b-b062-0607e306a615"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# You can still apply the UDF from Python\ndisplay(salesDF.select(firstLetterUDF(col(\"email\"))))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19e920bf-2367-429b-a028-1d468075a33d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- You can now also apply the UDF from SQL\nSELECT sql_udf(email) AS firstLetter FROM sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07718310-15ab-4c8e-a9f4-0a70fcd40cc5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use Decorator Syntax (Python Only)\n\nAlternatively, you can define and register a UDF using <a href=\"https://realpython.com/primer-on-python-decorators/\" target=\"_blank\">Python decorator syntax</a>. The `@udf` decorator parameter is the Column datatype the function returns.\n\nYou will no longer be able to call the local Python function (i.e., `firstLetterUDF(\"annagray@kaufman.com\")` will not work).\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This example also uses <a href=\"https://docs.python.org/3/library/typing.html\" target=\"_blank\">Python type hints</a>, which were introduced in Python 3.5. Type hints are not required for this example, but instead serve as \"documentation\" to help developers use the function correctly. They are used in this example to emphasize that the UDF processes one record at a time, taking a single `str` argument and returning a `str` value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3a3ba1f-8471-47eb-97be-fd649cbf2084"}}},{"cell_type":"code","source":["# Our input/output is a string\n@udf(\"string\")\ndef firstLetterUDF(email: str) -> str:\n    return email[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54bf343c-e5f5-48d4-9f34-6565937ef1b2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And let's use our decorator UDF here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b4b253-4957-47b8-b27f-2b558e55c5d4"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nsalesDF = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\ndisplay(salesDF.select(firstLetterUDF(col(\"email\"))))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff39f5b4-ad6b-4993-a45d-e01de945ce70"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pandas/Vectorized UDFs\n\nAs of Spark 2.3, there are Pandas UDFs available in Python to improve the efficiency of UDFs. Pandas UDFs utilize Apache Arrow to speed up computation.\n\n* <a href=\"https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\" target=\"_blank\">Blog post</a>\n* <a href=\"https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html?highlight=arrow\" target=\"_blank\">Documentation</a>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\" alt=\"Benchmark\" width =\"500\" height=\"1500\">\n\nThe user-defined functions are executed using: \n* <a href=\"https://arrow.apache.org/\" target=\"_blank\">Apache Arrow</a>, an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost\n* Pandas inside the function, to work with Pandas instances and APIs\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> As of Spark 3.0, you should **always** define your Pandas UDF using Python type hints."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10452255-be6b-4ff7-93ba-7f2580d1e35c"}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n# We have a string input/output\n@pandas_udf(\"string\")\ndef vectorizedUDF(email: pd.Series) -> pd.Series:\n    return email.str[0]\n\n# Alternatively\n# def vectorizedUDF(email: pd.Series) -> pd.Series:\n#     return email.str[0]\n# vectorizedUDF = pandas_udf(vectorizedUDF, \"string\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"571134ae-630f-4341-abec-40ab64468c4b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(salesDF.select(vectorizedUDF(col(\"email\"))))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a992d40-246a-4ae7-bc81-81f099e98fc9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can also register these Pandas UDFs to the SQL namespace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ee31809-c30d-421d-804d-f38f4bc9a03f"}}},{"cell_type":"code","source":["spark.udf.register(\"sql_vectorized_udf\", vectorizedUDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1403cf58-76f2-4887-918b-df5afe8ed6f8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- Use the Pandas UDF from SQL\nSELECT sql_vectorized_udf(email) AS firstLetter FROM sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77c4c94a-d611-4cd9-81f2-c12b01125294"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Sort Day Lab\n\n##### Tasks\n1. Define a UDF to label the day of week\n1. Apply the UDF to label and sort by day of week\n1. Plot active users by day of week as a bar graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e672218-80bb-4042-8907-248e78f0d68b"}}},{"cell_type":"markdown","source":["Start with a DataFrame of the average number of active users by day of week.\n\nThis was the resulting `df` in a previous lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dedec924-4e3f-4a43-be51-ed767337bb6d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n\ndf = (spark\n      .read\n      .parquet(eventsPath)\n      .withColumn(\"ts\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n      .withColumn(\"date\", to_date(\"ts\"))\n      .groupBy(\"date\").agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n      .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n      .groupBy(\"day\").agg(avg(col(\"active_users\")).alias(\"avg_users\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"766c4c40-32ca-4722-833b-9520e6fe0998"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Define UDF to label day of week\n\nUse the **`labelDayOfWeek`** function provided below to create the UDF **`labelDowUDF`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47b83883-7985-409f-9a06-1f272373926f"}}},{"cell_type":"code","source":["def labelDayOfWeek(day: str) -> str:\n    dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\",\n           \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n    return dow.get(day) + \"-\" + day"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85c71c25-db71-47ca-b390-3c2e0f75d39c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nlabelDowUDF = spark.udf.register(\"labelDow\", labelDayOfWeek)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b180d6f-231c-4815-b516-b9387640626d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Apply UDF to label and sort by day of week\n- Update the **`day`** column by applying the UDF and replacing this column\n- Sort by **`day`**\n- Plot as a bar graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2460584e-5816-40ae-9938-281314079531"}}},{"cell_type":"code","source":["# ANSWER\nfinalDF = (df\n           .withColumn(\"day\", labelDowUDF(col(\"day\")))\n           .sort(\"day\")\n          )\ndisplay(finalDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdcf04fe-4639-4b73-9d8b-6de1a349997e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4420e1d3-8765-40a2-8063-82c5f3100c21"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8207fb11-d714-4dc2-a58b-d45f4bc22c81"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2897724c-7052-49b0-b080-d6846044e030"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.5 - UDFs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399376}},"nbformat":4,"nbformat_minor":0}
