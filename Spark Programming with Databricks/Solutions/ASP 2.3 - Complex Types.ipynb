{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd4040c7-7d23-419c-aaa4-be55d569ffba"}}},{"cell_type":"markdown","source":["# Complex Types\n\nExplore built-in functions for working with collections and strings.\n\n##### Objectives\n1. Apply collection functions to process arrays\n1. Union DataFrames together\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>: `unionByName`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html?#functions\" target=\"_blank\">Built-In Functions</a>:\n  - Aggregate: `collect_set`\n  - Collection: `array_contains`, `element_at`, `explode`\n  - String: `split`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"166aa6ef-e700-4473-8804-705cfd51c1be"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bc8cf95-7ffd-431b-a1af-3a016344be05"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In this demo, we're going to use the sales data set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac646a35-0b98-4a66-adb9-f3f0b571ac10"}}},{"cell_type":"code","source":["df = spark.read.parquet(salesPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b555a3c5-342b-4efa-84be-55038cf4a72d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### String Functions\nHere are some of the built-in functions available for manipulating strings.\n\n| Method | Description |\n| --- | --- |\n| translate | Translate any character in the src by a character in replaceString |\n| regexp_replace | Replace all substrings of the specified string value that match regexp with rep |\n| regexp_extract | Extract a specific group matched by a Java regex, from the specified string column |\n| ltrim | Removes the leading space characters from the specified string column |\n| lower | Converts a string column to lowercase |\n| split | Splits str around matches of the given pattern |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98ed9b24-ea1f-43f7-ae64-a7a3491ce5b2"}}},{"cell_type":"markdown","source":["### Collection Functions\n\nHere are some of the built-in functions available for working with arrays.\n\n| Method | Description |\n| --- | --- |\n| array_contains | Returns null if the array is null, true if the array contains value, and false otherwise. |\n| element_at | Returns element of array at given index. Array elements are numbered starting with **1**. |\n| explode | Creates a new row for each element in the given array or map column. |\n| collect_set | Returns a set of objects with duplicate elements eliminated. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8323edd4-305a-4cdf-b8d9-4f94a2432000"}}},{"cell_type":"markdown","source":["### Aggregate Functions\n\nHere are some of the built-in aggregate functions available for creating arrays, typically from GroupedData.\n\n| Method | Description |\n| --- | --- |\n| collect_list | Returns an array consisting of all values within the group. |\n| collect_set | Returns an array consisting of all unique values within the group. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67f50a76-79f2-41cf-8722-28d20d915fc6"}}},{"cell_type":"markdown","source":["# User Purchases\n\nList all size and quality options purchased by each buyer.\n1. Extract item details from purchases\n2. Extract size and quality options from mattress purchases\n3. Extract size and quality options from pillow purchases\n4. Combine data for mattress and pillows\n5. List all size and quality options bought by each user"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4bfa563-ceb2-43c2-b0df-158f4dd439d4"}}},{"cell_type":"markdown","source":["### 1. Extract item details from purchases\n\n- Explode the **`items`** field in **`df`** with the results replacing the existing **`items`** field\n- Select the **`email`** and **`item.item_name`** fields\n- Split the words in **`item_name`** into an array and alias the column to \"details\"\n\nAssign the resulting DataFrame to **`detailsDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd98cf3a-e9c9-4090-b91f-4ba7cf29d653"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndetailsDF = (df\n             .withColumn(\"items\", explode(\"items\"))\n             .select(\"email\", \"items.item_name\")\n             .withColumn(\"details\", split(col(\"item_name\"), \" \"))\n            )\ndisplay(detailsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7247e712-381a-4051-822b-d8b4b61bda5c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["So you can see that our **`details`** column is now an array containing the quality, size, and object type."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ae85486-1bce-4ce1-8b85-a150cbdc6d15"}}},{"cell_type":"markdown","source":["### 2. Extract size and quality options from mattress purchases\n\n- Filter **`detailsDF`** for records where **`details`** contains \"Mattress\"\n- Add a **`size`** column by extracting the element at position 2\n- Add a **`quality`** column by extracting the element at position 1\n\nSave the result as **`mattressDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c11edf17-e4ec-4b14-8cc9-6e36a088edbe"}}},{"cell_type":"code","source":["mattressDF = (detailsDF\n              .filter(array_contains(col(\"details\"), \"Mattress\"))\n              .withColumn(\"size\", element_at(col(\"details\"), 2))\n              .withColumn(\"quality\", element_at(col(\"details\"), 1))\n             )\ndisplay(mattressDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e616b3d5-744d-4ece-b637-3030e81db915"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next we're going to do the same thing for pillow purchases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5ffc444-14a5-42ac-aecf-6771a2df539f"}}},{"cell_type":"markdown","source":["### 3. Extract size and quality options from pillow purchases\n- Filter **`detailsDF`** for records where **`details`** contains \"Pillow\"\n- Add a **`size`** column by extracting the element at position 1\n- Add a **`quality`** column by extracting the element at position 2\n\nNote the positions of **`size`** and **`quality`** are switched for mattresses and pillows.\n\nSave result as **`pillowDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bcd0237-a037-4e81-8ea4-004feb0e6a74"}}},{"cell_type":"code","source":["pillowDF = (detailsDF\n            .filter(array_contains(col(\"details\"), \"Pillow\"))\n            .withColumn(\"size\", element_at(col(\"details\"), 1))\n            .withColumn(\"quality\", element_at(col(\"details\"), 2))\n           )\ndisplay(pillowDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92648418-e5c7-46b5-b29e-458e45dee429"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Combine data for mattress and pillows\n\n- Perform a union on **`mattressDF`** and **`pillowDF`** by column names\n- Drop the **`details`** column\n\nSave the result as **`unionDF`**.\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> The DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html\" target=\"_blank\">`union`</a> method resolves columns by position, as in standard SQL. You should use it only if the two DataFrames have exactly the same schema, including the column order. In contrast, the DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html\" target=\"_blank\">`unionByName`</a> method resolves columns by name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db85ea3d-4492-428a-8367-c4c660084138"}}},{"cell_type":"code","source":["unionDF = mattressDF.unionByName(pillowDF).drop(\"details\")\ndisplay(unionDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33ecc09c-6201-46c0-9bf0-fe596594c135"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. List all size and quality options bought by each user\n\n- Group rows in **`unionDF`** by **`email`**\n  - Collect the set of all items in **`size`** for each user and alias the column to \"size options\"\n  - Collect the set of all items in **`quality`** for each user and alias the column to \"quality options\"\n\nSave the result as **`optionsDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63bcabc3-f2e5-443f-b693-8fc4f9ab0481"}}},{"cell_type":"code","source":["optionsDF = (unionDF\n             .groupBy(\"email\")\n             .agg(collect_set(\"size\").alias(\"size options\"),\n                  collect_set(\"quality\").alias(\"quality options\"))\n            )\ndisplay(optionsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aa97b70-027c-4741-b44f-0e2e81c72ada"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom\n\nAnd lastly, we'll clean up the classroom."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b107185-b8b0-4b98-9d65-12166e346545"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbb13107-5a77-43fc-87e7-3cd827dc2da4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a21a30-ecb1-44d4-b9bb-28d510a66de0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.3 - Complex Types","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399608}},"nbformat":4,"nbformat_minor":0}
