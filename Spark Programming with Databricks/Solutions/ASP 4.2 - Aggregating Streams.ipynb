{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99ada6c8-6fb1-4082-9c5a-2232eef2433c"}}},{"cell_type":"markdown","source":["# Aggregating Streams\n\n##### Objectives\n1. Add watermarking\n1. Aggregate with windows\n1. Display streaming query results\n1. Monitor streaming queries\n\n##### Classes\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html#pyspark.sql.streaming.DataStreamReader\" target=\"_blank\">DataStreamReader</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html#pyspark.sql.streaming.DataStreamWriter\" target=\"_blank\">DataStreamWriter</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery\" target=\"_blank\">StreamingQuery</a>\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.StreamingQueryManager.html#pyspark.sql.streaming.StreamingQueryManager\" target=\"_blank\">StreamingQueryManager</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51cc0c8-ea7f-4363-853f-69f3dfd05be5"}}},{"cell_type":"markdown","source":["## Hourly Activity by Traffic Lab\nProcess streaming data to display the total active users by traffic source with a 1 hour window.\n1. Cast to timestamp and add watermark for 2 hours\n2. Aggregate active users by traffic source for 1 hour windows\n3. Execute query with `display` and plot results\n5. Use query name to stop streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02a16960-38ad-43f7-9a37-7ac50593568d"}}},{"cell_type":"markdown","source":["### Setup\nRun the cells below to generate hourly JSON files of event data for July 3, 2020."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4dfd9e-a4f5-41b4-bc60-0d2713176790"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b3d733b-6b8d-4deb-98da-39e1c3fef658"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\n# Directory of hourly events logged from the BedBricks website on July 3, 2020\nhourlyEventsPath = \"/mnt/training/ecommerce/events/events-2020-07-03.json\"\n\ndf = (spark\n      .readStream\n      .schema(schema)\n      .option(\"maxFilesPerTrigger\", 1)\n      .json(hourlyEventsPath)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7febf6d2-0694-400b-9283-6d6cf4e90871"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Cast to timestamp and add watermark for 2 hours\n- Add a **`createdAt`** column by dividing **`event_timestamp`** by 1M and casting to timestamp\n- Set a watermark of 2 hours on the **`createdAt`** column\n\nAssign the resulting DataFrame to **`eventsDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec1f60bc-86f0-4599-bd13-b8f9c10f93fe"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\neventsDF = (df\n            .withColumn(\"createdAt\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n            .withWatermark(\"createdAt\", \"2 hours\")\n           )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8619f545-0328-4dd4-a829-69421695a4c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90c8ab0a-de8a-4a2b-8c49-4f34d2ed9ed9"}}},{"cell_type":"code","source":["assert \"StructField(createdAt,TimestampType,true\" in str(eventsDF.schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1fbf213-77ba-4c49-b569-1492d983c014"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Aggregate active users by traffic source for 1 hour windows\n- Set the default shuffle partitions to the number of cores on your cluster (not required, but runs faster)\n- Group by **`traffic_source`** with 1-hour tumbling windows based on the **`createdAt`** column\n- Aggregate the approximate count of distinct users per **`traffic_source`** and hour, and alias the column to \"active_users\"\n- Select **`traffic_source`**, **`active_users`**, and the **`hour`** extracted from **`window.start`** with an alias of \"hour\"\n- Sort by **`hour`** in ascending order\n\nAssign the resulting DataFrame to **`trafficDF`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e6f4bd6-a836-4324-b375-f32c697bf576"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import approx_count_distinct, hour, window\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n\ntrafficDF = (eventsDF\n             .groupBy(\"traffic_source\", window(col(\"createdAt\"), \"1 hour\"))\n             .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n             .select(col(\"traffic_source\"), col(\"active_users\"), hour(col(\"window.start\")).alias(\"hour\"))\n             .sort(\"hour\")\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89382ef0-969b-4423-9af1-88d7e54cdc0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dc38fd6-24db-4717-b53f-688e87cdfd97"}}},{"cell_type":"code","source":["assert str(trafficDF.schema) == \"StructType(List(StructField(traffic_source,StringType,true),StructField(active_users,LongType,false),StructField(hour,IntegerType,true)))\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2eb4909-98b9-45d9-a786-5d593ff6424f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Execute query with display() and plot results\n- Use `display` to start **`trafficDF`** as a streaming query and display the resulting memory sink\n  - Assign \"hourly_traffic\" as the name of the query by seting the **`streamName`** parameter of `display`\n- Plot the streaming query results as a bar graph\n- Configure the following plot options:\n  - Keys: **`hour`**\n  - Series groupings: **`traffic_source`**\n  - Values: **`active_users`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11e3b8ac-54ee-49d5-93a2-63982b725f1a"}}},{"cell_type":"code","source":["# ANSWER\ndisplay(trafficDF, streamName=\"hourly_traffic\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4495878b-a25f-4468-bba1-f8a0f5fc4ae4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**\n\n- The bar chart should plot `hour` on the x-axis and `active_users` on the y-axis\n- Six bars should appear at every hour for all traffic sources\n- The chart should stop at hour 23"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24be2085-1161-405f-8cca-0a8da342a3d2"}}},{"cell_type":"markdown","source":["### 4. Manage streaming query\n- Iterate over SparkSession's list of active streams to find one with name \"hourly_traffic\"\n- Stop the streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdfc4184-02ce-41fc-a66c-4ae3cde38166"}}},{"cell_type":"code","source":["# ANSWER\nuntilStreamIsReady(\"hourly_traffic\")\n\nfor s in spark.streams.active:\n    if s.name == \"hourly_traffic\":\n        s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d2fcfc0-ed0a-4e97-ae63-8608dc8382e9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["%md **CHECK YOUR WORK**  \nPrint all active streams to check that \"hourly_traffic\" is no longer there"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ab9bf10-dda5-449c-a6e3-165daadbaef1"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(s.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c86ecb29-695c-43c4-8db9-a1123aa10806"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Classroom Cleanup\nRun the cell below to clean up resources."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96e5e386-9d8f-467c-bafe-19ba969965db"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f66799a3-4217-471a-8651-55c596816bc2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ca44c72-e9d1-42a2-9c50-b5c73a3c3272"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 4.2 - Aggregating Streams","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399543}},"nbformat":4,"nbformat_minor":0}
