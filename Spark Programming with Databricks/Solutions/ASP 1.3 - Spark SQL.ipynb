{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"089f5d27-4f40-4b7b-9b5e-5441ed998fdb"}}},{"cell_type":"markdown","source":["# Spark SQL\n\nDemonstrate fundamental concepts in Spark SQL using the DataFrame API.\n\n##### Objectives\n1. Run a SQL query\n1. Create a DataFrame from a table\n1. Write the same query using DataFrame transformations\n1. Trigger computation with DataFrame actions\n1. Convert between DataFrames and SQL\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-session-apis\" target=\"_blank\">SparkSession</a>: `sql`, `table`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a>:\n  - Transformations:  `select`, `where`, `orderBy`\n  - Actions: `show`, `count`, `take`\n  - Other methods: `printSchema`, `schema`, `createOrReplaceTempView`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04d32618-1177-48c1-812b-17441ce2b665"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup-SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe97c8fc-bf75-4855-ba0f-ca836d4f9c37"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Multiple Interfaces\nSpark SQL is a module for structured data processing with multiple interfaces.  \n\nWe can interact with Spark SQL in two ways:\n1. Executing SQL queries\n1. Working with the DataFrame API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b80c1659-3548-49f1-acbb-dbd1d488ba94"}}},{"cell_type":"markdown","source":["**Method 1: Executing SQL queries**  \n\nThis is how we interacted with Spark SQL in the previous lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9b1fe22-a84c-46b2-a9a7-5494b696d41e"}}},{"cell_type":"code","source":["%sql\nSELECT name, price\nFROM products\nWHERE price < 200\nORDER BY price"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"574d7915-bcee-47ee-9e4d-188283b1e2f3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Method 2: Working with the DataFrame API**\n\nWe can also express Spark SQL queries using the DataFrame API.  \nThe following cell returns a DataFrame containing the same results as those retrieved above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e292c5ff-2afc-4259-a2f8-c9d4874916c1"}}},{"cell_type":"code","source":["display(spark.table(\"products\")\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cb01618-b870-43cb-88cd-bb5a35f031b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll go over the syntax for the DataFrame API later in the lesson, but you can see this builder design pattern allows us to chain a sequence of operations very similar to those we find in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d37a1db4-8af6-4ef5-8741-99531a2e66ab"}}},{"cell_type":"markdown","source":["## Query Execution\nWe can express the same query using any interface. The Spark SQL engine generates the same query plan used to optimize and execute on our Spark cluster.\n\n![query execution engine](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Resilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\">code manipulating RDDs directly</a>. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c7f21cc-689b-404c-86f1-6513619e4ed2"}}},{"cell_type":"markdown","source":["## Spark API Documentation\n\nTo learn how we work with DataFrames in Spark SQL, let's first look at the Spark API documentation.  \nThe main Spark [documentation](https://spark.apache.org/docs/latest/) page includes links to API docs and helpful guides for each version of Spark.  \n\nThe [Scala API](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html) and [Python API](https://spark.apache.org/docs/latest/api/python/index.html) are most commonly used, and it's often helpful to reference the documentation for both languages.  \nScala docs tend to be more comprehensive, and Python docs tend to have more code examples.\n\n#### Navigating Docs for the Spark SQL Module\nFind the Spark SQL module by navigating to `org.apache.spark.sql` in the Scala API or `pyspark.sql` in the Python API.  \nThe first class we'll explore in this module is the `SparkSession` class. You can find this by entering \"SparkSession\" in the search bar."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bba3e53b-9c76-4029-b394-3fcdeb12279d"}}},{"cell_type":"markdown","source":["## SparkSession\nThe `SparkSession` class is the single entry point to all functionality in Spark using the DataFrame API. \n\nIn Databricks notebooks, the SparkSession is created for you, stored in a variable called `spark`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9eed9984-c966-4f91-bfd4-86e6a6f0069f"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead54e12-c0ad-483b-ae41-a0ebbd686d94"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The example from the beginning of this lesson used the SparkSession method `table` to create a DataFrame from the `products` table. Let's save this in the variable `productsDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78392556-a3a3-4d59-b212-7c912fb6bf0d"}}},{"cell_type":"code","source":["productsDF = spark.table(\"products\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"273864d2-24e6-4258-b521-ad9994d262fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below are several additional methods we can use to create DataFrames. All of these can be found in the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html\" target=\"_blank\">documentation</a> for `SparkSession`.\n\n#### `SparkSession` Methods\n| Method | Description |\n| --- | --- |\n| sql | Returns a DataFrame representing the result of the given query | \n| table | Returns the specified table as a DataFrame |\n| read | Returns a DataFrameReader that can be used to read data in as a DataFrame |\n| range | Create a DataFrame with a column containing elements in a range from start to end (exclusive) with step value and number of partitions |\n| createDataFrame | Creates a DataFrame from a list of tuples, primarily used for testing |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"140994a2-7040-4b3c-8c34-4a678672158f"}}},{"cell_type":"markdown","source":["Let's use a SparkSession method to run SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f98aff1-a343-4196-96d4-890f21cd9341"}}},{"cell_type":"code","source":["resultDF = spark.sql(\"\"\"\nSELECT name, price\nFROM products\nWHERE price < 200\nORDER BY price\n\"\"\")\n\ndisplay(resultDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd8115cb-7ff3-4686-8a0f-36e7df825115"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## DataFrames\nRecall that expressing our query using methods in the DataFrame API returns results in a DataFrame. Let's store this in the variable `budgetDF`.\n\nA **DataFrame** is a distributed collection of data grouped into named columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b96ba32b-239c-42ad-b54a-22069025f0f6"}}},{"cell_type":"code","source":["budgetDF = (spark.table(\"products\")\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"494956e2-98e8-443a-9cc8-0402ae728064"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can use `display()` to output the results of a dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c51d26cd-6883-4b98-b5bf-9e7bc97026f1"}}},{"cell_type":"code","source":["display(budgetDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e9569a2-5d0d-47e0-a107-768da6cffd96"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The **schema** defines the column names and types of a dataframe.\n\nAccess a dataframe's schema using the `schema` attribute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"370d1f29-fe0f-4d5e-a283-eff1c0deab3c"}}},{"cell_type":"code","source":["budgetDF.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70b46429-85f6-4d52-819e-fc3ca0c015ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["View a nicer output for this schema using the `printSchema()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0461f4c7-47c7-4963-a04f-451afe0ba882"}}},{"cell_type":"code","source":["budgetDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0a304d0-cf29-4357-9b4b-8aabebffcd45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Transformations\nWhen we created `budgetDF`, we used a series of DataFrame transformation methods e.g. `select`, `where`, `orderBy`. \n\n```\nproductsDF\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\")\n```\nTransformations operate on and return DataFrames, allowing us to chain transformation methods together to construct new DataFrames.  \nHowever, these operations can't execute on their own, as transformation methods are **lazily evaluated**. \n\nRunning the following cell does not trigger any computation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"338f4548-4d95-4861-8fcf-6c516ccbddc3"}}},{"cell_type":"code","source":["(productsDF\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74726ab4-da5d-4ae9-a236-c7ba21651df5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Actions\nConversely, DataFrame actions are methods that **trigger computation**.  \nActions are needed to trigger the execution of any DataFrame transformations. \n\nThe `show` action causes the following cell to execute transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1111ac9-c14a-44ab-9768-54de77404ef7"}}},{"cell_type":"code","source":["(productsDF\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\")\n  .show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"262bf3f2-0b26-4a53-9827-63589d1b5460"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below are several examples of <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis\" target=\"_blank\">DataFrame</a> actions.\n\n### DataFrame Action Methods\n| Method | Description |\n| --- | --- |\n| show | Displays the top n rows of DataFrame in a tabular form |\n| count | Returns the number of rows in the DataFrame |\n| describe,  summary | Computes basic statistics for numeric and string columns |\n| first, head | Returns the the first row |\n| collect | Returns an array that contains all rows in this DataFrame |\n| take | Returns an array of the first n rows in the DataFrame |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e21d4e0-b92a-4472-9e9c-d1f015ba1c3a"}}},{"cell_type":"markdown","source":["`count` returns the number of records in a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2db83b0b-f405-4856-9efa-2368ae998d80"}}},{"cell_type":"code","source":["budgetDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7026ef16-154e-4335-9996-b1cb7016bd2a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["`collect` returns an array of all rows in a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc2f00ea-ceb6-43cf-a6d9-dd9d46ad1687"}}},{"cell_type":"code","source":["budgetDF.collect() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"793172e7-e68b-43ed-bde2-2ea7a2a42544"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Convert between DataFrames and SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d5c681b-2ce5-4d25-9f85-6a5fb75043c2"}}},{"cell_type":"markdown","source":["`createOrReplaceTempView` creates a temporary view based on the DataFrame. The lifetime of the temporary view is tied to the SparkSession that was used to create the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67e0fe0f-1ef0-436a-987f-fe8536fb38d5"}}},{"cell_type":"code","source":["budgetDF.createOrReplaceTempView(\"budget\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"769bc3a9-3ae0-466d-baf3-18a9aed0cf70"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM budget\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fc4e39f-0c01-46e4-9e28-e86436a64462"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Spark SQL Lab\n\n##### Tasks\n1. Create a DataFrame from the `events` table\n1. Display the DataFrame and inspect its schema\n1. Apply transformations to filter and sort `macOS` events\n1. Count results and take the first 5 rows\n1. Create the same DataFrame using a SQL query\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html?highlight=sparksession\" target=\"_blank\">SparkSession</a>: `sql`, `table`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a> transformations: `select`, `where`, `orderBy`\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a> actions: `select`, `count`, `take`\n- Other <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\" target=\"_blank\">DataFrame</a> methods: `printSchema`, `schema`, `createOrReplaceTempView`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05888f43-7d98-40cf-ae7e-b7f864f7a698"}}},{"cell_type":"markdown","source":["### 1. Create a DataFrame from the `events` table\n- Use SparkSession to create a DataFrame from the `events` table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"016f2eed-9750-4fc9-90d5-f273be04d8fb"}}},{"cell_type":"code","source":["# ANSWER\neventsDF = spark.table(\"events\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bc2522f-c572-4080-be4e-9c8e1b25dcb2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Display DataFrame and inspect schema\n- Use methods above to inspect DataFrame contents and schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3638bedd-6496-4140-8688-a3c7253c8e07"}}},{"cell_type":"code","source":["# ANSWER\ndisplay(eventsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ac7b1cd-45fa-40f3-b05d-015f37273041"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\neventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d27c1d2e-4f06-4163-b921-fdd9ad3e3ac6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Apply transformations to filter and sort `macOS` events\n- Filter for rows where `device` is `macOS`\n- Sort rows by `event_timestamp`\n\n<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> Use single and double quotes in your filter SQL expression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3de7f409-4a61-4fee-9a2b-831d85f403df"}}},{"cell_type":"code","source":["# ANSWER\nmacDF = (eventsDF\n         .where(\"device == 'macOS'\")\n         .sort(\"event_timestamp\")\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75ba94fc-f882-41ae-9f95-6ee3c289fbda"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Count results and take first 5 rows\n- Use DataFrame actions to count and take rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6dbe9a3-b23c-40c4-868d-e0afbb566885"}}},{"cell_type":"code","source":["# ANSWER\nnumRows = macDF.count()\nrows = macDF.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1aa29f6-85ef-4d3f-a5ef-f2cb0335ed38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"169ac0b6-f1ef-4b4a-b24b-1c493df11ed1"}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nassert(numRows == 1938215)\nassert(len(rows) == 5)\nassert(type(rows[0]) == Row)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b006354-f7b7-4cbb-af40-1c1913bc4167"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 5. Create the same DataFrame using SQL query\n- Use SparkSession to run a SQL query on the `events` table\n- Use SQL commands to write the same filter and sort query used earlier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fd94bd0-a701-493c-95c0-0b2526dc5601"}}},{"cell_type":"code","source":["# ANSWER\nmacSQLDF = spark.sql(\"\"\"\nSELECT *\nFROM events\nWHERE device = 'macOS'\nORDER By event_timestamp\n\"\"\")\n\ndisplay(macSQLDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46557479-5a59-4591-a36d-800f149cc6cb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**CHECK YOUR WORK**\n- You should only see `macOS` values in the `device` column\n- The fifth row should be an event with timestamp `1592539226602157`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9c6e316-a8cf-4569-a1ed-103a58195435"}}},{"cell_type":"code","source":["verify_rows = macSQLDF.take(5)\nassert (macSQLDF.select(\"device\").distinct().count() == 1 and len(verify_rows) == 5 and verify_rows[0]['device'] == \"macOS\"), \"Incorrect filter condition\"\nassert (verify_rows[4]['event_timestamp'] == 1592539226602157), \"Incorrect sorting\"\ndel verify_rows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55968af4-dc2c-4f63-82cd-8ba53d656846"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Classroom Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8905f48a-9785-4404-bccd-8f4257109136"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5babc331-0030-48da-a809-c13701ea74ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05e2469b-3ad4-4eb8-8448-775530fdba65"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 1.3 - Spark SQL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2331746562399414}},"nbformat":4,"nbformat_minor":0}
