{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad5edd2b-37b3-465e-a22f-a5d3f2536516"}}},{"cell_type":"markdown","source":["# Time Series Forecasting\n\nWorking with time series data is an often under-represented skill in data science.  In this notebook, you explore three main approaches to time series: Prophet, ARIMA, and exponential smoothing.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you will:<br>\n- Introduce the main concepts in Time Series\n- Forecast COVID data using Prophet\n- Forecast using ARIMA\n- Forecast using Exponential Smoothing\n\nIn this notebook we will be using the <a href=\"https://www.kaggle.com/kimjihoo/coronavirusdataset\" target=\"_blank\">Coronavirus dataset</a> containing data about Coronavirus patients in South Korea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af6659b6-e3c8-42e1-b2a8-ee18f142c330"}}},{"cell_type":"code","source":["%pip install --upgrade pystan==2.19.1.1 fbprophet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8998085e-93d7-4eba-a40c-8e567b3977d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2249b6c-fd43-4c47-b341-0b7867451236"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### <a href=\"https://en.wikipedia.org/wiki/Time_series\", target=\"_blank\">Time Series</a>\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series include:<br><br>\n\n- Heights of ocean tides\n- Counts of sunspots\n- Daily closing value of the Dow Jones Industrial Average\n\nIn this notebook, we will be focusing on time series forecasting, or, the use of a model to predict future values based on previously observed values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6259d78-b35e-4246-9fda-4c4039ff8693"}}},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/COVID/coronavirusdataset/Time.csv\"\n\nspark_df = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .option(\"header\", True)\n            .csv(file_path)\n           )\n  \ndisplay(spark_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c953b8a-7f09-4692-a087-4878b294b087"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Convert the Spark DataFrame to a Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a0acb2d-6faa-4bf6-a30d-99875bc80465"}}},{"cell_type":"code","source":["df = spark_df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f4eadb4-3f0a-4300-bdd0-72ece03d341b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Looking at the data, the time column (what time of day the data was reported) is not especially relevant to our forecast, so we can go ahead and drop it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95ede0a7-a481-45eb-b174-74db8da4930a"}}},{"cell_type":"code","source":["df = df.drop(columns=\"time\")\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b610c9ec-fc5f-4f1d-ae12-dff0a06e58e0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Prophet\n<a href=\"https://facebook.github.io/prophet/\" target=\"_blank\">Facebook's Prophet</a> is widely considered the easiest way to forecast because it generally does all the heavy lifting for the user. Let's take a look at how Prophet works with our dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ec5c467-527a-4930-a3b6-29fb33e78e25"}}},{"cell_type":"code","source":["import pandas as pd\nfrom fbprophet import Prophet\nimport logging\n\n# Suppresses `java_gateway` messages from Prophet as it runs.\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03f5b3e6-305c-4927-8952-89c3345a8d39"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Prophet expects certain column names for its input DataFrame. The date column must be renamed ds, and the column to be forecast should be renamed y. Let's go ahead and forecast the number of confirmed patients in South Korea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3367628-5315-412d-8ddf-0a0c77605f7e"}}},{"cell_type":"code","source":["prophet_df = pd.DataFrame()\nprophet_df[\"ds\"] = pd.to_datetime(df[\"date\"])\nprophet_df[\"y\"] = df[\"confirmed\"]\nprophet_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc8ad409-73f7-42fc-bab7-ceeba4969379"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, let's specify how many days we want to forecast for. We can do this using the **`Prophet.make_future_dataframe`** method. With the size of our data, let's take a look at the numbers a month from now. \n\nWe can see dates up to one month in the future."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e89e5f-534c-4e3a-b7a7-3fd63bdcb1ad"}}},{"cell_type":"code","source":["prophet_obj = Prophet()\nprophet_obj.fit(prophet_df)\nprophet_future = prophet_obj.make_future_dataframe(periods=30)\nprophet_future.tail()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a38dbb20-89f2-402e-baa3-15adc2868778"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we can run the **`predict`** method to forecast our data points. The **`yhat`** column contains the forecasted values. You can also look at the entire DataFrame to see what other values Prophet generates."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0cc4259-15ed-46b3-aae2-f4dc57841643"}}},{"cell_type":"code","source":["prophet_forecast = prophet_obj.predict(prophet_future)\nprophet_forecast[['ds', 'yhat']].tail()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"800d6af2-eb17-4a74-a92a-d963ea9ff31a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at a graph representation of our forecast using **`plot`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c36eeb20-2eb8-4563-a1b4-5520cfbb6f43"}}},{"cell_type":"code","source":["prophet_plot = prophet_obj.plot(prophet_forecast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52dec7fe-1848-4e15-b022-d1f2bed19a2f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can also use **`plot_components`** to get a more detailed look at our forecast."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa3f5a16-f0a8-447a-83b8-b4b2ef03e17e"}}},{"cell_type":"code","source":["prophet_plot2 = prophet_obj.plot_components(prophet_forecast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50c9a0b8-4843-4bbd-a723-267a7ae55428"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can also use Prophet to identify <a href=\"https://facebook.github.io/prophet/docs/trend_changepoints.html\" target=\"_blank\">changepoints</a>, points where the dataset had an abrupt change. This is especially useful for our dataset because it could identify time periods where Coronavirus cases spiked."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d4e169d-ef15-4d64-89d8-09fce83031f6"}}},{"cell_type":"code","source":["from fbprophet.plot import add_changepoints_to_plot\n\nprophet_plot = prophet_obj.plot(prophet_forecast)\nchangepts = add_changepoints_to_plot(prophet_plot.gca(), prophet_obj, prophet_forecast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d73f11c-1a6a-4ad1-a850-37f2d3c165ab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(prophet_obj.changepoints)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db382b5f-0d6c-43c6-937d-a3107df52c7c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, let's find out if there's any correlation between holidays in South Korea and increases in confirmed cases. We can use the built-in **`add_country_holidays`** <a href=\"https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#built-in-country-holidays\" target=\"_blank\">method</a> to find out about any trends.\n\nYou can find a complete list of country codes <a href=\"https://github.com/dr-prodigy/python-holidays/blob/master/holidays/countries/\" target=\"_blank\">here</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1af8c3e-d1f3-474e-a474-53bffa966b12"}}},{"cell_type":"code","source":["holidays = pd.DataFrame({\"ds\": [], \"holiday\": []})\nprophet_holiday = Prophet(holidays=holidays)\n\nprophet_holiday.add_country_holidays(country_name='KR')\nprophet_holiday.fit(prophet_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"940f438b-add2-4531-a8f9-241ffb3fafaf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can check what holidays are included by running the following cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0cfeeae-4186-41e8-81e0-fcb7d4cf4558"}}},{"cell_type":"code","source":["prophet_holiday.train_holiday_names"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc81d800-c9aa-4a7e-90a3-07d4ef188f0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["prophet_future = prophet_holiday.make_future_dataframe(periods=30)\nprophet_forecast = prophet_holiday.predict(prophet_future)\nprophet_plot_holiday = prophet_holiday.plot_components(prophet_forecast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf3bf161-b8de-4813-a728-45d059557960"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ARIMA\n\nARIMA stands for Auto-Regressive (AR) Integrated (I) Moving Average (MA). An ARIMA model is a form of regression analysis that gauges the strength of one dependent variable relative to other changing variables.\n\nMuch like Prophet, ARIMA  predicts future values based on the past values of your dataset. Unlike Prophet, ARIMA has a lot more set-up work but can be applied to a wide variety of time series.\n\nTo create our ARIMA model, we need to find the following parameters:<br><br>\n\n- **`p`**: The number of lag observations included in the model, also called the lag order.\n- **`d`**: The number of times that the raw observations are differenced, also called the degree of differencing.\n- **`q`**: The size of the moving average window, also called the order of moving average."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b0f11ba-7a0b-4586-9b87-17dbf11cf5da"}}},{"cell_type":"markdown","source":["Start with making our new ARIMA DataFrame. Since we already forecast the confirmed cases using Prophet, let's take a look at predictions for released patients."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9587dc4-0200-40d4-8c23-32d20e9c3d15"}}},{"cell_type":"code","source":["arima_df = pd.DataFrame()\narima_df[\"date\"] = pd.to_datetime(df[\"date\"])\narima_df[\"released\"] = df[\"released\"]\narima_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1421bfa0-d872-4188-bbc8-f4cdaf1cddb6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The first step of creating an ARIMA model is to find the d-parameter by making sure your dataset is stationary. This is easy to check using an <a href=\"https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\" target=\"_blank\">Augmented Dickey Fuller Test</a> from the **`statsmodels`** library. \n\nSince the P-value is larger than the ADF statistic, we will have to difference the dataset. Differencing helps stabilize the mean of the dataset, therefore removing the influence of past trends and seasonality on your data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d65516-758c-4031-939e-49643d553518"}}},{"cell_type":"code","source":["from statsmodels.tsa.stattools import adfuller\nfrom numpy import log\n\nresult = adfuller(df.released.dropna())\nprint(f'ADF Statistic: {result[0]}')\nprint(f'p-value: {result[1]}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a7f16c8-0c68-49f4-8c49-e162c32c5a27"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To difference the dataset, call **`diff`** on the value column. We are looking for a near-stationary series which roams around a defined mean and an ACF plot that reaches zero fairly quickly. After looking at our graphs, we can determine that our d-parameter should either be 1 or 2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68e1324c-2800-45d8-be34-e5a646cda0f9"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.rcParams.update({\"figure.figsize\":(9,7), \"figure.dpi\":120})\n\n# Original Series\nfig, axes = plt.subplots(3, 2, sharex=True)\naxes[0, 0].plot(arima_df.released); axes[0, 0].set_title('Original Series')\nplot_acf(arima_df.released, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(arima_df.released.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(arima_df.released.diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(arima_df.released.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(arima_df.released.diff().diff().dropna(), ax=axes[2, 1])\n\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98b381f6-769b-4129-b59b-9444185bf7d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the next section we'll find the required number of AR terms using the Partial Autocorrection Plot. This is the p-parameter.\n\nPartial Autocorrection is the correlation between a series and its lag. From the graphs, our p-parameter should be 1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee498210-5280-4ed6-a660-b6c51734fa7a"}}},{"cell_type":"code","source":["plt.rcParams.update({\"figure.figsize\":(9,3), \"figure.dpi\":120})\n\nfig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(arima_df.released.diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,5))\nplot_pacf(arima_df.released.diff().dropna(), ax=axes[1])\n\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a34e68a-5fbe-40e3-9874-8f87c0bddedf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we'll find the q-parameter by looking at the ACF plot to find the number of Moving Average terms. A Moving Average incorporates the dependency between an observation and a residual error applied to lagged observations. From the graphs, our q-parameter should be 1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"783ea883-fc93-442c-b1ec-7b38c7df7f43"}}},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(arima_df.released.diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\nplot_acf(arima_df.released.diff().dropna(), ax=axes[1])\n\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b142180-c12e-45c4-a412-dd3325e66d31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Once we have found our p, d, and q parameter values, we can fit our ARIMA model by passing the parameters in. The following cell shows a summary of the model including dataset information and model coefficients."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b47c591c-2553-4869-ac4a-4d5480d5bd40"}}},{"cell_type":"code","source":["from statsmodels.tsa.arima_model import ARIMA\n\n# p, d, q\n# 1, 2, 1 ARIMA Model\nmodel = ARIMA(arima_df.released, order=(1,2,1))\narima_fit = model.fit(disp=0)\nprint(arima_fit.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9951724c-34e0-4a99-91f8-84c511831786"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, let's split our data into train and test data to test the accuracy of our model. Note that since we have to split the data sequentially for time series, functions like sklearn's **`train_test_split`** cannot be used here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53809cd0-f7ae-4537-aae9-5e0dadeb9151"}}},{"cell_type":"code","source":["split_ind = int(len(arima_df)*.7)\ntrain_df = arima_df[ :split_ind]\ntest_df = arima_df[split_ind: ]\n#train_df.tail()\n#test_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bf6be98-68fb-4f68-96b2-74d0aa195a20"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To forecast we use Out of Sample Cross Validation. We can see from the graph that our forecast is slightly more linear than the actual values but overall the values are pretty close to expected."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e926704-dd7b-4264-88aa-21325c875f8a"}}},{"cell_type":"code","source":["train_model = ARIMA(train_df.released, order=(1,2,1))  \ntrain_fit = train_model.fit()  \n\nfc, se, conf = train_fit.forecast(int(len(arima_df)-split_ind))\n\nfc_series = pd.Series(fc, index=test_df.index)\n\nplt.plot(train_df.released, label='train', color=\"dodgerblue\")\nplt.plot(test_df.released, label='actual', color=\"orange\")\nplt.plot(fc_series, label='forecast', color=\"green\")\nplt.title('Forecast vs Actuals')\nplt.ylabel(\"Number of Released Patients\")\nplt.xlabel(\"Day Number\")\nplt.legend(loc='upper left', fontsize=8)\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"caa85aa8-3c7b-4a2a-b25d-4130b1d81320"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Exponential Smoothing\n\n<a href=\"(https://en.wikipedia.org/wiki/Exponential_smoothing\" target=\"_blank\">Exponential smoothing</a> is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data.\n\nThere are three types of Exponential Smoothing:<br><br>\n- Single Exponential Smoothing (SES)\n  - Used for datasets without trends or seasonality.\n- Double Exponential Smoothing (also known as Holt's Linear Smoothing)\n  - Used for datasets with trends but without seasonality.\n- Triple Exponential Smoothing (also known as Holt-Winters Exponential Smoothing)\n  - Used for datasets with both trends and seasonality.\n\nIn our case, the Coronavirus dataset has a clear trend, but seasonality is not especially important, therefore we will be using double exponential smoothing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f7cc536-856b-4ba7-a6db-d7cb218cd6fa"}}},{"cell_type":"markdown","source":["Since we have already forecast the other two columns, let's take a look at a forecast for the number of coronavirus related deaths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9b26cf6-ef0c-41fc-a525-e93adfb388c6"}}},{"cell_type":"code","source":["exp_df = pd.DataFrame()\nexp_df[\"date\"] = pd.to_datetime(df[\"date\"])\nexp_df[\"deceased\"] = df[\"deceased\"]\nexp_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baeb0c50-6a23-4073-9852-a50317f6571f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Holt's Linear Smoothing only works on data points that are greater than 0, therefore we have to drop the corresponding rows. Additionally, we need to set the index of our DataFrame to the date column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26012fc3-737b-438e-9e4d-3af7879f3b79"}}},{"cell_type":"code","source":["exp_df = exp_df[exp_df[\"deceased\"] != 0]\nexp_df = exp_df.set_index(\"date\")\nexp_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9653caac-0710-4dcc-ab9e-51b99139e899"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Luckily, statsmodel does most of the work for us. However, we still have to tweak the parameters to get an accurate forecast. The available parameters here are α or **`smoothing_level`** and β or **`smoothing_slope`**. α defines the smoothing factor of the level and β defines the smoothing factor of the trend.\n\nIn the cell below, we are trying three different kinds of predictions. The first, Holt's Linear Trend, forecasts with a linear trend. The second, Exponential Trend, forecasts with an exponential trend. The third, Additive Damped Trend, damps the forecast trend linearly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f12bbf3f-b70e-4c9a-a358-5b49d3f863aa"}}},{"cell_type":"code","source":["from statsmodels.tsa.holtwinters import Holt\n\nexp_fit1 = Holt(exp_df.deceased).fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False)\nexp_forecast1 = exp_fit1.forecast(30).rename(\"Holt's linear trend\")\n\nexp_fit2 = Holt(exp_df.deceased, exponential=True).fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False)\nexp_forecast2 = exp_fit2.forecast(30).rename(\"Exponential trend\")\n\nexp_fit3 = Holt(exp_df.deceased, damped=True).fit(smoothing_level=0.8, smoothing_slope=0.2)\nexp_forecast3 = exp_fit3.forecast(30).rename(\"Additive damped trend\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1194e110-f22e-41bb-aadd-68fe20f77d4f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["After plotting the three models, we can see that the standard Holt's Linear, and the Exponential trend lines give very similar forecasts while the Additive Damped trend gives a slightly lower number of deceased patients."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c346ef-9408-44b7-98f9-33ea7adae689"}}},{"cell_type":"code","source":["exp_fit1.fittedvalues.plot(color=\"orange\", label=\"Holt's linear trend\")\nexp_fit2.fittedvalues.plot(color=\"red\", label=\"Exponential trend\")\nexp_fit3.fittedvalues.plot(color=\"green\", label=\"Additive damped trend\")\n\nplt.legend()\nplt.ylabel(\"Number of Deceased Patients\")\nplt.xlabel(\"Day Number\")\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4456304c-7ffd-44cd-842e-42460995c54f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can zoom in on the forecast part of our graph to see the graph in more detail.\n\nWe can see that the exponential trendline starts in-line with the linear trendline but slowly starts resembling an exponential trend towards the end of the graph. The damped trendline starts and ends below the other trendlines."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5bd976d-6577-4013-a581-31f5f3426649"}}},{"cell_type":"code","source":["exp_forecast1.plot(legend=True, color=\"orange\")\nexp_forecast2.plot(legend=True, color=\"red\")\nexp_forecast3.plot(legend=True, color=\"green\")\n\nplt.ylabel(\"Number of Deceased Patients\")\nplt.xlabel(\"Day Number\")\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"583620ef-528a-4a2f-a8d4-edf8eef2247d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"626bc440-01de-4f6f-933c-30efbb1575bb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MLE 04 - Time Series Forecasting","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157727922}},"nbformat":4,"nbformat_minor":0}
