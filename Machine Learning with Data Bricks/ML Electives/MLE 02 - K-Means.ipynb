{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ba2d49f-7354-412e-a8cb-a2311a7bfb01"}}},{"cell_type":"markdown","source":["# Distributed K-Means\n\nIn this notebook, we are going to use K-Means to cluster our data. We will be using the Iris dataset, which has labels (the type of iris), but we will only use the labels to evaluate the model, not to train it. \n\nAt the end, we will look at how it is implemented in the distributed setting.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Build a K-Means model\n - Analyze the computation and communication of K-Means in a distributed setting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b700f8eb-d053-4998-be3e-769a00e3b501"}}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load in a Dataset from sklearn and convert to a Spark DataFrame\niris = load_iris()\niris_pd = pd.concat([pd.DataFrame(iris.data, columns=iris.feature_names), pd.DataFrame(iris.target, columns=[\"label\"])], axis=1)\niris_df = spark.createDataFrame(iris_pd)\ndisplay(iris_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7c7c442-434a-47d8-9026-cbe5c419a8e1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Notice that we have four values as \"features\".  We'll reduce those down to two values (for visualization purposes) and convert them to a **`DenseVector`**.  To do that we'll use the **`VectorAssembler`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2867333b-9040-482c-a072-2d60eb80ab3b"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvec_assembler = VectorAssembler(inputCols=[\"sepal length (cm)\", \"sepal width (cm)\"], outputCol=\"features\")\niris_two_features_df = vec_assembler.transform(iris_df)\ndisplay(iris_two_features_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f469979-caa1-47fe-a432-7ca4d728317d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=3, seed=221, maxIter=20)\n\n#  Call fit on the estimator and pass in iris_two_features_df\nmodel = kmeans.fit(iris_two_features_df)\n\n# Obtain the clusterCenters from the KMeansModel\ncenters = model.clusterCenters()\n\n# Use the model to transform the DataFrame by adding cluster predictions\ntransformed_df = model.transform(iris_two_features_df)\n\nprint(centers)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac7232c6-a832-4fa3-9eb0-6e0ba70c0c89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model_centers = []\niterations = [0, 2, 4, 7, 10, 20]\nfor i in iterations:\n    kmeans = KMeans(k=3, seed=221, maxIter=i)\n    model = kmeans.fit(iris_two_features_df)\n    model_centers.append(model.clusterCenters())   "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0abd159e-223d-46cd-8c1e-5d0c50d3124a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"model_centers:\")\nfor centroids in model_centers:\n    print(centroids)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57c8c62f-277d-42e8-a465-70b03d23a4d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's visualize how our clustering performed against the true labels of our data.\n\nRemember: K-means doesn't use the true labels when training, but we can use them to evaluate. \n\nHere, the star marks the cluster center."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"446b435b-c389-41b4-b50c-2c37bd95e82c"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\ndef prepare_subplot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor=\"#999999\", gridWidth=1.0, subplots=(1, 1)):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    fig, ax_list = plt.subplots(subplots[0], subplots[1], figsize=figsize, facecolor=\"white\", \n                               edgecolor=\"white\")\n    if not isinstance(ax_list, np.ndarray):\n        ax_list = np.array([ax_list])\n    \n    for ax in ax_list.flatten():\n        ax.axes.tick_params(labelcolor=\"#999999\", labelsize=\"10\")\n        for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n            axis.set_ticks_position(\"none\")\n            axis.set_ticks(ticks)\n            axis.label.set_color(\"#999999\")\n            if hideLabels: axis.set_ticklabels([])\n        ax.grid(color=gridColor, linewidth=gridWidth, linestyle=\"-\")\n        map(lambda position: ax.spines[position].set_visible(False), [\"bottom\", \"top\", \"left\", \"right\"])\n        \n    if ax_list.size == 1:\n        ax_list = ax_list[0]  # Just return a single axes object for a regular plot\n    return fig, ax_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8950691-2d85-421b-8824-3cb76638541d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = iris_two_features_df.select(\"features\", \"label\").collect()\nfeatures, labels = zip(*data)\n\nx, y = zip(*features)\ncenters = model_centers[5]\ncentroid_x, centroid_y = zip(*centers)\ncolor_map = \"Set1\"\n\nfig, ax = prepare_subplot(np.arange(-1, 1.1, .4), np.arange(-1, 1.1, .4), figsize=(8,6))\nplt.scatter(x, y, s=14**2, c=labels, edgecolors=\"#8cbfd0\", alpha=0.80, cmap=color_map)\nplt.scatter(centroid_x, centroid_y, s=22**2, marker=\"*\", c=\"yellow\")\ncmap = cm.get_cmap(color_map)\n\ncolor_index = [.5, .99, .0]\nfor i, (x,y) in enumerate(centers):\n    print(cmap(color_index[i]))\n    for size in [.10, .20, .30, .40, .50]:\n        circle1=plt.Circle((x,y), size, color=cmap(color_index[i]), alpha=.10, linewidth=2)\n        ax.add_artist(circle1)\n\nax.set_xlabel(\"Sepal Length\"), ax.set_ylabel(\"Sepal Width\")\nfig"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df95842c-038c-4c68-a9ce-1b7adcf3dfd1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In addition to seeing the overlay of the clusters at each iteration, we can see how the cluster centers moved with each iteration (and what our results would have looked like if we used fewer iterations)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"341ee5ac-350a-4159-ae57-62a47cb8e13c"}}},{"cell_type":"code","source":["x, y = zip(*features)\n\nold_centroid_x, old_centroid_y = None, None\n\nfig, ax_list = prepare_subplot(np.arange(-1, 1.1, .4), np.arange(-1, 1.1, .4), figsize=(11, 15),\n                             subplots=(3, 2))\nax_list = ax_list.flatten()\n\nfor i,ax in enumerate(ax_list[:]):\n    ax.set_title(\"K-means for {0} iterations\".format(iterations[i]), color=\"#999999\")\n    centroids = model_centers[i]\n    centroid_x, centroid_y = zip(*centroids)\n    \n    ax.scatter(x, y, s=10**2, c=labels, edgecolors=\"#8cbfd0\", alpha=0.80, cmap=color_map, zorder=0)\n    ax.scatter(centroid_x, centroid_y, s=16**2, marker=\"*\", c=\"yellow\", zorder=2)\n    if old_centroid_x and old_centroid_y:\n      ax.scatter(old_centroid_x, old_centroid_y, s=16**2, marker=\"*\", c=\"grey\", zorder=1)\n    cmap = cm.get_cmap(color_map)\n    \n    color_index = [.5, .99, 0.]\n    for i, (x1,y1) in enumerate(centroids):\n      print(cmap(color_index[i]))\n      circle1=plt.Circle((x1,y1),.35,color=cmap(color_index[i]), alpha=.40)\n      ax.add_artist(circle1)\n    \n    ax.set_xlabel(\"Sepal Length\"), ax.set_ylabel(\"Sepal Width\")\n    old_centroid_x, old_centroid_y = centroid_x, centroid_y\n\nplt.tight_layout()\n\nfig"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3174e19-a38b-4431-8451-7bceb32f1d45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["So let's take a look at what's happening here in the distributed setting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea2569ba-fad1-4a5d-93f0-0f078c909c9c"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/Mapstage.png\" height=200px>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"069568a9-c90c-4021-bda9-4cfabc87234f"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/Mapstage2.png\" height=500px>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb618ccb-3c37-4efa-8ec0-3eff8fc0a488"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/ReduceStage.png\" height=500px>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05958374-e833-4901-b2b8-8e45d67f6640"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/Communication.png\" height=500px>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cd8b2e7-9490-4226-a681-5126549b307d"}}},{"cell_type":"markdown","source":["## Take Aways\n\nWhen designing/choosing distributed ML algorithms\n* Communication is key!\n* Consider your data/model dimensions & how much data you need.\n* Data partitioning/organization is important."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c23edb5-c9c0-442a-8a44-7d2f48561a7d"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f149964-ec8a-418c-af3d-d3cac01dbeaa"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MLE 02 - K-Means","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157728014}},"nbformat":4,"nbformat_minor":0}
