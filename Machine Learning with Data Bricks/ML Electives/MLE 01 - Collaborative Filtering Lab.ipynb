{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d077f8cb-b781-455c-b5c9-666ddf3ae04d"}}},{"cell_type":"markdown","source":["-sandbox\n\n<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/movie-camera.png\" style=\"float:right; height: 200px; margin: 10px; border: 1px solid #ddd; border-radius: 15px 15px 15px 15px; padding: 10px\"/>\n\n# Predicting Movie Ratings\n\nOne of the most common uses of big data is to predict what users want.  This allows Google to show you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you might like.  This lab will demonstrate how we can use Apache Spark to recommend movies to a user.  We will start with some basic techniques, and then use the SparkML library's Alternating Least Squares method to make more sophisticated predictions. Here are the SparkML <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\" target=\"_blank\">Python docs</a>.\n\nFor this lab, we will use 1 million movie ratings from the <a href=\"http://grouplens.org/datasets/movielens/\" target=\"_blank\">MovieLens stable benchmark rating dataset</a>. \n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Exploring the dataset and build a baseline model\n - Build a Collaborative Filtering model\n - Make customized movie predictions for yourself"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9af1fd2-480a-4fa5-80f6-585431ad313f"}}},{"cell_type":"markdown","source":["##### Motivation: Want to win $1,000,000?\n\nAll you needed to do was improve Netflixâ€™s movie recommendation system by 10% in 2008. This competition is known as the <a href=\"https://en.wikipedia.org/wiki/Netflix_Prize\" target=\"_blank\">Netflix Prize</a>. \n\nGood recommendations are vital to sites such as Netflix, where 75 percent of what consumers watch come from movie recommendations.\n\nSo, how do we create recommendations and evaluate their relevance?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6ce94a3-64a4-4eaa-a575-ace2179071ce"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be18b5f5-6139-4189-b0d4-830e96701a0c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Part 1: Exploring our Dataset\n\nFirst, let's take a look at the directory containing our files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d696ac4e-d3f1-42f5-99e9-692711f65163"}}},{"cell_type":"code","source":["files = dbutils.fs.ls(f\"{datasets_dir}/movielens\")\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b49debe-2e48-4f50-81d5-b5b4379bcf52"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Load and Cache\n\nWe're going to be accessing this data a lot. \n\nRather than reading it from source over and over again, we'll cache both the movies DataFrame and the ratings DataFrame into the executor's memory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a317a48-a348-4d27-a4a5-16c40dad1ef9"}}},{"cell_type":"code","source":["movies_df = spark.read.parquet(f\"{datasets_dir}/movielens/movies.parquet/\").cache()\nratings_df = spark.read.parquet(f\"{datasets_dir}/movielens/ratings.parquet/\").cache()\n\nratings_count = ratings_df.count()\nmovies_count = movies_df.count()\n\nprint(f\"There are {ratings_count} ratings and {movies_count} movies in the datasets\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe25571a-80d8-4d14-bcb8-d62fbb8d4fbe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a quick look at some of the data in the two DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9675464e-595d-4053-bbed-60dc88610fc7"}}},{"cell_type":"code","source":["display(movies_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c412e54a-f50f-45aa-abfa-c66b719566d7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(ratings_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"157ee310-21ac-42d8-b44f-29b585149283"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## **Part 2: Collaborative Filtering**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05b30cdb-5db8-405a-a336-22d749c5fa48"}}},{"cell_type":"markdown","source":["### (2a) Creating a Training Set\n\nBefore we jump into using machine learning, we need to break up the **`ratings_df`** dataset into two DataFrames:\n* A training set, which we will use to train models\n* A test set, which we will use for our experiments\n\nTo randomly split the dataset into the multiple groups, we can use the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html?highlight=randomsplit#pyspark.sql.DataFrame.randomSplit\" target=\"_blank\">randomSplit()</a> transformation. **`randomSplit()`** takes a set of splits and a seed and returns multiple DataFrames. Use the seed given below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fecf7f6-81dc-471e-b09f-fbea3f4552ad"}}},{"cell_type":"code","source":["# TODO\n# We'll hold out 80% for training and leave 20% for testing \nseed = 42\ntrain_df, test_df = <FILL_IN>\n\nprint(f\"Training: {train_df.count()}, test: {test_df.count()}\")\n\ntrain_df.show(3)\ntest_df.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa045585-55f0-4a3c-ac61-4ac1c233911e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (2b) Benchmark Model\n\nLet's always predict the average movie rating in our dataset to use as our benchmark model, and see what our RMSE is."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5155f5c0-bc45-4cfc-9354-dcb2f72cb988"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.functions import lit, avg\n\naverage_rating = train_df.select(avg(\"rating\")).first()[0]\n\nbenchmark_df = train_df.withColumn(\"prediction\", lit(average_rating))\n\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\nbaseline_rmse = reg_eval.evaluate(benchmark_df)\n\nprint(f\"Baseline RMSE: {baseline_rmse:.3}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c550673-7f13-462a-a20b-a4892706bdd4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (2c) Alternating Least Squares\n\nIn this part, we will use the Apache Spark ML Pipeline implementation of Alternating Least Squares, <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html?highlight=als#pyspark.ml.recommendation.ALS\" target=\"_blank\">ALS</a>. To determine the best values for the hyperparameters, we will use ALS to train several models, and then we will select the best model and use the parameters from that model in the rest of this lab exercise.\n\nThe process we will use for determining the best model is as follows:\n1. Pick a set of model hyperparameters. The most important hyperparameter to model is the *rank*, which is the number of columns in the Users matrix or the number of rows in the Movies matrix. In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to <a href=\"https://en.wikipedia.org/wiki/Overfitting\" target=\"_blank\">overfitting</a>.  We will train models with ranks of 4 and 12 using the **`train_df`** dataset.\n\n2. Set the appropriate values:\n    * The \"User\" column will be set to the values in our **`userId`** DataFrame column.\n    * The \"Item\" column will be set to the values in our **`movieId`** DataFrame column.\n    * The \"Rating\" column will be set to the values in our **`rating`** DataFrame column.\n    * **`nonnegative`** = True (whether to use nonnegative constraint for least squares)\n    * **`regParam`** = 0.1.\n    \n   **Note**: Read the documentation for the ALS class **carefully**. It will help you accomplish this step.\n\n4. Create multiple models using the **`ParamGridBuilder`** and the **`CrossValidator`**, one for each of our rank values.\n\n6. We'll keep the model with the lowest error rate. Such a model will be selected automatically by the CrossValidator."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784af106-122f-4b06-b919-f50b35d6b79e"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml.recommendation import ALS\n\nals = ALS(maxIter=5, seed=seed, coldStartStrategy=\"drop\", <FILL_IN>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a614ec5-3409-468f-a786-785ba2d4cdeb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Test our solution\nassert als.getItemCol() == \"movieId\", f\"Incorrect choice of {als.getItemCol()} for ALS item column.\"\nassert als.getUserCol() == \"userId\", f\"Incorrect choice of {als.getUserCol()} for ALS user column.\"\nassert als.getRatingCol() == \"rating\", f\"Incorrect choice of {als.getRatingCol()} for ALS rating column.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1501acfd-d583-4b39-97ad-8e8490ff1a17"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that we have initialized a model, we need to fit it to our training data, and evaluate how well it does on the validation dataset. Create a **`CrossValidator`** and **`ParamGridBuilder`** that will decide whether *rank* value *4* or *12* gives a lower *RMSE*.  \n\nNOTE: This cell may take a few minutes to run."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6fc1224-1b9a-4a6a-b6f0-2dc47e8b7667"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nreg_eval = <FILL_IN> # Create RegressionEvaluator\n\ngrid = (\n        <FILL_IN> # Create grid for rank values 4 and 12 \n        )\n\nseed = 42\ncv = CrossValidator(<FILL_IN>) # Set number of folds to 3. Add grid, als, reg_eval, and seed           \n\ncv_model = cv.fit(train_df)\n\nmy_model = cv_model.bestModel\n\nprint(f\"The best model was trained with rank {my_model.rank}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af2d8f80-a2ff-433e-9891-553be7a1adf6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Test our solution\nassert my_model.rank == 12, f\"Unexpected value for best rank. Expected 12, got {my_model.rank}\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe38e659-6e16-43dc-a29a-37bd25f2d9eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (2d) Testing Your Model\n\nSo far, we used the **`train_df`** dataset to select the best model. Since we used this dataset to determine what model is best, we cannot use it to test how good the model is; otherwise, we would be very vulnerable to <a href=\"https://en.wikipedia.org/wiki/Overfitting\" target=\"_blank\">overfitting</a>.  To decide how good our model is, we need to use the **`test_df`** dataset.  We will use the best model you created in part (2b) for predicting the ratings for the test dataset and then we will compute the RMSE.\n\nThe steps you should perform are:\n* Run a prediction, using **`my_model`** as created above, on the test dataset (**`test_df`**), producing a new **`predicted_test_df`** DataFrame.\n* Use the previously created RMSE evaluator, **`reg_eval`** to evaluate the filtered DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4be9b000-8823-4ccc-a133-0d7d87b0d765"}}},{"cell_type":"code","source":["# TODO\n\npredicted_test_df = my_model.<FILL_IN>\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\ntest_rmse = <FILL_IN>\n\nprint(f\"The model had a RMSE on the test set of {test_rmse}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0547b9c6-7a7d-4963-86a8-f58797be0de6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Part 3: Predictions for Yourself\nThe ultimate goal of this lab exercise is to predict what movies to recommend to yourself.  In order to do that, you will first need to add ratings for yourself to the **`ratings_df`** dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8afff1ea-d661-4e73-9498-f07083ba9bb0"}}},{"cell_type":"markdown","source":["**(3a) Your Movie Ratings**\n\nTo help you provide ratings for yourself, we have included the following code to list the names and movie IDs of the 100 highest-rated movies that have at least 500 ratings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88b34418-a4a4-4377-8945-e44cc754d6ed"}}},{"cell_type":"code","source":["movies_df.createOrReplaceTempView(\"movies\")\nratings_df.createOrReplaceTempView(\"ratings\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32f5cde6-1c91-481c-b469-821ead3fba8f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT movieId, title, AVG(rating) AS avg_rating, COUNT(*) AS num_ratings\nFROM ratings r JOIN movies m ON (r.movieID = m.ID)\nGROUP BY r.movieId, m.title\nHAVING COUNT(*) > 500\nORDER BY avg_rating DESC\nLIMIT 100"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61275cd4-516f-4df1-8fe0-415e9d443433"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The user ID 0 is unassigned, so we will use it for your ratings. We set the variable **`myUserId`** to 0 for you. \n\nNext, create a new DataFrame called **`my_ratings_df`**, with your ratings for at least 10 movie ratings. Each entry should be formatted as **`(myUserId, movieId, rating)`**.  As in the original dataset, ratings should be between 1 and 5 (inclusive). \n\nIf you have not seen at least 10 of these movies, you can increase the parameter passed to **`LIMIT`** in the above cell until there are 10 movies that you have seen (or you can also guess what your rating would be for movies you have not seen)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dae77f56-f704-4aac-bf68-4fc83c275c5e"}}},{"cell_type":"code","source":["# TODO\nmyUserId = 0\n\n# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.\nmyRatedMovies = [\n     <FILL_IN>\n     # The format of each line is (myUserId, movie ID, your rating)\n     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n     #   (myUserId, 260, 5),\n]\n\nmy_ratings_df = spark.createDataFrame(myRatedMovies, [\"userId\", \"movieId\", \"rating\"])\ndisplay(my_ratings_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e21eac-0286-4af6-b165-7cba62ee886b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (3b) Add Your Movies to Training Dataset\n\nNow that you have ratings for yourself, you need to add your ratings to the **`train_df`** dataset so that the model you train will incorporate your preferences.  Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html?highlight=union#pyspark.sql.DataFrame.union\" target=\"_blank\">union()</a> transformation combines two DataFrames; use **`union()`** to create a new training dataset that includes your ratings and the data in the original training dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b974d1d-e107-4f3a-84f3-2351f02f4977"}}},{"cell_type":"code","source":["# TODO\ntraining_with_my_ratings_df = <FILL_IN>\n\ncount_diff = training_with_my_ratings_df.count() - train_df.count()\nprint(f\"The training dataset now has {count_diff} more entries than the original training dataset\")\nassert (count_diff == myratings_df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95e2e867-79a0-4d56-bb06-b48370f5a4cf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (3c) Train a Model with Your Ratings\n\nNow, train a model with your ratings added and the parameters you used in in part (2b) and (2c). Make sure you include **all** of the parameters.\n\n**Note**: This cell will take about 1 minute to run."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69d3828f-22ce-4573-9cd7-6a009ab93b76"}}},{"cell_type":"code","source":["# TODO\n\nals.<FILL_IN>\n\n# Create the model with these parameters\nmy_ratings_model = als.<FILL_IN>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fd01716-0b22-4ac8-a298-03026fc9547a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (3d) Predict Your Ratings\n\nNow that we have trained a new model, let's predict what ratings you would give to the movies that you did not already provide ratings for. The code below filters out all of the movies you have rated, and creates a **`predicted_ratings_df`** DataFrame of the predicted ratings for all of your unseen movies."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7db2f49-7ec1-48a0-bcb4-f32554129ffe"}}},{"cell_type":"code","source":["# Create a list of the my rated movie IDs \nmy_rated_movie_ids = [x[1] for x in myRatedMovies]\n\n# Filter out the movies I already rated.\nnot_rated_df = movies_df.filter(~ movies_df[\"ID\"].isin(my_rated_movie_ids))\n\n# Rename the \"ID\" column to be \"movieId\", and add a column with myUserId as \"userId\".\nmy_unrated_movies_df = not_rated_df.withColumnRenamed(\"ID\", \"movieId\").withColumn(\"userId\", lit(myUserId))       \n\n# Use my_ratings_model to predict ratings for the movies that I did not manually rate.\npredicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e22deab-d0ec-4e04-8760-7100936b65c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### (3e) Predict Your Ratings\n\nWe have our predicted ratings. Now we can print out the 25 movies with the highest predicted ratings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88051dab-df98-42f6-a853-fbe7c5759e17"}}},{"cell_type":"code","source":["predicted_ratings_df.createOrReplaceTempView(\"predictions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c53d341b-2cdf-48e6-90c1-420c35c8ed83"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at the raw predictions:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa16b065-f8ff-4efb-9aae-afbcb440a376"}}},{"cell_type":"code","source":["%sql \nSELECT * FROM predictions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc1ddf51-2bbd-4076-803a-1264c64e9236"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now print out the 25 movies with the highest predicted ratings. We will only include movies that have at least 75 ratings in total."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"833b3f80-5351-4075-bbcb-a30cebb17e13"}}},{"cell_type":"code","source":["%sql\nSELECT p.title, p.prediction AS your_predicted_rating\nFROM ratings r INNER JOIN predictions p \nON (r.movieID = p.movieID)\nWHERE p.userId = 0\nGROUP BY p.title, p.prediction\nHAVING COUNT(*) > 75\nORDER BY p.prediction DESC\nLIMIT 25"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3138b0e-f605-48d6-88b7-36af5df44462"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edb71571-0988-47eb-a223-d01ee51222f5"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MLE 01 - Collaborative Filtering Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157727878}},"nbformat":4,"nbformat_minor":0}
