{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07d50b5a-108a-4efe-8c63-401dd0af2f3f"}}},{"cell_type":"markdown","source":["# Decision Trees\n\nIn the previous notebook, you were working with the parametric model, Linear Regression. We could do some more hyperparameter tuning with the linear regression model, but we're going to try tree based methods and see if our performance improves.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Identify the differences between single node and distributed decision tree implementations\n - Get the feature importance\n - Examine common pitfalls of decision trees"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"656e3ca5-a696-4274-90be-0ab59ac68ddd"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fe38d97-59b4-491f-a9a2-14dac27f6f99"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\nairbnb_df = spark.read.format(\"delta\").load(file_path)\ntrain_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a6fc9ca-34fc-403b-af36-36911d09a845"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## How to Handle Categorical Features?\n\nWe saw in the previous notebook that we can use StringIndexer/OneHotEncoder/VectorAssembler or RFormula.\n\n**However, for decision trees, and in particular, random forests, we should not OHE our variables.**\n\nThere is an excellent <a href=\"https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769#:~:text=One%2Dhot%20encoding%20categorical%20variables,importance%20resulting%20in%20poorer%20performance\" target=\"_blank\">blog</a> on this, and the essence is:\n>>> \"One-hot encoding categorical variables with high cardinality can cause inefficiency in tree-based methods. Continuous variables will be given more importance than the dummy variables by the algorithm, which will obscure the order of feature importance and can result in poorer performance.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05004c71-d830-40f9-9041-d57a67dd69fd"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\ncategorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\nindex_output_cols = [x + \"Index\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04a11eae-627c-417e-8600-a16f2346c376"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## VectorAssembler\n\nLet's use the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a> to combine all of our categorical and numeric inputs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3112db0b-9265-4d12-88e6-1f45d5b874d4"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\n# Filter for just numeric columns (and exclude price, our label)\nnumeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n# Combine output of StringIndexer defined above and numeric columns\nassembler_inputs = index_output_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5d81134-29be-4da1-bda6-88190d67643e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Decision Tree\n\nNow let's build a <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html?highlight=decisiontreeregressor#pyspark.ml.regression.DecisionTreeRegressor\" target=\"_blank\">DecisionTreeRegressor</a> with the default hyperparameters."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90c05d3b-223b-4260-9398-eeddfddb4984"}}},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(labelCol=\"price\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ff1ea4e-7b7a-4e56-a503-523768377201"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Fit Pipeline\n\nThe following cell is expected to error, but we subsequently fix this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17002f30-1aaf-4661-b59c-6db0dcf7a7c8"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# Combine stages into pipeline\nstages = [string_indexer, vec_assembler, dt]\npipeline = Pipeline(stages=stages)\n\n# Uncomment to perform fit\n# pipeline_model = pipeline.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6e02317-2be0-43d4-ac28-f129300d5ea4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## maxBins\n\nWhat is this parameter <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html?highlight=decisiontreeregressor#pyspark.ml.regression.DecisionTreeRegressor.maxBins\" target=\"_blank\">maxBins</a>? Let's take a look at the PLANET implementation of distributed decision trees to help explain the **`maxBins`** parameter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78ccb036-a287-4c44-8f24-abf44a54ba89"}}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/DistDecisionTrees.png\" height=500px>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d546a35-383d-41da-968a-2128f1f346a3"}}},{"cell_type":"markdown","source":["In Spark, data is partitioned by row. So when it needs to make a split, each worker has to compute summary statistics for every feature for  each split point. Then these summary statistics have to be aggregated (via tree reduce) for a split to be made. \n\nThink about it: What if worker 1 had the value **`32`** but none of the others had it. How could you communicate how good of a split that would be? So, Spark has a maxBins parameter for discretizing continuous variables into buckets, but the number of buckets has to be as large as the categorical variable with the highest cardinality."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f80d3c5-e3ff-4f07-829c-228df328f50e"}}},{"cell_type":"markdown","source":["Let's go ahead and increase maxBins to **`40`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f38f842-985e-44e0-b2ec-ef644955a173"}}},{"cell_type":"code","source":["dt.setMaxBins(40)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05144ad9-ef79-437d-a604-813736c21fb9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Take two."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dbf3bf9-f0a7-4ceb-9639-2d97452735f2"}}},{"cell_type":"code","source":["pipeline_model = pipeline.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"318d9597-099d-4367-ae65-d40db5ba393f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Feature Importance\n\nLet's go ahead and get the fitted decision tree model, and look at the feature importance scores."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07354d9-f3ab-4e77-977e-b770205c2f02"}}},{"cell_type":"code","source":["dt_model = pipeline_model.stages[-1]\ndisplay(dt_model)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"735ae7ad-a6e0-491b-a480-c472aa43d801"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dt_model.featureImportances"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"126170f5-796d-4bdd-a0c4-33172bc3ccce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Interpreting Feature Importance\n\nHmmm... it's a little hard to know what feature 4 vs 11 is. Given that the feature importance scores are \"small data\", let's use Pandas to help us recover the original column names."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6494798c-bb60-440e-a291-d60f1acc7579"}}},{"cell_type":"code","source":["import pandas as pd\n\nfeatures_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), dt_model.featureImportances)), columns=[\"feature\", \"importance\"])\nfeatures_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e315361b-7434-4305-8111-e5f60328da6b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Why so few features are non-zero?\n\nWith SparkML, the default **`maxDepth`** is 5, so there are only a few features we could consider (we can also split on the same feature many times at different split points).\n\nLet's use a Databricks widget to get the top-K features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a90d8afb-5bd9-4d02-af16-06e11790ebbf"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"top_k\", \"5\")\ntop_k = int(dbutils.widgets.get(\"top_k\"))\n\ntop_features = features_df.sort_values([\"importance\"], ascending=False)[:top_k][\"feature\"].values\nprint(top_features)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b6ed9ae-e4a7-474f-b947-096fab379c01"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Scale Invariant\n\nWith decision trees, the scale of the features does not matter. For example, it will split 1/3 of the data if that split point is 100 or if it is normalized to be .33. The only thing that matters is how many data points fall left and right of that split point - not the absolute value of the split point.\n\nThis is not true for linear regression, and the default in Spark is to standardize first. Think about it: If you measure shoe sizes in American vs European sizing, the corresponding weight of those features will be very different even those those measures represent the same thing: the size of a person's foot!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6979b52b-edec-4722-91a8-fa59202efd4a"}}},{"cell_type":"markdown","source":["## Apply model to test set"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ac5724f-a492-4c3b-9c11-2106291e1397"}}},{"cell_type":"code","source":["pred_df = pipeline_model.transform(test_df)\n\ndisplay(pred_df.select(\"features\", \"price\", \"prediction\").orderBy(\"price\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66a569b6-d7d2-483c-b0f8-d11ac1e278cf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Pitfall\n\nWhat if we get a massive Airbnb rental? It was 20 bedrooms and 20 bathrooms. What will a decision tree predict?\n\nIt turns out decision trees cannot predict any values larger than they were trained on. The max value in our training set was $10,000, so we can't predict any values larger than that."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a9f28a5-6b37-4ab5-af32-504996ab3ba7"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n\nrmse = regression_evaluator.evaluate(pred_df)\nr2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\nprint(f\"RMSE is {rmse}\")\nprint(f\"R2 is {r2}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08c16d8a-bbb5-4d6e-869d-ac523f12fa69"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Uh oh!\n\nThis model is way worse than the linear regression model, and it's even worse than just predicting the average value.\n\nIn the next few notebooks, let's look at hyperparameter tuning and ensemble models to improve upon the performance of our single decision tree."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9055cdcc-deab-436d-9b5a-da872cf841de"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf4aa7ef-3131-41c1-8c76-3d8a50c14c6f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 06 - Decision Trees","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726619}},"nbformat":4,"nbformat_minor":0}
