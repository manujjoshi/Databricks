{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18594b8a-ab3c-45de-9ac7-6f195252b753"}}},{"cell_type":"markdown","source":["# Feature Store Lab\n\nNow that you are familiar with the <a href=\"https://docs.databricks.com/applications/machine-learning/feature-store.html\" target=\"_blank\">Databricks Feature Store</a>, try applying the concepts we learned to a new dataset below.\n\nThe Feature Store Python API documentation can be found <a href=\"https://docs.databricks.com/dev-tools/api/python/latest/index.html#feature-store-python-api-reference\" target=\"_blank\">here</a>.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you will:<br>\n - Create a feature store \n - Update existing feature tables\n - Register a MLflow model with feature tables\n - Perform batch scoring"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46fddcef-ee9e-4411-82ae-7035521c55a2"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df473333-3862-4a6a-9b68-578f7fb38aa5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Load the data\nFor this example, we will use a new COVID-19 dataset. Run the cell below to create our dataframe **`covid_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a797e37e-1d67-47cf-817e-7beb846c5224"}}},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id\n\nfile_path = f\"{datasets_dir}/COVID/coronavirusdataset/Time.csv\"\ncovid_df = (spark.read\n            .format(\"csv\")\n            .option(\"header\",True)\n            .option(\"inferSchema\", True)\n            .load(file_path)\n            .withColumn(\"index\", monotonically_increasing_id()))\n\ndisplay(covid_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71940b08-b9a0-4d1f-b53a-3e367857bbca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the cell below to set up a database and unique table name **`table_name`** for the lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8638f69-a3fc-46a6-ac82-9e41a3ab329f"}}},{"cell_type":"code","source":["import uuid\n\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS {cleaned_username}\")\ntable_name = f\"{cleaned_username}.airbnb_{str(uuid.uuid4())[:6]}\"\n\nprint(table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b53787ff-d21c-4db6-90e8-cb9d92bfc2e4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's set up our FeatureStoreClient **`fs`**. \n\nTo create a feature store client, initialize a **`FeatureStoreClient`** object from the **`feature_store`** module."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c92f8e9-5722-45c6-8234-4aa347e4235d"}}},{"cell_type":"code","source":["# ANSWER\nfrom databricks import feature_store\n\nfs = feature_store.FeatureStoreClient()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6941a985-09fa-444d-b4ed-677c7fafd463"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Extract Features\n\nIn this simple example we want to predict the number of daily deceased using the other information from from day. \n\nBefore we write to our feature table, we will need to write a feature computation function that separates our features from the label. \n\nFill in the feature computation function below to select only the feature columns, not **`deceased`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db0c98d2-1a74-4af4-91b9-fcb2074d82c3"}}},{"cell_type":"code","source":["# ANSWER\ncolumns = covid_df.columns\ncolumns.remove(\"deceased\")\n\n@feature_store.feature_table\ndef select_features(dataframe):\n    return dataframe.select(columns)\n\ncovid_features_df = select_features(covid_df)\ndisplay(covid_features_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7f8fd58-01ae-4d15-adea-3e649c73ce42"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Create Feature Table\n\nNow that we have our features ready, complete the cell below to create our feature table.\n\nMake sure to set the name to the **`table_name`** we defined above.\n\n**NOTE:** The primary key needs to be defined in a list as follows: [\"primary key name\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a0c53e8-85ab-433d-9685-00605def31c7"}}},{"cell_type":"code","source":["# ANSWER\nfs.create_table(\n    name=table_name,\n    primary_keys=[\"index\"],\n    df=covid_features_df,\n    schema=covid_features_df.schema,\n    description=\"Example Description\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dfc475f-5b80-44c3-985a-b64cdf3678fc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Update Feature Table\n\nImagine now that we wanted to add separate columns for the month and day of the date for each entry. \n\nRather than recompute the table with these values, we just want to append these new columns to the existing table. \n\nFirst, let's create columns for the month and day."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a80ba44-9f3f-4d12-b55c-203b129691ce"}}},{"cell_type":"code","source":["from pyspark.sql.functions import month, dayofmonth\n\nadd_df = (covid_features_df\n  .select(\"date\", \"index\")\n  .withColumn(\"month\", month(\"date\"))\n  .withColumn(\"day\", dayofmonth(\"date\"))\n)\n\ndisplay(add_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73cfb7b1-061d-4a45-a9c2-fe42f19e8057"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we want to add this information to our feature table using **`write_table`**. \n\n**NOTE:** Remember, we can use either **`\"overwrite\"`** or **`\"merge\"`** mode. Which one should we use here?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd4698b6-7589-47f0-bff4-070eaaba3882"}}},{"cell_type":"code","source":["# ANSWER\nfs.write_table(\n    name=table_name,\n    df=add_df,\n    mode=\"merge\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"980d3249-c5fa-4d04-8730-f8ac147d4644"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now try using **`fs.read_table`**, specifying the **`table_name`** to see our updated feature table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98eb8b90-59a1-4b57-a0bc-d56b2a300123"}}},{"cell_type":"code","source":["# ANSWER\nupdated_df = fs.read_table(table_name)\n\ndisplay(updated_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9086d0cb-921a-499b-addb-1892f7f7f300"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Training \n\nNow that we have our feature table, we are ready to use it for model training. We'll need our target variable **`deceased`** in addition to our features, so let's get that first."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2fb8f55-ccee-4451-8cce-8005b461a5df"}}},{"cell_type":"code","source":["target_df = covid_df.select([\"index\", \"deceased\"])\n\ndisplay(target_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62fef5f5-22a4-48e4-9700-46a4b7275055"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's create our training and test datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b861a966-ebc4-4c54-98c8-2ecd6ba3b51c"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\ndef load_data(table_name, lookup_key):\n    model_feature_lookups = [feature_store.FeatureLookup(table_name=table_name, lookup_key=lookup_key)]\n\n    # fs.create_training_set will look up features in model_feature_lookups with matched key from inference_data_df\n    training_set = fs.create_training_set(target_df, model_feature_lookups, label=\"deceased\", exclude_columns=[\"index\",\"date\"])\n    training_pd = training_set.load_df().toPandas()\n\n    # Create train and test datasets\n    X = training_pd.drop(\"deceased\", axis=1)\n    y = training_pd[\"deceased\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test, training_set\n\nX_train, X_test, y_train, y_test, training_set = load_data(table_name, \"index\")\nX_train.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4db03fcc-322f-47c2-9112-81ce14136ceb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we can train a model and register it to the feature store."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6909cf17-1473-4282-be2e-6523c4abdd2a"}}},{"cell_type":"code","source":["from mlflow.tracking.client import MlflowClient\n\nclient = MlflowClient()\ntry:\n    client.delete_registered_model(f\"feature_store_covid_{cleaned_username}\") # Deleting model if already created\nexcept:\n    None"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00cdeef1-a195-4a87-90ae-ae7f7e3bcbbd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import mlflow\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom mlflow.models.signature import infer_signature\n\ndef train_model(table_name):\n    X_train, X_test, y_train, y_test, training_set = load_data(table_name, \"index\")\n\n    ## fit and log model\n    with mlflow.start_run() as run:\n\n        rf = RandomForestRegressor(max_depth=3, n_estimators=20, random_state=42)\n        rf.fit(X_train, y_train)\n        y_pred = rf.predict(X_test)\n\n        mlflow.log_metric(\"mse\", mean_squared_error(y_test, y_pred))\n        mlflow.log_metric(\"r2\", r2_score(y_test, y_pred))\n\n        fs.log_model(\n            model=rf,\n            artifact_path=\"feature-store-model\",\n            flavor=mlflow.sklearn,\n            training_set=training_set,\n            registered_model_name=f\"feature_store_covid_{cleaned_username}\",\n            input_example=X_train[:5],\n            signature=infer_signature(X_train, y_train)\n        )\n    \ntrain_model(table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"834e4361-61d8-4fac-bf62-92fd22aa7443"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we have a trained model! Check the Feature Store UI to see that our model is now there. Can you tell which features this model uses from that table and which we excluded?\n\nFinally, let's apply the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"921ecba4-75a7-4987-9dc7-dd3ba09367a6"}}},{"cell_type":"code","source":["## For sake of simplicity, we will just predict on the same inference_data_df\nbatch_input_df = target_df.drop(\"deceased\") # Exclude true label\npredictions_df = fs.score_batch(f\"models:/feature_store_covid_{cleaned_username}/1\", \n                                  batch_input_df, result_type=\"double\")\ndisplay(predictions_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2361e72-f5ab-4342-b2bb-0fc1141262d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"286e5491-f3c6-4e2a-b6f8-917b09615b6f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 10L - Feature Store Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157727548}},"nbformat":4,"nbformat_minor":0}
