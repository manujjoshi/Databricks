{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca8f94dd-c38e-4a85-ac5f-a9135c7730ee"}}},{"cell_type":"markdown","source":["# Hyperopt Lab\n\n**The <a href=\"https://github.com/hyperopt/hyperopt\" target=\"_blank\">Hyperopt library</a> allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). With MLflow, we can record the hyperparameters and corresponding metrics for each hyperparameter combination. You can read more on <a href=\"https://github.com/hyperopt/hyperopt/blob/master/docs/templates/scaleout/spark.md\" target=\"_blank\">SparkTrials w/ Hyperopt</a>.**\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n- **Learn how to distribute tuning tasks when training a single-node machine learning model by using **`SparkTrials`** class, rather than the default **`Trials`** class.** \n\n> SparkTrials fits and evaluates each model on one Spark executor, allowing massive scale-out for tuning. To use SparkTrials with Hyperopt, simply pass the SparkTrials object to Hyperopt's fmin() function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07464841-1ab0-49c7-9fbd-e8300f76894c"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c45ad7f-2ae3-4d88-b7a2-438ca808d7bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Defining courseware-specific utility methods...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Defining courseware-specific utility methods..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The source for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/scalable-machine-learning-with-apache-spark/v01/\n\nYour dataset directory is\ndbfs:/user/manujkumar.joshi@celebaltech.com/dbacademy/machine_learning/datasets\n\nSkipping install of existing dataset.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The source for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/scalable-machine-learning-with-apache-spark/v01/\n\nYour dataset directory is\ndbfs:/user/manujkumar.joshi@celebaltech.com/dbacademy/machine_learning/datasets\n\nSkipping install of existing dataset.\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Read in a cleaned version of the Airbnb dataset with just numeric features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a5e2519-0a29-45f9-b3e9-3395b36afe7b"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndf = pd.read_csv(f\"{datasets_dir}/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\".replace(\"dbfs:/\", \"/dbfs/\")).drop([\"zipcode\"], axis=1)\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1),\n                                                    df[[\"price\"]].values.ravel(),\n                                                    test_size = 0.2,\n                                                    random_state = 42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"397f1ee8-1d96-4f86-b412-f79bef4dc7c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we need to define an **`objective_function`** where you evaluate the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\">random forest's</a> predictions using R2.\n\nIn the code below, compute the **`r2`** and return it (remember we are trying to maximize R2, so we need to return it as a negative value)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32c561c7-dc7c-475c-a671-cd5d87277c02"}}},{"cell_type":"code","source":["# ANSWER\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, r2_score\nfrom numpy import mean\n  \ndef objective_function(params):\n    # set the hyperparameters that we want to tune:\n    max_depth = params[\"max_depth\"]\n    max_features = params[\"max_features\"]\n\n    regressor = RandomForestRegressor(max_depth=max_depth, max_features=max_features, random_state=42)\n\n    # Evaluate predictions\n    r2 = mean(cross_val_score(regressor, X_train, y_train, cv=3))\n\n    # Note: since we aim to maximize r2, we need to return it as a negative value (\"loss\": -metric)\n    return -r2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ffe5651-ffc3-4c4c-91f3-4855b39b039e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We need to define a search space for HyperOpt. Let the **`max_depth`** vary between 2-10, and **`max_features`** be one of: \"auto\", \"sqrt\", or \"log2\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f648f39d-b269-47a9-b1e5-e02d0d7f657c"}}},{"cell_type":"code","source":["#%pip install hyperopt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3aa2cad-75d7-4242-a0fb-25f49df60992"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nfrom hyperopt import hp\n\nmax_features_choices =  [\"auto\", \"sqrt\", \"log2\"]\nsearch_space = {\n    \"max_depth\": hp.quniform(\"max_depth\", 2, 10, 1),\n    \"max_features\": hp.choice(\"max_features\", max_features_choices)\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42cf4fb2-b36d-4183-b5e7-73c781598c96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Instead of using the default **`Trials`** class, you can leverage the **`SparkTrials`** class to trigger the distribution of tuning tasks across Spark executors. On Databricks, SparkTrials are automatically logged with MLflow.\n\n**`SparkTrials`** takes 3 optional arguments, namely **`parallelism`**, **`timeout`**, and **`spark_session`**. You can refer to this <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">page</a> to read more.\n\nIn the code below, fill in the **`fmin`** function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2394323a-d27f-4705-b134-332efa25c4dc"}}},{"cell_type":"code","source":["# ANSWER\nfrom hyperopt import fmin, tpe, SparkTrials\nimport mlflow\nimport numpy as np\n\n# Number of models to evaluate\nnum_evals = 8\n# Number of models to train concurrently\nspark_trials = SparkTrials(parallelism=2)\n# Automatically logs to MLflow\nbest_hyperparam = fmin(fn=objective_function, \n                       space=search_space,\n                       algo=tpe.suggest, \n                       trials=spark_trials,\n                       max_evals=num_evals,\n                       rstate=np.random.default_rng(42))\n\n# Re-train best model and log metrics on test dataset\nwith mlflow.start_run(run_name=\"best_model\"):\n    # get optimal hyperparameter values\n    best_max_depth = best_hyperparam[\"max_depth\"]\n    best_max_features = max_features_choices[best_hyperparam[\"max_features\"]]\n\n    # train model on entire training data\n    regressor = RandomForestRegressor(max_depth=best_max_depth, max_features=best_max_features, random_state=42)\n    regressor.fit(X_train, y_train)\n\n    # evaluate on holdout/test data\n    r2 = regressor.score(X_test, y_test)\n\n    # Log param and metric for the final model\n    mlflow.log_param(\"max_depth\", best_max_depth)\n    mlflow.log_param(\"max_features\", best_max_features)\n    mlflow.log_metric(\"loss\", r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"341a2210-44fe-4b62-b47c-1acc736f5d87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\r  0%|          | 0/8 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r/databricks/spark/python/pyspark/rdd.py:980: FutureWarning: Deprecated in 3.1, Use pyspark.InheritableThread with the pinned thread mode enabled.\n  warnings.warn(\n\n\r  0%|          | 0/8 [00:01&lt;?, ?trial/s, best loss=?]\r 12%|█▎        | 1/8 [00:07&lt;00:49,  7.02s/trial, best loss: -0.6468700448781513]\r 25%|██▌       | 2/8 [00:08&lt;00:20,  3.49s/trial, best loss: -0.6488058922093433]\r 50%|█████     | 4/8 [00:10&lt;00:07,  1.93s/trial, best loss: -0.6488058922093433]\r 75%|███████▌  | 6/8 [00:13&lt;00:03,  1.73s/trial, best loss: -0.6488058922093433]\r 88%|████████▊ | 7/8 [00:16&lt;00:02,  2.05s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:17&lt;00:00,  1.77s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:17&lt;00:00,  2.13s/trial, best loss: -0.6658971922113791]\nTotal Trials: 8: 8 succeeded, 0 failed, 0 cancelled.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/8 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r/databricks/spark/python/pyspark/rdd.py:980: FutureWarning: Deprecated in 3.1, Use pyspark.InheritableThread with the pinned thread mode enabled.\n  warnings.warn(\n\n\r  0%|          | 0/8 [00:01&lt;?, ?trial/s, best loss=?]\r 12%|█▎        | 1/8 [00:07&lt;00:49,  7.02s/trial, best loss: -0.6468700448781513]\r 25%|██▌       | 2/8 [00:08&lt;00:20,  3.49s/trial, best loss: -0.6488058922093433]\r 50%|█████     | 4/8 [00:10&lt;00:07,  1.93s/trial, best loss: -0.6488058922093433]\r 75%|███████▌  | 6/8 [00:13&lt;00:03,  1.73s/trial, best loss: -0.6488058922093433]\r 88%|████████▊ | 7/8 [00:16&lt;00:02,  2.05s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:17&lt;00:00,  1.77s/trial, best loss: -0.6658971922113791]\r100%|██████████| 8/8 [00:17&lt;00:00,  2.13s/trial, best loss: -0.6658971922113791]\nTotal Trials: 8: 8 succeeded, 0 failed, 0 cancelled.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now you can compare all of the models using the MLflow UI. \n\nTo understand the effect of tuning a hyperparameter:\n\n0. Select the resulting runs and click Compare.\n0. In the Scatter Plot, select a hyperparameter for the X-axis and loss for the Y-axis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c8b7c96-e743-4298-9241-5b298e3f171d"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95ef3401-839f-4a14-bb74-6b191502c5b5"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 08L - Hyperopt Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157727661}},"nbformat":4,"nbformat_minor":0}
