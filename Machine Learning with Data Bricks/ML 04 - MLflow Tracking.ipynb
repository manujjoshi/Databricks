{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3555cbb3-798c-4889-b2b8-041d1074c447"}}},{"cell_type":"markdown","source":["# MLflow\n\n<a href=\"https://mlflow.org/docs/latest/concepts.html\" target=\"_blank\">MLflow</a> seeks to address these three core issues:\n\n* It’s difficult to keep track of experiments\n* It’s difficult to reproduce code\n* There’s no standard way to package and deploy models\n\nIn the past, when examining a problem, you would have to manually keep track of the many models you created, as well as their associated parameters and metrics. This can quickly become tedious and take up valuable time, which is where MLflow comes in.\n\nMLflow is pre-installed on the Databricks Runtime for ML.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n* Use MLflow to track experiments, log metrics, and compare runs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0360d797-796b-4ac9-90eb-0190d974c8ee"}}},{"cell_type":"markdown","source":["<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-tracking.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84cfc46e-67aa-4e0f-bf8f-16831f9be610"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0e194b8-892f-421b-8a3b-6f85f72b38a7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's start by loading in our SF Airbnb Dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd75cac-9a25-4189-8c73-09b2c82b11e5"}}},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\nairbnb_df = spark.read.format(\"delta\").load(file_path)\n\ntrain_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\nprint(train_df.cache().count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b986f0ab-90fe-4b19-a3c9-ecd5f4da9faa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### MLflow Tracking\n\nMLflow Tracking is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n\nYou can use <a href=\"https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_experiment\" target=\"_blank\">mlflow.set_experiment()</a> to set an experiment, but if you do not specify an experiment, it will automatically be scoped to this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c4eaded-ee07-4bdd-9b0a-3abd03e93d86"}}},{"cell_type":"markdown","source":["### Track Runs\n\nEach run can record the following information:<br><br>\n\n- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n- **Source:** The code that originally ran the experiment\n\n**NOTE**: For Spark models, MLflow can only log PipelineModels."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b133176b-97ed-442e-91c6-6bad4529dda2"}}},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nwith mlflow.start_run(run_name=\"LR-Single-Feature\") as run:\n    # Define pipeline\n    vec_assembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n    lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n    pipeline = Pipeline(stages=[vec_assembler, lr])\n    pipeline_model = pipeline.fit(train_df)\n\n    # Log parameters\n    mlflow.log_param(\"label\", \"price\")\n    mlflow.log_param(\"features\", \"bedrooms\")\n\n    # Log model\n    mlflow.spark.log_model(pipeline_model, \"model\", input_example=train_df.limit(5).toPandas()) \n\n    # Evaluate predictions\n    pred_df = pipeline_model.transform(test_df)\n    regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n    rmse = regression_evaluator.evaluate(pred_df)\n\n    # Log metrics\n    mlflow.log_metric(\"rmse\", rmse)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eeb8cd3c-219b-4c48-b396-6b3aa6b1b95f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There, all done! Let's go through the other two linear regression models and then compare our runs. \n\n**Question**: Does anyone remember the RMSE of the other runs?\n\nNext let's build our linear regression model but use all of our features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a6d29e4-2170-4bbe-8d1a-dc199cb3d643"}}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\n\nwith mlflow.start_run(run_name=\"LR-All-Features\") as run:\n    # Create pipeline\n    r_formula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")\n    lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n    pipeline = Pipeline(stages=[r_formula, lr])\n    pipeline_model = pipeline.fit(train_df)\n\n    # Log pipeline\n    mlflow.spark.log_model(pipeline_model, \"model\", input_example=train_df.limit(5).toPandas())\n\n    # Log parameter\n    mlflow.log_param(\"label\", \"price\")\n    mlflow.log_param(\"features\", \"all_features\")\n\n    # Create predictions and metrics\n    pred_df = pipeline_model.transform(test_df)\n    regression_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n\n    # Log both metrics\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"r2\", r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17dc8348-f122-40b1-9921-1459dadb58b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we will use Linear Regression to predict the log of the price, due to its log normal distribution. \n\nWe'll also practice logging artifacts to keep a visual of our log normal histogram."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61e42cc9-fee2-43c8-aa59-1fb061ee18f6"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, log, exp\nimport matplotlib.pyplot as plt\n\nwith mlflow.start_run(run_name=\"LR-Log-Price\") as run:\n    # Take log of price\n    log_train_df = train_df.withColumn(\"log_price\", log(col(\"price\")))\n    log_test_df = test_df.withColumn(\"log_price\", log(col(\"price\")))\n\n    # Log parameter\n    mlflow.log_param(\"label\", \"log_price\")\n    mlflow.log_param(\"features\", \"all_features\")\n\n    # Create pipeline\n    r_formula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\")  \n    lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_prediction\")\n    pipeline = Pipeline(stages=[r_formula, lr])\n    pipeline_model = pipeline.fit(log_train_df)\n\n    # Log model\n    mlflow.spark.log_model(pipeline_model, \"log-model\", input_example=log_train_df.limit(5).toPandas())\n\n    # Make predictions\n    pred_df = pipeline_model.transform(log_test_df)\n    exp_df = pred_df.withColumn(\"prediction\", exp(col(\"log_prediction\")))\n\n    # Evaluate predictions\n    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(exp_df)\n\n    # Log metrics\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.log_metric(\"r2\", r2)\n\n    # Log artifact\n    plt.clf()\n\n    log_train_df.toPandas().hist(column=\"log_price\", bins=100)\n    fig = plt.gcf()\n    mlflow.log_figure(fig, username + \"_log_normal.png\")\n    plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a685410-ef70-4fed-9dad-68eca222dd62"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["That's it! Now, let's use MLflow to easily look over our work and compare model performance. You can either query past runs programmatically or use the MLflow UI."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaf8d859-89a6-4557-a891-19a827259540"}}},{"cell_type":"markdown","source":["### Querying Past Runs\n\nYou can query past runs programmatically in order to use this data back in Python.  The pathway to doing this is an **`MlflowClient`** object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48632b70-7591-45fe-a3db-3679beefa171"}}},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acb12e61-a251-4d93-a734-2dace3f70689"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["client.list_experiments()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"067e4cd7-9b7f-47bd-8c49-6d639b95d942"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also use <a href=\"https://mlflow.org/docs/latest/search-syntax.html\" target=\"_blank\">search_runs</a> to find all runs for a given experiment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"041256f1-9242-4da5-a7da-5287d9d0b42b"}}},{"cell_type":"code","source":["experiment_id = run.info.experiment_id\nruns_df = mlflow.search_runs(experiment_id)\n\ndisplay(runs_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5488bba-0766-423f-987a-e02bc5e08188"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Pull the last run and look at metrics."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46b63aa5-50e8-49d3-a1aa-81b3cbe136b4"}}},{"cell_type":"code","source":["runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=1)\nruns[0].data.metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09b241ae-8292-4f36-99b1-7772a6156529"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["runs[0].info.run_id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dfcc95c-9712-4f7b-ad62-fef46751f7a9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Examine the results in the UI.  Look for the following:<br><br>\n\n1. The **`Experiment ID`**\n2. The artifact location.  This is where the artifacts are stored in DBFS.\n3. The time the run was executed.  **Click this to see more information on the run.**\n4. The code that executed the run.\n\n\nAfter clicking on the time of the run, take a look at the following:<br><br>\n\n1. The Run ID will match what we printed above\n2. The model that we saved, included a pickled version of the model as well as the Conda environment and the **`MLmodel`** file.\n\nNote that you can add notes under the \"Notes\" tab to help keep track of important information about your models. \n\nAlso, click on the run for the log normal distribution and see that the histogram is saved in \"Artifacts\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef21130d-2f0e-4b91-aebd-3d355386e275"}}},{"cell_type":"markdown","source":["### Load Saved Model\n,\nLet's practice <a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html\" target=\"_blank\">loading</a> our logged log-normal model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd2789ef-dad4-467b-8169-ef579a176b8a"}}},{"cell_type":"code","source":["model_path = f\"runs:/{run.info.run_id}/log-model\"\nloaded_model = mlflow.spark.load_model(model_path)\n\ndisplay(loaded_model.transform(test_df))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc1114ab-db67-4211-a191-4b409f98ca76"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9596b482-f7fb-46a6-a48c-a0ede8a0b5f9"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 04 - MLflow Tracking","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726802}},"nbformat":4,"nbformat_minor":0}
