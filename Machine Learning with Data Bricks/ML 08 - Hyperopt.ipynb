{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8afefbb7-3d23-4808-92f7-9ef9021d0066"}}},{"cell_type":"markdown","source":["# Hyperopt\n\nHyperopt is a Python library for \"serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions\".\n\nIn the machine learning workflow, hyperopt can be used to distribute/parallelize the hyperparameter optimization process with more advanced optimization strategies than are available in other libraries.\n\nThere are two ways to scale hyperopt with Apache Spark:\n* Use single-machine hyperopt with a distributed training algorithm (e.g. MLlib)\n* Use distributed hyperopt with single-machine training algorithms (e.g. scikit-learn) with the SparkTrials class. \n\nIn this lesson, we will use single-machine hyperopt with MLlib, but in the lab, you will see how to use hyperopt to distribute the hyperparameter tuning of single node models. \n\nUnfortunately you can’t use hyperopt to distribute the hyperparameter optimization for distributed training algorithms at this time. However, you do still get the benefit of using more advanced hyperparameter search algorthims (random search, TPE, etc.) with Spark ML.\n\n\nResources:\n\n0. <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">Documentation</a>\n0. <a href=\"https://docs.databricks.com/applications/machine-learning/automl/hyperopt/index.html\" target=\"_blank\">Hyperopt on Databricks</a>\n0. <a href=\"https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html\" target=\"_blank\">Hyperparameter Tuning with MLflow, Apache Spark MLlib and Hyperopt</a>\n0. <a href=\"https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html\" target=\"_blank\">How (Not) to Tune Your Model With Hyperopt</a>\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Use hyperopt to find the optimal parameters for an MLlib model using TPE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"491cfcdf-d187-4881-9cec-f25310f1d66e"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33680bab-73b1-47b4-87ab-97a8da5c27e1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's start by loading in our SF Airbnb Dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"827043b7-5e15-440a-83fe-26f74d084ad2"}}},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\nairbnb_df = spark.read.format(\"delta\").load(file_path)\ntrain_df, val_df, test_df = airbnb_df.randomSplit([.6, .2, .2], seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e491b861-ace4-4ebe-9ae9-9750a8708eed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We will then create our random forest pipeline and regression evaluator."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fb42e58-e763-4ff9-9d40-4f9511f27abf"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ncategorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\nindex_output_cols = [x + \"Index\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n\nnumeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\nassembler_inputs = index_output_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)\npipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])\nregression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cb96de1-187a-4151-bf8b-ac411af17868"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we get to the hyperopt-specific part of the workflow.\n\nFirst, we define our **objective function**. The objective function has two primary requirements:\n\n1. An **input** **`params`** including hyperparameter values to use when training the model\n2. An **output** containing a loss metric on which to optimize\n\nIn this case, we are specifying values of **`max_depth`** and **`num_trees`** and returning the RMSE as our loss metric.\n\nWe are reconstructing our pipeline for the **`RandomForestRegressor`** to use the specified hyperparameter values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebeae4ff-925a-49bd-b754-3863cbea7c04"}}},{"cell_type":"code","source":["def objective_function(params):    \n    # set the hyperparameters that we want to tune\n    max_depth = params[\"max_depth\"]\n    num_trees = params[\"num_trees\"]\n\n    with mlflow.start_run():\n        estimator = pipeline.copy({rf.maxDepth: max_depth, rf.numTrees: num_trees})\n        model = estimator.fit(train_df)\n\n        preds = model.transform(val_df)\n        rmse = regression_evaluator.evaluate(preds)\n        mlflow.log_metric(\"rmse\", rmse)\n\n    return rmse"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dabd075-ef2f-4a2c-8561-0b4d7929c792"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we define our search space. \n\nThis is similar to the parameter grid in a grid search process. However, we are only specifying the range of values rather than the individual, specific values to be tested. It's up to hyperopt's optimization algorithm to choose the actual values.\n\nSee the <a href=\"https://github.com/hyperopt/hyperopt/wiki/FMin\" target=\"_blank\">documentation</a> for helpful tips on defining your search space."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be27f658-a113-4ede-8ffb-15e9b89781f0"}}},{"cell_type":"code","source":["from hyperopt import hp\n\nsearch_space = {\n    \"max_depth\": hp.quniform(\"max_depth\", 2, 5, 1),\n    \"num_trees\": hp.quniform(\"num_trees\", 10, 100, 1)\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbe2c5d2-1853-4c0d-aaef-4f33af644329"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**`fmin()`** generates new hyperparameter configurations to use for your **`objective_function`**. It will evaluate 4 models in total, using the information from the previous models to make a more informative decision for the the next hyperparameter to try. \n\nHyperopt allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). Note that in the cell below, we are importing **`tpe`**. According to the <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">documentation</a>, TPE is an adaptive algorithm that \n\n> iteratively explores the hyperparameter space. Each new hyperparameter setting tested will be chosen based on previous results. \n\nHence, **`tpe.suggest`** is a Bayesian method.\n\nMLflow also integrates with Hyperopt, so you can track the results of all the models you’ve trained and their results as part of your hyperparameter tuning. Notice you can track the MLflow experiment in this notebook, but you can also specify an external experiment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9e74866-2240-4485-aece-f22d8a825326"}}},{"cell_type":"code","source":["from hyperopt import fmin, tpe, Trials\nimport numpy as np\nimport mlflow\nimport mlflow.spark\nmlflow.pyspark.ml.autolog(log_models=False)\n\nnum_evals = 4\ntrials = Trials()\nbest_hyperparam = fmin(fn=objective_function, \n                       space=search_space,\n                       algo=tpe.suggest, \n                       max_evals=num_evals,\n                       trials=trials,\n                       rstate=np.random.default_rng(42))\n\n# Retrain model on train & validation dataset and evaluate on test dataset\nwith mlflow.start_run():\n    best_max_depth = best_hyperparam[\"max_depth\"]\n    best_num_trees = best_hyperparam[\"num_trees\"]\n    estimator = pipeline.copy({rf.maxDepth: best_max_depth, rf.numTrees: best_num_trees})\n    combined_df = train_df.union(val_df) # Combine train & validation together\n\n    pipeline_model = estimator.fit(combined_df)\n    pred_df = pipeline_model.transform(test_df)\n    rmse = regression_evaluator.evaluate(pred_df)\n\n    # Log param and metrics for the final model\n    mlflow.log_param(\"maxDepth\", best_max_depth)\n    mlflow.log_param(\"numTrees\", best_num_trees)\n    mlflow.log_metric(\"rmse\", rmse)\n    mlflow.spark.log_model(pipeline_model, \"model\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81d0d2b0-b241-46ab-84d9-80d48a5f3151"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2903f8e0-6aee-4b6e-92ae-82ce15fcebe2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 08 - Hyperopt","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157728035}},"nbformat":4,"nbformat_minor":0}
