{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3491b742-7f9b-41af-9798-394c93943013"}}},{"cell_type":"markdown","source":["# Data Cleansing\n\nWe will be using Spark to do some exploratory data analysis & cleansing of the SF Airbnb rental dataset from <a href=\"http://insideairbnb.com/get-the-data.html\" target=\"_blank\">Inside Airbnb</a>.\n\n<img src=\"https://files.training.databricks.com/images/301/sf.jpg\" style=\"height: 200px; margin: 10px; border: 1px solid #ddd; padding: 10px\"/>\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Impute missing values\n - Identify & remove outliers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e7c8a17-a348-498e-8fe9-22101d1159d1"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27c1af37-5d4b-4dd5-af39-c05b5b564ba1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's load the Airbnb dataset in."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68635560-9d4e-4197-ba9f-3c1ecd0623c2"}}},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06.csv\"\n\nraw_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n\ndisplay(raw_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"456c9087-a1ef-4df9-b7fe-d9eaf2d66cb7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["raw_df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05fbde16-79bc-4c4f-840d-df11f0016cf1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For the sake of simplicity, only keep certain columns from this dataset. We will talk about feature selection later."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e454104f-2c9b-4a4e-8191-5a67bd91f0a1"}}},{"cell_type":"code","source":["columns_to_keep = [\n    \"host_is_superhost\",\n    \"cancellation_policy\",\n    \"instant_bookable\",\n    \"host_total_listings_count\",\n    \"neighbourhood_cleansed\",\n    \"latitude\",\n    \"longitude\",\n    \"property_type\",\n    \"room_type\",\n    \"accommodates\",\n    \"bathrooms\",\n    \"bedrooms\",\n    \"beds\",\n    \"bed_type\",\n    \"minimum_nights\",\n    \"number_of_reviews\",\n    \"review_scores_rating\",\n    \"review_scores_accuracy\",\n    \"review_scores_cleanliness\",\n    \"review_scores_checkin\",\n    \"review_scores_communication\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n    \"price\"\n]\n\nbase_df = raw_df.select(columns_to_keep)\nbase_df.cache().count()\ndisplay(base_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28c160b6-2d40-4d8e-b585-22e041f74600"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Fixing Data Types\n\nTake a look at the schema above. You'll notice that the **`price`** field got picked up as string. For our task, we need it to be a numeric (double type) field. \n\nLet's fix that."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ea0b19f-3dd8-4ead-8870-4af7e322d7dc"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, translate\n\nfixed_price_df = base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n\ndisplay(fixed_price_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"445da5a8-3533-40c2-80c5-a200cc373a36"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Summary statistics\n\nTwo options:\n* **`describe`**: count, mean, stddev, min, max\n* **`summary`**: describe + interquartile range (IQR)\n\n**Question:** When to use IQR/median over mean? Vice versa?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b07e7128-21ae-4c72-b8b5-ebe574c71c07"}}},{"cell_type":"code","source":["display(fixed_price_df.describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa7d0fe2-49bd-4b42-ab74-7fb5ad9ebde5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(fixed_price_df.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a58a3e88-77bf-49c5-b7fe-c3f9433bd93f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Dbutils Data Summary\n\nWe can also use **`dbutils.data.summarize`** to see more detailed summary statistics and data plots."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e425692e-c8d6-443e-89c7-c182066ffb39"}}},{"cell_type":"code","source":["dbutils.data.summarize(fixed_price_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f6f3648-bd61-4bb9-8139-6436afcd7820"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Getting rid of extreme values\n\nLet's take a look at the *min* and *max* values of the **`price`** column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47670c0b-fde1-46f1-adf0-c21753f82bd5"}}},{"cell_type":"code","source":["display(fixed_price_df.select(\"price\").describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b70924df-15d6-43a5-8c07-a2c33a8da964"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There are some super-expensive listings, but it's up to the SME (Subject Matter Experts) to decide what to do with them. We can certainly filter the \"free\" Airbnbs though.\n\nLet's see first how many listings we can find where the *price* is zero."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a87f838-e765-4789-a78d-d8ec8c734663"}}},{"cell_type":"code","source":["fixed_price_df.filter(col(\"price\") == 0).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4a959b9-745f-41d0-b379-88cfdf953a17"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now only keep rows with a strictly positive *price*."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee236b04-4ba9-4378-9366-9251bcc3f284"}}},{"cell_type":"code","source":["pos_prices_df = fixed_price_df.filter(col(\"price\") > 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96add526-c6fd-4e1f-ad1c-ccacd07eb37b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at the *min* and *max* values of the *minimum_nights* column:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"882a705f-6776-4617-ac94-6264d0ffb6f4"}}},{"cell_type":"code","source":["display(pos_prices_df.select(\"minimum_nights\").describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92dcdfe5-3de4-4e56-81fa-30f73bd45b9c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(pos_prices_df\n        .groupBy(\"minimum_nights\").count()\n        .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9febc7d5-daa9-4eda-9613-3fd332f4f4b0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A minimum stay of one year seems to be a reasonable limit here. Let's filter out those records where the *minimum_nights* is greater then 365."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7342afc-f909-4501-b4df-548d129a0acb"}}},{"cell_type":"code","source":["min_nights_df = pos_prices_df.filter(col(\"minimum_nights\") <= 365)\n\ndisplay(min_nights_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccaada47-28ac-4183-9747-67e239b6ac2b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Handling Null Values\n\nThere are a lot of different ways to handle null values. Sometimes, null can actually be a key indicator of the thing you are trying to predict (e.g. if you don't fill in certain portions of a form, probability of it getting approved decreases).\n\nSome ways to handle nulls:\n* Drop any records that contain nulls\n* Numeric:\n  * Replace them with mean/median/zero/etc.\n* Categorical:\n  * Replace them with the mode\n  * Create a special category for null\n* Use techniques like ALS (Alternating Least Squares) which are designed to impute missing values\n  \n**If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed.**\n\nSparkML's Imputer (covered below) does not support imputation for categorical features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58df871d-5991-48b3-a9ba-249aac750143"}}},{"cell_type":"markdown","source":["### Impute: Cast to Double\n\nSparkML's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\">Imputer </a> requires all fields be of type double. Let's cast all integer fields to double."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc10635e-2106-46eb-8bd2-28205c0ee63c"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType\n\ninteger_columns = [x.name for x in min_nights_df.schema.fields if x.dataType == IntegerType()]\ndoubles_df = min_nights_df\n\nfor c in integer_columns:\n    doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n\ncolumns = \"\\n - \".join(integer_columns)\nprint(f\"Columns converted from Integer to Double:\\n - {columns}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c2a91f3-8c08-47f3-81ec-c5900ae88cdf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Add a dummy column to denote presence of null values before imputing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0cea36a-61a2-4a0b-abd3-eb83f1ad2dee"}}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n\nimpute_cols = [\n    \"bedrooms\",\n    \"bathrooms\",\n    \"beds\", \n    \"review_scores_rating\",\n    \"review_scores_accuracy\",\n    \"review_scores_cleanliness\",\n    \"review_scores_checkin\",\n    \"review_scores_communication\",\n    \"review_scores_location\",\n    \"review_scores_value\"\n]\n\nfor c in impute_cols:\n    doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e734e687-8956-4ae0-84a9-bd297ce62c3a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(doubles_df.describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b8e6a98-5274-42ea-8a02-fb69d1b77dda"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Transformers and Estimators\n\nSpark ML standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. Let's cover two key concepts introduced by the Spark ML API: **`transformers`** and **`estimators`**.\n\n**Transformer**: Transforms one DataFrame into another DataFrame. It accepts a DataFrame as input, and returns a new DataFrame with one or more columns appended to it. Transformers do not learn any parameters from your data and simply apply rule-based transformations. It has a **`.transform()`** method.\n\n**Estimator**: An algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model. It has a **`.fit()`** method because it learns (or \"fits\") parameters from your DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ac0948-3376-4cbc-ba58-272740823a4e"}}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\n\nimputer = Imputer(strategy=\"median\", inputCols=impute_cols, outputCols=impute_cols)\n\nimputer_model = imputer.fit(doubles_df)\nimputed_df = imputer_model.transform(doubles_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acb50a40-8f60-456b-8c3b-2d901e092922"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["OK, our data is cleansed now. Let's save this DataFrame to Delta so that we can start building models with it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4db29ea-be45-4dd2-89b6-7fa5061605e5"}}},{"cell_type":"code","source":["imputed_df.write.format(\"delta\").mode(\"overwrite\").save(working_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8da5a11-b367-41cf-9d8f-69da84263853"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9dee485-64f9-4324-8ae9-5916d2cfedd5"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 01 - Data Cleansing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726858}},"nbformat":4,"nbformat_minor":0}
