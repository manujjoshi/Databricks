{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92e125ce-d1a4-419e-a256-a965fc5f81de"}}},{"cell_type":"markdown","source":["# Linear Regression: Improving our model\n\nIn this notebook we will be adding additional features to our model, as well as discuss how to handle categorical features.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - One Hot Encode categorical variables\n - Use the Pipeline API\n - Save and load models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d97cdd1-9e1a-421b-b656-0bd07813de5a"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac7f6bc2-d661-43fa-9312-5500cc8ac6fe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["file_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\nairbnb_df = spark.read.format(\"delta\").load(file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab2fac35-05fe-440a-a28a-3ab9e66a6031"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Train/Test Split\n\nLet's use the same 80/20 split with the same seed as the previous notebook so we can compare our results apples to apples (unless you changed the cluster config!)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5da3dd33-78db-44d5-be6b-167813f64ba5"}}},{"cell_type":"code","source":["train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bf5f03c-fcbe-48b1-8a8d-0a0dbb15a482"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Categorical Variables\n\nThere are a few ways to handle categorical features:\n* Assign them a numeric value\n* Create \"dummy\" variables (also known as One Hot Encoding)\n* Generate embeddings (mainly used for textual data)\n\n### One Hot Encoder\nHere, we are going to One Hot Encode (OHE) our categorical variables. Spark doesn't have a **`dummies`** function, and OHE is a two step process. First, we need to use <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a> to map a string column of labels to an ML column of label indices.\n\nThen, we can apply the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html?highlight=onehotencoder#pyspark.ml.feature.OneHotEncoder\" target=\"_blank\">OneHotEncoder</a> to the output of the StringIndexer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"022ec275-6d99-4921-bc8c-65db1aa725dd"}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ncategorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\nindex_output_cols = [x + \"Index\" for x in categorical_cols]\nohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\nohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90a80cf7-16ea-4b91-9d39-573e8875be92"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Vector Assembler\n\nNow we can combine our OHE categorical features with our numeric features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34acd50e-20bf-416c-b2d4-b7bf39a86616"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nnumeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\nassembler_inputs = ohe_output_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df6068b0-7981-423d-88d4-0a8637ec8b58"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Linear Regression\n\nNow that we have all of our features, let's build a linear regression model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"082a3369-d9f0-4b29-b4c2-af7a9e4f060e"}}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90a5cdf4-d363-4c82-98e3-2f783617afd5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Pipeline\n\nLet's put all these stages in a Pipeline. A <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html?highlight=pipeline#pyspark.ml.Pipeline\" target=\"_blank\">Pipeline</a> is a way of organizing all of our transformers and estimators.\n\nThis way, we don't have to worry about remembering the same ordering of transformations to apply to our test dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a86d78b-c700-47cd-9d0e-356616a972de"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\nstages = [string_indexer, ohe_encoder, vec_assembler, lr]\npipeline = Pipeline(stages=stages)\n\npipeline_model = pipeline.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10aa6644-5ab1-494a-ad7f-112dd3086582"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Saving Models\n\nWe can save our models to persistent storage (e.g. DBFS) in case our cluster goes down so we don't have to recompute our results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09dd3e66-333a-4d4f-bbb0-28ab43748fee"}}},{"cell_type":"code","source":["pipeline_model.write().overwrite().save(working_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19ba4ed7-f246-4e5f-a42f-8a5750dc6c4b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Loading models\n\nWhen you load in models, you need to know the type of model you are loading back in (was it a linear regression or logistic regression model?).\n\nFor this reason, we recommend you always put your transformers/estimators into a Pipeline, so you can always load the generic PipelineModel back in."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62770760-cb92-4a02-8726-1bd2f208ee09"}}},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\n\nsaved_pipeline_model = PipelineModel.load(working_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7f1dcc2-df60-4448-900e-f3ea6de2b6d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Apply model to test set"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5d3d26c-48da-4ed8-93fb-0b0b390a944d"}}},{"cell_type":"code","source":["pred_df = saved_pipeline_model.transform(test_df)\n\ndisplay(pred_df.select(\"features\", \"price\", \"prediction\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80872ffb-96b0-4149-99e8-be88dd01ee9b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Evaluate model\n\n![](https://files.training.databricks.com/images/r2d2.jpg) How is our R2 doing?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b93345c7-0818-4565-836d-a64bf07bc54f"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n\nrmse = regression_evaluator.evaluate(pred_df)\nr2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\nprint(f\"RMSE is {rmse}\")\nprint(f\"R2 is {r2}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce72577f-e3ca-4644-8708-fb1c6b7024ea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As you can see, our RMSE decreased when compared to the model without one-hot encoding, and the R2 increased as well!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b08ba3e5-cc96-4b36-a91b-126542fab39b"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f05fcba8-c63c-413e-a700-32d44a25fab6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 03 - Linear Regression II","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726760}},"nbformat":4,"nbformat_minor":0}
