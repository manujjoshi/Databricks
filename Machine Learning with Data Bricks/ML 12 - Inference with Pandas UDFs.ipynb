{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f2668f5-2af8-441f-9ac8-5230ae52971e"}}},{"cell_type":"markdown","source":["# Inference with Pandas UDFs\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n- Build a scikit-learn model, track it with MLflow, and apply it at scale using the Pandas Scalar Iterator UDFs and **`mapInPandas()`**\n\nTo learn more about Pandas UDFs, you can refer to this <a href=\"https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\" target=\"_blank\">blog post</a> to see what's new in Spark 3.0."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f16689ac-5a86-4af1-8a68-d619bc2f787f"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"021850a4-f04d-4f05-bd68-de963fb1baed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Train sklearn model and log it with MLflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f0e1e97-023c-4fe2-b895-e2de8a890079"}}},{"cell_type":"code","source":["import mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nwith mlflow.start_run(run_name=\"sklearn-random-forest\") as run:\n    # Enable autologging \n    mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True, log_models=True)\n    # Import the data\n    df = pd.read_csv(f\"{datasets_dir}/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\".replace(\"dbfs:/\", \"/dbfs/\")).drop([\"zipcode\"], axis=1)\n    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n\n    # Create model\n    rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n    rf.fit(X_train, y_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d96ca5b-a42e-4568-8673-8520b96e9e27"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Create Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10fc19e0-ba65-4018-88dc-d1661a27f528"}}},{"cell_type":"code","source":["spark_df = spark.createDataFrame(X_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab65d366-6ed5-4af7-902f-06daadd5ff84"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pandas/Vectorized UDFs\n\nAs of Spark 2.3, there are Pandas UDFs available in Python to improve the efficiency of UDFs. Pandas UDFs utilize Apache Arrow to speed up computation. Let's see how that helps improve our processing time.\n\n* <a href=\"https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\" target=\"_blank\">Blog post</a>\n* <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#pyspark-usage-guide-for-pandas-with-apache-arrow\" target=\"_blank\">Documentation</a>\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\" alt=\"Benchmark\" width =\"500\" height=\"1500\">\n\nThe user-defined functions are executed by: \n* <a href=\"https://arrow.apache.org/\" target=\"_blank\">Apache Arrow</a>, is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost. See more <a href=\"https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html\" target=\"_blank\">here</a>.\n* pandas inside the function, to work with pandas instances and APIs.\n\n**NOTE**: In Spark 3.0, you should define your Pandas UDF using Python type hints."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf61013d-c27d-4e1b-8982-b37f3f6fe0d0"}}},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf\n\n@pandas_udf(\"double\")\ndef predict(*args: pd.Series) -> pd.Series:\n    model_path = f\"runs:/{run.info.run_id}/model\" \n    model = mlflow.sklearn.load_model(model_path) # Load model\n    pdf = pd.concat(args, axis=1)\n    return pd.Series(model.predict(pdf))\n\nprediction_df = spark_df.withColumn(\"prediction\", predict(*spark_df.columns))\ndisplay(prediction_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d2fe3c4-f02a-4628-bbac-3d07f702a80e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pandas Scalar Iterator UDF\n\nIf your model is very large, then there is high overhead for the Pandas UDF to repeatedly load the same model for every batch in the same Python worker process. In Spark 3.0, Pandas UDFs can accept an iterator of pandas.Series or pandas.DataFrame so that you can load the model only once instead of loading it for every series in the iterator.\n\nThis way the cost of any set-up needed will be incurred fewer times. When the number of records youâ€™re working with is greater than **`spark.conf.get('spark.sql.execution.arrow.maxRecordsPerBatch')`**, which is 10,000 by default, you should see speed ups over a pandas scalar UDF because it iterates through batches of pd.Series.\n\nIt has the general syntax of: \n\n**`@pandas_udf(...)\ndef predict(iterator):\n    model = ... # load model\n    for features in iterator:\n        yield model.predict(features)`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21071ba6-b016-41e3-817a-9ade02995001"}}},{"cell_type":"code","source":["from typing import Iterator, Tuple\n\n@pandas_udf(\"double\")\ndef predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n    model_path = f\"runs:/{run.info.run_id}/model\" \n    model = mlflow.sklearn.load_model(model_path) # Load model\n    for features in iterator:\n        pdf = pd.concat(features, axis=1)\n        yield pd.Series(model.predict(pdf))\n\nprediction_df = spark_df.withColumn(\"prediction\", predict(*spark_df.columns))\ndisplay(prediction_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"920083b1-8830-4a6b-8785-c70d96c8fcc2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pandas Function API\n\nInstead of using a Pandas UDF, we can use a Pandas Function API. This new category in Apache Spark 3.0 enables you to directly apply a Python native function, which takes and outputs Pandas instances against a PySpark DataFrame. Pandas Functions APIs supported in Apache Spark 3.0 are: grouped map, map, and co-grouped map.\n\n**`mapInPandas()`** takes an iterator of pandas.DataFrame as input, and outputs another iterator of pandas.DataFrame. It's flexible and easy to use if your model requires all of your columns as input, but it requires serialization/deserialization of the whole DataFrame (as it is passed to its input). You can control the size of each pandas.DataFrame with the **`spark.sql.execution.arrow.maxRecordsPerBatch`** config."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1740be6a-ed06-4dfe-82f1-5ec7d387def5"}}},{"cell_type":"code","source":["def predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    model_path = f\"runs:/{run.info.run_id}/model\" \n    model = mlflow.sklearn.load_model(model_path) # Load model\n    for features in iterator:\n        yield pd.concat([features, pd.Series(model.predict(features), name=\"prediction\")], axis=1)\n    \ndisplay(spark_df.mapInPandas(predict, \"\"\"`host_total_listings_count` DOUBLE,`neighbourhood_cleansed` BIGINT,`latitude` DOUBLE,`longitude` DOUBLE,`property_type` BIGINT,`room_type` BIGINT,`accommodates` DOUBLE,`bathrooms` DOUBLE,`bedrooms` DOUBLE,`beds` DOUBLE,`bed_type` BIGINT,`minimum_nights` DOUBLE,`number_of_reviews` DOUBLE,`review_scores_rating` DOUBLE,`review_scores_accuracy` DOUBLE,`review_scores_cleanliness` DOUBLE,`review_scores_checkin` DOUBLE,`review_scores_communication` DOUBLE,`review_scores_location` DOUBLE,`review_scores_value` DOUBLE, `prediction` DOUBLE\"\"\")) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74820c98-9603-4501-b06d-1b0dd1c42fc4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Or you can define the schema like this below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6adb6470-7c7c-45b2-a22b-fe1b59e2b92e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\nfrom pyspark.sql.types import DoubleType\n\nschema = spark_df.withColumn(\"prediction\", lit(None).cast(DoubleType())).schema\ndisplay(spark_df.mapInPandas(predict, schema)) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55ae1aae-3415-4630-8365-7d3e683e9c8d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b34fdc11-4236-407c-8927-88424c3afb2b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 12 - Inference with Pandas UDFs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726785}},"nbformat":4,"nbformat_minor":0}
