{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ffb7dfb-0674-448a-8898-e2c49d342ff5"}}},{"cell_type":"markdown","source":["# Spark Review\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Create a Spark DataFrame\n - Analyze the Spark UI\n - Cache data\n - Go between Pandas and Spark DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3a8f599-b0e6-423d-b6fe-484cf242b8ba"}}},{"cell_type":"markdown","source":["![](https://files.training.databricks.com/images/sparkcluster.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa1242dc-6c2f-47f7-a31a-f3434c02afcc"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f0cc464-02f3-4209-aee8-c43e3c1a6540"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff09e6dc-68c1-4540-8ba4-c65d4e85693f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, rand\n\ndf = (spark.range(1, 1000000)\n      .withColumn(\"id\", (col(\"id\") / 1000).cast(\"integer\"))\n      .withColumn(\"v\", rand(seed=1)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cc80094-00d9-415b-82df-72d66efe79e3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Why were no Spark jobs kicked off above? Well, we didn't have to actually \"touch\" our data, so Spark didn't need to execute anything across the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c8fcb2d-098c-433a-b040-6775e76d024a"}}},{"cell_type":"code","source":["display(df.sample(.001))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da08d8c2-8506-4f2d-b3ae-9b5b7c807684"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Views\n\nHow can I access this in SQL?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ad25d4b-58bb-4913-bcd1-6a6c19789520"}}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"df_temp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8707f85-1072-4524-b776-ae66c2a766e8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM df_temp LIMIT 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dd04b3b-2ef2-4c44-aaf3-e0077802afdc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Count\n\nLet's see how many records we have."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e16ff3f-7d74-40b3-80ab-e31a75d5215d"}}},{"cell_type":"code","source":["df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b4c7216-73b2-4ed2-9176-4a91b9f7eee8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Spark UI\n\nOpen up the Spark UI - what are the shuffle read and shuffle write fields? The command below should give you a clue."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd399b2c-ce03-4b01-aad6-7d814d80e53f"}}},{"cell_type":"code","source":["df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"facab67c-7fdd-436a-bd82-364ae7a044e3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Cache\n\nFor repeated access, it will be much faster if we cache our data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"509c6ec7-364f-4ef4-9c3d-88ef7e91d188"}}},{"cell_type":"code","source":["df.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee3ecdee-22b1-44b3-b37d-5787ac38338e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Re-run Count\n\nWow! Look at how much faster it is now!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77bbda4c-b9b7-4729-9b6c-a0177451e161"}}},{"cell_type":"code","source":["df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e1819a9-ebbb-4e82-b0e3-389bc687f6a3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Collect Data\n\nWhen you pull data back to the driver  (e.g. call **`.collect()`**, **`.toPandas()`**,  etc), you'll need to be careful of how much data you're bringing back. Otherwise, you might get OOM exceptions!\n\nA best practice is explicitly limit the number of records, unless you know your data set is small, before calling **`.collect()`** or **`.toPandas()`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e9a0b6c-5012-4eb4-aff0-1c000db34452"}}},{"cell_type":"code","source":["df.limit(10).toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f292c15c-1c28-4ff9-a3ba-31500f0bc546"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## What's new in <a href=\"https://www.youtube.com/watch?v=l6SuXvhorDY&feature=emb_logo\" target=\"_blank\">Spark 3.0</a>\n\n* <a href=\"https://www.youtube.com/watch?v=jzrEc4r90N8&feature=emb_logo\" target=\"_blank\">Adaptive Query Execution</a>\n  * Dynamic query optimization that happens in the middle of your query based on runtime statistics\n    * Dynamically coalesce shuffle partitions\n    * Dynamically switch join strategies\n    * Dynamically optimize skew joins\n  * Enable it with: **`spark.sql.adaptive.enabled=true`**\n* Dynamic Partition Pruning (DPP)\n  * Avoid partition scanning based on the query results of the other query fragments\n* Join Hints\n* <a href=\"https://www.youtube.com/watch?v=UZl0pHG-2HA&feature=emb_logo\" target=\"_blank\">Improved Pandas UDFs</a>\n  * Type Hints\n  * Iterators\n  * Pandas Function API (mapInPandas, applyInPandas, etc)\n* And many more! See the <a href=\"https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_2.4_to_3.0.html\" target=\"_blank\">migration guide</a> and resources linked above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d40bc218-fefb-4b05-b4c1-6d86abe5b910"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"827bc0b1-a426-4cd9-8fe6-83876ac64167"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 00b - Spark Review","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726657}},"nbformat":4,"nbformat_minor":0}
