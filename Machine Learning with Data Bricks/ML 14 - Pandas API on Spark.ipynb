{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3faafbe0-b41f-4e44-9d5b-f4e1ab7b0b5b"}}},{"cell_type":"markdown","source":["# Pandas API on Spark\n\nThe pandas API on Spark project makes data scientists more productive when interacting with big data, by implementing the pandas DataFrame API on top of Apache Spark. By unifying the two ecosystems with a familiar API, pandas API on Spark offers a seamless transition between small and large data. \n\nSome of you might be familiar with the <a href=\"https://github.com/databricks/koalas\" target=\"_blank\">Koalas</a> project, which has been merged into PySpark in 3.2. For Apache Spark 3.2 and above, please use PySpark directly as the standalone Koalas project is now in maintenance mode. See this <a href=\"https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html\" target=\"_blank\">blog post</a>.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n- Demonstrate the similarities of the pandas API on Spark API with the pandas API\n- Understand the differences in syntax for the same DataFrame operations in pandas API on Spark vs PySpark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee08860a-08b0-4b69-a7ca-7a5b85af7176"}}},{"cell_type":"markdown","source":["<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/301/31gb.png\" width=\"900\"/>\n</div>\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/301/95gb.png\" width=\"900\"/>\n</div>\n\n**Pandas** DataFrames are mutable, eagerily evaluated, and maintain row order. They are restricted to a single machine, and are very performant when the data sets are small, as shown in a).\n\n**Spark** DataFrames are distributed, lazily evaluated, immutable, and do not maintain row order. They are very performant when working at scale, as shown in b) and c).\n\n**pandas API on Spark** provides the best of both worlds: pandas API with the performance benefits of Spark. However, it is not as fast as implementing your solution natively in Spark, and let's see why below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7efa5a6d-ef24-4b9e-874f-f921662478f1"}}},{"cell_type":"markdown","source":["## InternalFrame\n\nThe InternalFrame holds the current Spark DataFrame and internal immutable metadata.\n\nIt manages mappings from pandas API on Spark column names to Spark column names, as well as from pandas API on Spark index names to Spark column names. \n\nIf a user calls some API, the pandas API on Spark DataFrame updates the Spark DataFrame and metadata in InternalFrame. It creates or copies the current InternalFrame with the new states, and returns a new pandas API on Spark DataFrame.\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/301/InternalFramePs.png\" width=\"900\"/>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52bbb7d6-6eec-4229-8664-3d47440e41e7"}}},{"cell_type":"markdown","source":["## InternalFrame Metadata Updates Only\n\nSometimes the update of Spark DataFrame is not needed but of metadata only, then new structure will be like this.\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/301/InternalFrameMetadataPs.png\" width=\"900\"/>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cf6862e-39af-4156-a53d-70e5030e4226"}}},{"cell_type":"markdown","source":["## InternalFrame Inplace Updates\n\nOn the other hand, sometimes pandas API on Spark DataFrame updates internal state instead of returning a new DataFrame, for example, the argument  inplace=True is provided, then new structure will be like this.\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/301/InternalFrameUpdate.png\" width=\"900\"/>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c181070b-115b-4cc9-8149-505c8612ce3e"}}},{"cell_type":"markdown","source":["### Read in the dataset\n\n* PySpark\n* pandas\n* pandas API on Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b992ca74-9a7b-437e-a8e7-2931524b02c7"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01e2c6d4-8deb-426e-8430-295936515f8d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read in Parquet with PySpark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61f2e625-15d0-479b-827d-0ed2c3afc0ff"}}},{"cell_type":"code","source":["spark_df = spark.read.parquet(f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\")\ndisplay(spark_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad631da4-36af-43c0-bb26-fc74edc1b4b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read in Parquet with pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bea44e8e-3863-4be8-9a57-a1004a95b163"}}},{"cell_type":"code","source":["import pandas as pd\n\npandas_df = pd.read_parquet(f\"{datasets_dir.replace('dbfs:/', '/dbfs/')}/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\")\npandas_df.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ddb3a82-5ec7-4c0d-814b-a7eb92426bed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read in Parquet with pandas API on Spark. You'll notice pandas API on Spark generates an index column for you, like in pandas.\n\nPandas API on Spark also supports reading from Delta (**`read_delta`**), but pandas does not support that yet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b3b12a0-bd27-4320-8bc9-ecec23b2f484"}}},{"cell_type":"code","source":["import pyspark.pandas as ps\n\ndf = ps.read_parquet(f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\")\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e15c53f5-7c2a-4a5b-8472-25ce827fdc4c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### <a href=\"https://koalas.readthedocs.io/en/latest/user_guide/options.html#default-index-type\" target=\"_blank\">Index Types</a>\n\n![](https://files.training.databricks.com/images/301/koalas_index.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb2fc77d-37f3-4270-b530-4b1deac6c309"}}},{"cell_type":"code","source":["ps.set_option(\"compute.default_index_type\", \"distributed-sequence\")\ndf_dist_sequence = ps.read_parquet(f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\")\ndf_dist_sequence.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d75dfcaf-5f1e-4070-b288-1156372f6a33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Converting to pandas API on Spark DataFrame to/from Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02e0bfa8-8fe9-4483-83fc-382de89fb88c"}}},{"cell_type":"markdown","source":["Creating a pandas API on Spark DataFrame from PySpark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"381532a8-3e60-475e-917a-a647fdb0643e"}}},{"cell_type":"code","source":["df = ps.DataFrame(spark_df)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc9c3e7b-1f4f-476a-a770-5beb335f2f50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternative way of creating a pandas API on Spark DataFrame from PySpark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9abc260-e232-4ec3-8ecc-4e30bef4d4b7"}}},{"cell_type":"code","source":["df = spark_df.to_pandas_on_spark()\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e50e0ff-ab96-4160-a1ed-2ded7fed7ae6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Go from a pandas API on Spark DataFrame to a Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bed3e803-1efd-4247-ba0e-1f49dbc3fd78"}}},{"cell_type":"code","source":["display(df.to_spark())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa5e6161-f2ff-4dd2-b53f-3226d5de3f0c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Value Counts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b815a70a-c855-42f6-bbf7-1fc4f9cb9988"}}},{"cell_type":"markdown","source":["Get value counts of the different property types with PySpark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0341cbb7-b595-4ead-b654-7f25f4f8f9f5"}}},{"cell_type":"code","source":["display(spark_df.groupby(\"property_type\").count().orderBy(\"count\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c24f274-e001-4299-a347-a4eccd8dac10"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Get value counts of the different property types with pandas API on Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d66338c-9937-4025-b525-81bee120852c"}}},{"cell_type":"code","source":["df[\"property_type\"].value_counts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b94d8fda-1894-4b58-b04c-1d492066a7cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Visualizations\n\nBased on the type of visualization, the pandas API on Spark has optimized ways to execute the plotting.\n<br><br>\n\n![](https://files.training.databricks.com/images/301/ps_plotting.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecd9eb3-c8e9-41bc-86bd-e0ad8eaee287"}}},{"cell_type":"code","source":["df.plot(kind=\"hist\", x=\"bedrooms\", y=\"price\", bins=200)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c3d42ce-f49e-4020-94bb-1a29d59cc13f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### SQL on pandas API on Spark DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5a83b2c-0212-4385-aeb7-36c60b7ca367"}}},{"cell_type":"code","source":["ps.sql(\"SELECT distinct(property_type) FROM {df}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d721f56-cee3-4f53-a4e1-0c9cf7d46890"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Interesting Facts\n\n* With pandas API on Spark you can read from Delta Tables and read in a directory of files\n* If you use apply on a pandas API on Spark DF and that DF is <1000 (by default), pandas API on Spark will use pandas as a shortcut - this can be adjusted using **`compute.shortcut_limit`**\n* When you create bar plots, the top n rows are only used - this can be adjusted using **`plotting.max_rows`**\n* How to utilize **`.apply`** <a href=\"https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.apply.html#databricks.koalas.DataFrame.apply\" target=\"_blank\">docs</a> with its use of return type hints similar to pandas UDFs\n* How to check the execution plan, as well as caching a pandas API on Spark DF (which aren't immediately intuitive)\n* Koalas are marsupials whose max speed is 30 kph (20 mph)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebe9b6dc-5e24-48c9-85d8-6cf317d4e017"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b6ee64f-2fa0-4091-bf9e-a35f82b9dddb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 14 - Pandas API on Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726946}},"nbformat":4,"nbformat_minor":0}
