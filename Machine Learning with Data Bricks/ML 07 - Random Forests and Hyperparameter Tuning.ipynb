{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b5eae2d-e935-4a9a-9e8e-1c1b2ab9d574"}}},{"cell_type":"markdown","source":["# Random Forests and Hyperparameter Tuning\n\nNow let's take a look at how to tune random forests using grid search and cross validation in order to find the optimal hyperparameters.  Using the Databricks Runtime for ML, MLflow automatically logs your experiments with the SparkML cross-validator!\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Tune hyperparameters using Grid Search\n - Optimize a SparkML pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"767fa9f7-6db5-4284-b126-fc004ffca300"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e7b08cf-dd2b-4e15-864d-ec92a2055e62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\n\nfile_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\nairbnb_df = spark.read.format(\"delta\").load(file_path)\ntrain_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n\ncategorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\nindex_output_cols = [x + \"Index\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n\nnumeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\nassembler_inputs = index_output_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\nstages = [string_indexer, vec_assembler, rf]\npipeline = Pipeline(stages=stages)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb2e2315-d250-4390-b2a1-991fda618900"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ParamGrid\n\nFirst let's take a look at the various hyperparameters we could tune for random forest.\n\n**Pop quiz:** what's the difference between a parameter and a hyperparameter?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df84570c-c0ca-4c9b-aafc-723f950485ce"}}},{"cell_type":"code","source":["print(rf.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc054a7b-2d27-4ca6-882e-f54992e141a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n\nInstead of a manual (ad-hoc) approach, let's use Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgridbuilder#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> to find the optimal hyperparameters in a more systematic approach.\n\nLet's define a grid of hyperparameters to test:\n  - **`maxDepth`**: max depth of each decision tree (Use the values **`2, 5`**)\n  - **`numTrees`**: number of decision trees to train (Use the values **`5, 10`**)\n\n**`addGrid()`** accepts the name of the parameter (e.g. **`rf.maxDepth`**), and a list of the possible values (e.g. **`[2, 5]`**)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06a83a37-6b17-4f95-b583-5fee95d7b8a7"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparam_grid = (ParamGridBuilder()\n              .addGrid(rf.maxDepth, [2, 5])\n              .addGrid(rf.numTrees, [5, 10])\n              .build())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9b9b66a-dc1e-49a2-9394-bd545022eb0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Cross Validation\n\nWe are also going to use 3-fold cross validation to identify the optimal hyperparameters.\n\n![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n\nWith 3-fold cross-validation, we train on 2/3 of the data, and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b82fb971-d901-4120-92d0-7e1f72e382b7"}}},{"cell_type":"markdown","source":["We pass in the **`estimator`** (pipeline), **`evaluator`**, and **`estimatorParamMaps`** to <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a> so that it knows:\n- Which model to use\n- How to evaluate the model\n- What hyperparameters to set for the model\n\nWe can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"952fe0b1-5d9d-46df-8167-c714e89ab7c5"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nevaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n\ncv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid, \n                    numFolds=3, seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6099c449-f3b4-4426-93d0-86b5ddc6730c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Question**: How many models are we training right now?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78515774-99fe-472d-bb3a-dd0305a22095"}}},{"cell_type":"code","source":["cv_model = cv.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46a2b9e7-0f82-49d6-ac1b-42906d36badc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parallelism Parameter\n\nHmmm... that took a long time to run. That's because the models were being trained sequentially rather than in parallel!\n\nIn Spark 2.3, a <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator.parallelism\" target=\"_blank\">parallelism</a> parameter was introduced. From the docs: **`the number of threads to use when running parallel algorithms (>= 1)`**.\n\nLet's set this value to 4 and see if we can train any faster. The Spark <a href=\"https://spark.apache.org/docs/latest/ml-tuning.html\" target=\"_blank\">docs</a> recommend a value between 2-10."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f06ca2e9-9f2f-43f5-8204-f3db6178d2bb"}}},{"cell_type":"code","source":["cv_model = cv.setParallelism(4).fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c79444dc-932e-4625-855b-4972a903de34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Question**: Hmmm... that still took a long time to run. Should we put the pipeline in the cross validator, or the cross validator in the pipeline?\n\nIt depends if there are estimators or transformers in the pipeline. If you have things like StringIndexer (an estimator) in the pipeline, then you have to refit it every time if you put the entire pipeline in the cross validator.\n\nHowever, if there is any concern about data leakage from the earlier steps, the safest thing is to put the pipeline inside the CV, not the other way. CV first splits the data and then .fit() the pipeline. If it is placed at the end of the pipeline, we potentially can leak the info from hold-out set to train set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afecf7a3-e269-472d-be75-6f0482b8985d"}}},{"cell_type":"code","source":["cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=param_grid, \n                    numFolds=3, parallelism=4, seed=42)\n\nstages_with_cv = [string_indexer, vec_assembler, cv]\npipeline = Pipeline(stages=stages_with_cv)\n\npipeline_model = pipeline.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c077e24c-16b0-4baa-8fbf-05aac3dd4d91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at the model with the best hyperparameter configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e035dc9b-53ea-45c3-ae64-42e5eeaab924"}}},{"cell_type":"code","source":["list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b864e4c-ce56-4910-8c9c-3c69b8ec3d25"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pred_df = pipeline_model.transform(test_df)\n\nrmse = evaluator.evaluate(pred_df)\nr2 = evaluator.setMetricName(\"r2\").evaluate(pred_df)\nprint(f\"RMSE is {rmse}\")\nprint(f\"R2 is {r2}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df75eae2-c221-4ea4-9d00-b50467c3c1de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Progress!  Looks like we're out-performing decision trees."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8947b88c-5d37-44ae-a988-c1630de33f5f"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"783efaec-289b-47cb-b9b7-701699146fd0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 07 - Random Forests and Hyperparameter Tuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726349}},"nbformat":4,"nbformat_minor":0}
