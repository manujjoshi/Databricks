{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"959784f3-a8b0-478b-9c17-29a74ba1c31e"}}},{"cell_type":"markdown","source":["# Hyperparameter Tuning with Random Forests\n\n**In this lab, you will convert the Airbnb problem to a classification dataset, build a random forest classifier, and tune some hyperparameters of the random forest.**\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - **Perform grid search on a random forest**\n - **Generate feature importance scores and classification metrics**\n - **Identify differences between scikit-learn's Random Forest and SparkML's**\n \nYou can read more about the distributed implementation of Random Forests in the Spark <a href=\"https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala#L42\" target=\"_blank\">source code</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a393bc26-4baf-44f2-a2ec-0375cd40624b"}}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5df8f75b-4620-4761-ba81-f11e8c066d92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Defining courseware-specific utility methods...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Defining courseware-specific utility methods..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The source for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/scalable-machine-learning-with-apache-spark/v01/\n\nYour dataset directory is\ndbfs:/user/manujkumar.joshi@celebaltech.com/dbacademy/machine_learning/datasets\n\nSkipping install of existing dataset.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The source for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/scalable-machine-learning-with-apache-spark/v01/\n\nYour dataset directory is\ndbfs:/user/manujkumar.joshi@celebaltech.com/dbacademy/machine_learning/datasets\n\nSkipping install of existing dataset.\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## From Regression to Classification\n\nIn this case, we'll turn the Airbnb housing dataset into a classification problem to **classify between high and low price listings.**  Our **`class`** column will be:<br><br>\n\n- **`0`** **for a low cost listing of under $150**\n- **`1`** **for a high cost listing of $150 or more**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6c134bf-2031-4d5f-a91a-355aa449b64a"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\n\nfile_path = f\"{datasets_dir}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n\nairbnb_df = (spark\n            .read\n            .format(\"delta\")\n            .load(file_path)\n            .withColumn(\"priceClass\", (col(\"price\") >= 150).cast(\"int\"))\n            .drop(\"price\")\n           )\n\ntrain_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n\ncategorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\nindex_output_cols = [x + \"Index\" for x in categorical_cols]\n\nstring_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n\nnumeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"priceClass\"))]\nassembler_inputs = index_output_cols + numeric_cols\nvec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b864154c-2d25-4868-a089-ff465cfca854"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Why can't we OHE?\n\n**Question:** What would go wrong if we One Hot Encoded our variables before passing them into the random forest?\n\n**HINT:** Think about what would happen to the \"randomness\" of feature selection."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5577da5c-bc84-491e-adcd-385427aee5f5"}}},{"cell_type":"markdown","source":["## Random Forest\n\nCreate a Random Forest classifer called **`rf`** with the **`labelCol=priceClass`**, **`maxBins=40`**, and **`seed=42`** (for reproducibility).\n\nIt's under **`pyspark.ml.classification.RandomForestClassifier`** in Python."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ae84e8d-3d1c-49a0-ab50-467b4ece2fbc"}}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(labelCol='priceClass',maxBins=40,seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11747cd1-17b6-4ec9-b03b-a48730f5cdd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Grid Search\n\nThere are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n\nLet's use Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgrid#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> to find the optimal hyperparameters in a more systematic approach. Call this variable **`param_grid`**.\n\nLet's define a grid of hyperparameters to test:\n  - maxDepth: max depth of the decision tree (Use the values **`2, 5, 10`**)\n  - numTrees: number of decision trees (Use the values **`10, 20, 100`**)\n\n**`addGrid()`** accepts the name of the parameter (e.g. **`rf.maxDepth`**), and a list of the possible values (e.g. **`[2, 5, 10]`**)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c32abadd-775e-40ad-b5fb-be1cc070d367"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparam_grid = (ParamGridBuilder()\n              .addGrid(rf.maxDepth,[2,5,10])\n              .addGrid(rf.numTrees,[10,20,100])\n              .build()\n             )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3f74d8c-e9fc-45a5-8068-c2e6e3516852"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Evaluator\n\nIn the past, we used a **`RegressionEvaluator`**.  For classification, we can use a <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html?highlight=binaryclass#pyspark.ml.evaluation.BinaryClassificationEvaluator\" target=\"_blank\">BinaryClassificationEvaluator</a> if we have two classes or <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html?highlight=multiclass#pyspark.ml.evaluation.MulticlassClassificationEvaluator\" target=\"_blank\">MulticlassClassificationEvaluator</a> for more than two classes.\n\nCreate a **`BinaryClassificationEvaluator`** with **`areaUnderROC`** as the metric.\n\n<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\">Read more on ROC curves here.</a>  In essence, it compares true positive and false positives."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3df8143-aba7-44ba-80e2-75814d339340"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(labelCol='priceClass',metricName='areaUnderROC')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e5688d1-7735-4650-a0e1-d52500467c8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Cross Validation\n\nWe are going to do 3-Fold cross-validation, with **`parallelism`**=4, and set the **`seed`**=42 on the cross-validator for reproducibility.\n\n**Put the Random Forest in the CV to speed up the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">cross validation</a> (as opposed to the pipeline in the CV).**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29fd0802-6914-4d91-b03f-bf6b799a0bab"}}},{"cell_type":"code","source":["# TODO\n\nfrom pyspark.ml.tuning import CrossValidator\n\ncv = CrossValidator(evaluator=evaluator,estimator=rf,estimatorParamMaps=param_grid,numFolds=3,parallelism=4,seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eefd4e1-e76a-4014-975e-e55469f7342e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Pipeline\n\n#### Let's fit the pipeline with our cross validator to our training data (this may take a few minutes)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69341a11-bebf-4999-ab36-1f78a352a04b"}}},{"cell_type":"code","source":["stages = [string_indexer, vec_assembler, cv]\n\npipeline = Pipeline(stages=stages)\n\npipeline_model = pipeline.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"190104b8-ae9f-4d8d-8b10-a0a31445d611"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Hyperparameter\n\n#### Which hyperparameter combination performed the best?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc261829-8640-41e7-96e3-93f49af06e8b"}}},{"cell_type":"code","source":["cv_model = pipeline_model.stages[-1]\nrf_model = cv_model.bestModel\n\nlist(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n\nprint(rf_model.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b4cfadc-2ced-4960-8b60-dd531c21627c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: &#39;auto&#39; (choose automatically for task: If numTrees == 1, set to &#39;all&#39;. If numTrees &gt; 1 (forest), set to &#39;sqrt&#39; for classification and to &#39;onethird&#39; for regression), &#39;all&#39; (use all features), &#39;onethird&#39; (use 1/3 of the features), &#39;sqrt&#39; (use sqrt(number of features)), &#39;log2&#39; (use log2(number of features)), &#39;n&#39; (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = &#39;auto&#39; (default: auto)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\nlabelCol: label column name. (default: label, current: priceClass)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature. (default: 32, current: 40)\nmaxDepth: Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 10)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (&gt;= 1). (default: 20, current: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nseed: random seed. (default: -5387697053847413545, current: 42)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class&#39;s threshold. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">bootstrap: Whether bootstrap samples are used when building trees. (default: True)\ncacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\ncheckpointInterval: set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\nfeatureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: &#39;auto&#39; (choose automatically for task: If numTrees == 1, set to &#39;all&#39;. If numTrees &gt; 1 (forest), set to &#39;sqrt&#39; for classification and to &#39;onethird&#39; for regression), &#39;all&#39; (use all features), &#39;onethird&#39; (use 1/3 of the features), &#39;sqrt&#39; (use sqrt(number of features)), &#39;log2&#39; (use log2(number of features)), &#39;n&#39; (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = &#39;auto&#39; (default: auto)\nfeaturesCol: features column name. (default: features)\nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\nlabelCol: label column name. (default: label, current: priceClass)\nleafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\nmaxBins: Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature. (default: 32, current: 40)\nmaxDepth: Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 10)\nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1. (default: 1)\nminWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\nnumTrees: Number of trees to train (&gt;= 1). (default: 20, current: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nseed: random seed. (default: -5387697053847413545, current: 42)\nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class&#39;s threshold. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Feature Importance"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b55ea313-320e-4eac-ba23-9c89fab1d22d"}}},{"cell_type":"code","source":["import pandas as pd\n\npandas_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), rf_model.featureImportances)), columns=[\"feature\", \"importance\"])\ntop_features = pandas_df.sort_values([\"importance\"], ascending=False)\ntop_features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4475b76d-ff18-41bc-a215-7ba208f6d34b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[16]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>bedrooms</td>\n      <td>0.159175</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>room_typeIndex</td>\n      <td>0.146187</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>accommodates</td>\n      <td>0.142711</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neighbourhood_cleansedIndex</td>\n      <td>0.093438</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>beds</td>\n      <td>0.076920</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>host_total_listings_count</td>\n      <td>0.056300</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>latitude</td>\n      <td>0.053346</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>longitude</td>\n      <td>0.040256</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>review_scores_rating</td>\n      <td>0.033948</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>number_of_reviews</td>\n      <td>0.031841</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minimum_nights</td>\n      <td>0.029069</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property_typeIndex</td>\n      <td>0.027489</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bathrooms</td>\n      <td>0.026930</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancellation_policyIndex</td>\n      <td>0.015427</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>review_scores_cleanliness</td>\n      <td>0.013915</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>review_scores_location</td>\n      <td>0.009295</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>host_is_superhostIndex</td>\n      <td>0.008667</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>review_scores_accuracy</td>\n      <td>0.007399</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>instant_bookableIndex</td>\n      <td>0.006725</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>review_scores_value</td>\n      <td>0.005798</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>review_scores_communication</td>\n      <td>0.003535</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>review_scores_checkin</td>\n      <td>0.002402</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>review_scores_rating_na</td>\n      <td>0.001588</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bed_typeIndex</td>\n      <td>0.001337</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>review_scores_communication_na</td>\n      <td>0.001261</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>review_scores_cleanliness_na</td>\n      <td>0.001224</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>review_scores_value_na</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>review_scores_location_na</td>\n      <td>0.000999</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>review_scores_checkin_na</td>\n      <td>0.000801</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>review_scores_accuracy_na</td>\n      <td>0.000574</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bathrooms_na</td>\n      <td>0.000189</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>beds_na</td>\n      <td>0.000112</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bedrooms_na</td>\n      <td>0.000043</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>bedrooms</td>\n      <td>0.159175</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>room_typeIndex</td>\n      <td>0.146187</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>accommodates</td>\n      <td>0.142711</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neighbourhood_cleansedIndex</td>\n      <td>0.093438</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>beds</td>\n      <td>0.076920</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>host_total_listings_count</td>\n      <td>0.056300</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>latitude</td>\n      <td>0.053346</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>longitude</td>\n      <td>0.040256</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>review_scores_rating</td>\n      <td>0.033948</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>number_of_reviews</td>\n      <td>0.031841</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minimum_nights</td>\n      <td>0.029069</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property_typeIndex</td>\n      <td>0.027489</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bathrooms</td>\n      <td>0.026930</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancellation_policyIndex</td>\n      <td>0.015427</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>review_scores_cleanliness</td>\n      <td>0.013915</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>review_scores_location</td>\n      <td>0.009295</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>host_is_superhostIndex</td>\n      <td>0.008667</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>review_scores_accuracy</td>\n      <td>0.007399</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>instant_bookableIndex</td>\n      <td>0.006725</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>review_scores_value</td>\n      <td>0.005798</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>review_scores_communication</td>\n      <td>0.003535</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>review_scores_checkin</td>\n      <td>0.002402</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>review_scores_rating_na</td>\n      <td>0.001588</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bed_typeIndex</td>\n      <td>0.001337</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>review_scores_communication_na</td>\n      <td>0.001261</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>review_scores_cleanliness_na</td>\n      <td>0.001224</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>review_scores_value_na</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>review_scores_location_na</td>\n      <td>0.000999</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>review_scores_checkin_na</td>\n      <td>0.000801</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>review_scores_accuracy_na</td>\n      <td>0.000574</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bathrooms_na</td>\n      <td>0.000189</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>beds_na</td>\n      <td>0.000112</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bedrooms_na</td>\n      <td>0.000043</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Do those features make sense? Would you use those features when picking an Airbnb rental?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"790be669-90ca-4a44-96fb-41051e1d1e96"}}},{"cell_type":"markdown","source":["## Apply Model to test set"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a135a346-780b-48f4-b7c6-462a7d7dc6c1"}}},{"cell_type":"code","source":["# TODO\n\npred_df = pipeline_model.transform(test_df)\narea_under_roc = evaluator.evaluate(pred_df)\nprint(f\"Area under ROC is {area_under_roc:.2f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27eb2ee-08e4-4cde-a1ae-538a2f6a37c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Area under ROC is 0.97\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC is 0.97\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Save Model\n\nSave the model to **`working_dir`** (variable defined in Classroom-Setup)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aeb2b19-e234-4d40-9ec1-391b7f10a896"}}},{"cell_type":"code","source":["pipeline_model.write().overwrite().save(working_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58c474a4-d96b-4b9c-aa00-10ca612e320a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Sklearn vs SparkML\n\n<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\">Sklearn RandomForestRegressor</a> vs <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html?highlight=randomfore#pyspark.ml.regression.RandomForestRegressor\" target=\"_blank\">SparkML RandomForestRegressor</a>.\n\nLook at these params in particular:\n* **n_estimators** (sklearn) vs **numTrees** (SparkML)\n* **max_depth** (sklearn) vs **maxDepth** (SparkML)\n* **max_features** (sklearn) vs **featureSubsetStrategy** (SparkML)\n* **maxBins** (SparkML only)\n\nWhat do you notice that is different?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c31725cc-f673-48fa-8501-84f32f51f982"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dfe760a-fd78-496f-8e58-ccc092b5f1e0"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 07L - Hyperparameter Tuning Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2051889157726466}},"nbformat":4,"nbformat_minor":0}
